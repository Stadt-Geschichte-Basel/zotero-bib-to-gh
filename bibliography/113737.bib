
@article{rieger_selbstversuch_2021,
	title = {Selbstversuch eines Autors: Wie gut dichtet künstliche Intelligenz?},
	url = {https://www.tagesschau.de/wirtschaft/daniel-kehlmann-kuenstliche-intelligenz-versuch-101.html},
	shorttitle = {Selbstversuch eines Autors},
	abstract = {Was passiert, wenn man künstliche Intelligenz dichten lässt? Auf dieses Experiment hat sich der Literat Daniel Kehlmann eingelassen. Die frohe Kunde: {KI} wird ihn auf absehbare Zeit nicht ersetzen. Von Jenni Rieger.},
	journaltitle = {tagesschau.de},
	author = {Rieger, Jenni},
	urldate = {2021-02-11},
	date = {2021},
	langid = {german},
}

@article{vogeler_was_2021,
	title = {Was sind Digitale Geisteswissenschaften?},
	url = {https://www.derstandard.at/story/2000123781103/was-sind-digitale-geisteswissenschaften},
	abstract = {Wie Technologie und das Internet in die Wissenschaft einzog},
	journaltitle = {Der Standard},
	author = {Vogeler, Georg},
	urldate = {2021-02-09},
	date = {2021},
	langid = {austrian},
}

@online{andrews_digital_2020,
	title = {Digital Humanities - Vom Schattencurriculum zum Schwerpunkt in der Lehre},
	url = {https://science.apa.at/site/home/dossier-detail.html?key=SCI_20200130_SCI85476051652879346&marsname=Digital%20Humanities},
	abstract = {Klagenfurt (Gastkommentar) - Das Digitale scheint sich heute
unvermittelt in jedem Aspekt des...},
	author = {Andrews, Tara},
	urldate = {2020-02-04},
	date = {2020},
	langid = {german},
	keywords = {goal\_Dissemination, meta\_GiveOverview, meta\_Teaching, obj\_DigitalHumanities},
}

@online{thaler_digital_2020,
	title = {Digital, im Geiste},
	url = {https://science.apa.at/site/home/dossier-detail.html?key=SCI_20200130_SCI85456051452907036&},
	abstract = {Wien ({APA}-Science) - Visionen und Engagement auf der einen,
Geldmangel und Unstimmigkeiten auf...},
	author = {Thaler, Stefan},
	urldate = {2020-02-04},
	date = {2020},
	langid = {german},
	keywords = {act\_Collaborating, goal\_Dissemination, meta\_CommunityBuilding, meta\_GiveOverview, obj\_DigitalHumanities, obj\_Project},
}

@online{neubauer_digitale_2020,
	title = {Der digitale Archäologe},
	url = {https://science.apa.at/site/home/dossier-detail.html?key=SCI_20200130_SCI85476051652866922&},
	abstract = {Klagenfurt (Gastkommentar) - Verschrobene Einzelgänger mit
Pinseln auf der Suche nach Artefakten...},
	author = {Neubauer, Wolfgang},
	urldate = {2020-02-04},
	date = {2020},
	langid = {german},
	keywords = {act\_Discovering, act\_Imaging, goal\_Capture, meta\_GiveOverview, obj\_Artefacts, obj\_Maps},
}

@online{wentker_digitale_2020,
	title = {Digitale Geisteswissenschaften? Läuft!},
	url = {https://science.apa.at/site/home/dossier-detail.html?key=SCI_20200130_SCI85476051652899346&},
	shorttitle = {Digitale Geisteswissenschaften?},
	abstract = {Wien (Gastkommentar) - Der digitale Wandel macht vor den
Geisteswissenschaften nicht halt. Junge...},
	author = {Wentker, Sibylle},
	urldate = {2020-02-04},
	date = {2020},
	langid = {german},
	keywords = {goal\_Dissemination, meta\_CommunityBuilding, meta\_GiveOverview, obj\_DigitalHumanities, obj\_Project, obj\_Research},
}

@online{bosse_digitale_2020,
	title = {Digitale Editionen retten das kulturelle Erbe},
	url = {https://science.apa.at/site/home/dossier-detail.html?key=SCI_20200130_SCI85476051652866914&},
	abstract = {Klagenfurt (Gastkommentar) - Die rasante Digitalisierung prägt
nicht nur unseren Alltag, sie hat...},
	author = {Bosse, Anke},
	urldate = {2020-02-04},
	date = {2020},
	langid = {german},
	keywords = {act\_Publishing, goal\_Dissemination, meta\_GiveOverview, obj\_DigitalHumanities, obj\_Project, obj\_Research},
}

@online{kubadinow_naturhistorische_2020,
	title = {Das Naturhistorische Museum Wien macht seine Sammlungen digital zugänglich},
	url = {https://science.apa.at/site/home/dossier-detail.html?key=SCI_20200130_SCI85476051652720792&},
	abstract = {Wien (Gastkommentar) - Das {NHM} Wien beherbergt zirka 30 Millionen
naturkundliche sowie...},
	author = {Kubadinow, Irina},
	urldate = {2020-02-04},
	date = {2020},
	langid = {german},
	keywords = {act\_Imaging, act\_Publishing, goal\_Capture, goal\_Dissemination, obj\_Artefacts},
}

@article{author_information_missing_how_2020,
	title = {How data analysis can enrich the liberal arts},
	issn = {0013-0613},
	url = {https://www.economist.com/christmas-specials/2020/12/19/how-data-analysis-can-enrich-the-liberal-arts?utm_campaign=editorial-social&utm_medium=social-organic&utm_source=twitter},
	abstract = {But despite data science’s exciting possibilities, plenty of other academics object to it},
	journaltitle = {The Economist},
	author = {{(author information missing)}},
	urldate = {2020-12-22},
	date = {2020-12-19},
	langid = {english},
}

@incollection{edmonds_collaboration_2016,
	location = {Oxford},
	title = {Collaboration and Infrastructure},
	pages = {54--67},
	booktitle = {A New Companion to Digital Humanities},
	publisher = {Blackwell},
	author = {Edmonds, Jennifer},
	date = {2016},
	keywords = {meta\_ProjectManagement},
}

@inproceedings{benardou_defining_2012,
	title = {Defining user requirements for holocaust research infrastructures and services in the {EHRI} project},
	isbn = {978-1-4503-0782-6},
	url = {http://dl.acm.org/citation.cfm?doid=2132176.2132322},
	doi = {10.1145/2132176.2132322},
	abstract = {The poster presents the background, conceptual framework, methodology and initial results of a mixed research project, investigating information practice and user requirements of historians, humanities scholars and social scientists working on the Holocaust. The results of the study will be a foundation for the specification of functionalities of the European digital infrastructure planned as part of the {EU}-funded European Holocaust Research Infrastructure project, and consisting of the {EHRI} search/portal and the {EHRI} Virtual Research Environment. Particular issues to be dealt with are the summarization of qualitative evidence from semi-open interviews by means of a conceptualization of relevant descriptive codes of research activities, resource types and tools/services; identification of specific user requirements and of different researcher profiles through statistical analysis of an online questionnaire defined on the basis of initial qualitative research; and, identification and theorization of special needs of Holocaust research, seen as a highly multi- and inter-disciplinary field of inquiry.},
	pages = {644--645},
	publisher = {{ACM} Press},
	author = {Benardou, Agiatis and Dallas, Costis},
	urldate = {2012-04-30},
	date = {2012},
	langid = {english},
	keywords = {X-{CHECK}, act\_Conceptualizing},
}

@article{tabak_hybrid_2017,
	title = {A Hybrid Model for Managing {DH} Projects},
	volume = {11},
	number = {1},
	journaltitle = {Digital Humanities Quarterly},
	author = {Tabak, Edin},
	date = {2017},
	keywords = {meta\_ProjectManagement},
}

@article{reed_managing_2014,
	title = {Managing an Established Digital Humanities Project: Principles and Practices from the Twentieth Year of the William Blake Archive},
	volume = {8},
	url = {http://www.digitalhumanities.org/dhq/vol/8/1/000174/000174.html},
	number = {1},
	journaltitle = {Digital Humanities Quarterly},
	author = {Reed, Ashley},
	date = {2014},
	keywords = {meta\_ProjectManagement},
}

@incollection{pitti_designing_2004,
	location = {Oxford},
	title = {Designing Sustainable Projects and Publications},
	pages = {471--487},
	booktitle = {A companion to digital humanities},
	publisher = {Blackwell},
	author = {Pitti, Daniel V.},
	editor = {Schreibman, Susan and Siemens, Ray and Unsworth, John},
	date = {2004},
	keywords = {meta\_ProjectManagement},
}

@online{cremer_gottes_2019,
	title = {Gottes Werk und Teufels Beitrag: Ein Essay zu Digital Humanities und Projektmanagement},
	url = {https://dhd-blog.org/?p=11283},
	titleaddon = {{DHd}-Blog},
	type = {Blog},
	author = {Cremer, Fabian},
	date = {2019},
	keywords = {meta\_ProjectManagement},
}

@incollection{crompton_project_2016,
	title = {Project Management and the Digital Humanist},
	isbn = {978-1-317-48113-3},
	abstract = {Digital Humanities is rapidly evolving as a significant approach to/method of teaching, learning and research across the humanities. This is a first-stop book for people interested in getting to grips with digital humanities whether as a student or a professor. The book offers a practical guide to the area as well as offering reflection on the main objectives and processes, including:  Accessible introductions of the basics of Digital Humanities through to more complex ideas A wide range of topics from feminist Digital Humanities, digital journal publishing, gaming, text encoding, project management and pedagogy Contextualised case studies Resources for starting Digital Humanities such as links, training materials and exercises   Doing Digital Humanities looks at the practicalities of how digital research and creation can enhance both learning and research and offers an approachable way into this complex, yet essential topic.},
	booktitle = {Doing Digital Humanities: Practice, Training, Research},
	publisher = {Taylor \& Francis},
	author = {Siemens, Lynne},
	editor = {Crompton, Constance and Lane, Richard J. and Siemens, Ray},
	date = {2016-09-13},
	langid = {english},
	note = {Google-Books-{ID}: {uC}4lDwAAQBAJ},
	keywords = {meta\_ProjectManagement},
}

@article{piotrowski_aint_2020,
	title = {Ain’t No Way Around It: Why We Need to Be Clear About What We Mean by “Digital Humanities”},
	shorttitle = {Ain’t No Way Around It},
	author = {Piotrowski, Michael},
	date = {2020},
}

@article{tahmasebi_visions_2015,
	title = {Visions and open challenges for a knowledge-based culturomics},
	volume = {15},
	pages = {169--187},
	number = {2},
	journaltitle = {International Journal on Digital Libraries},
	author = {Tahmasebi, Nina and Borin, Lars and Capannini, Gabriele and Dubhashi, Devdatt and Exner, Peter and Forsberg, Markus and Gossen, Gerhard and Johansson, Fredrik D. and Johansson, Richard and K\{{\textbackslash}textbackslash\}aagebäck, Mikael},
	date = {2015},
}

@inproceedings{kaageback_extractive_2014,
	title = {Extractive summarization using continuous vector space models},
	pages = {31--39},
	booktitle = {Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality ({CVSC})},
	author = {K\{{\textbackslash}textbackslash\}aagebäck, Mikael and Mogren, Olof and Tahmasebi, Nina and Dubhashi, Devdatt},
	date = {2014},
}

@article{tahmasebi_survey_2018,
	title = {Survey of computational approaches to diachronic conceptual change},
	journaltitle = {{arXiv} preprint {arXiv}:1811.06278},
	author = {Tahmasebi, Nina and Borin, Lars and Jatowt, Adam},
	date = {2018},
}

@article{kim_survey_2019,
	title = {A Survey on Sentiment and Emotion Analysis for Computational Literary Studies},
	rights = {{CC} {BY}-{SA} 4.0},
	url = {http://www.zfdg.de/2019_008},
	doi = {10.17175/2019_008},
	journaltitle = {Zeitschrift für digitale Geisteswissenschaften},
	author = {Kim, Evgeny and Klinger, Roman},
	urldate = {2020-07-28},
	date = {2019},
	note = {Publisher: Herzog August Bibliothek
Version Number: 1.0},
	keywords = {act\_Annotating, obj\_Text, t\_SentimentAnalysis},
}

@incollection{kollatz_epidat_2018,
	title = {{EPIDAT}. Research Platform for Jewish Epigraphy.{IN}: Crossing Experiences in Digital Epigraphy. From Practice to Discipline. Digital Epigraphy},
	abstract = {{EPIDAT}, the research platform for Jewish epigraphy, deals with Jewish epigraphy in all its aspects. This article describes the on-going project and datadriven development, since the year 2002, which resulted in a wide range of access options to the epigraphic records. Later on, the solid data basis hosted by {EPIDAT} enabled cooperation across disciplines (linguistics, art history, monument science, cultural heritage agencies) and other epigraphic projects. Interoperability is essential for epigraphy, but needs reliable ontologies and cooperation over several projects and beyond disciplines.},
	publisher = {De Gruyter open},
	author = {Kollatz, Thomas},
	date = {2018},
	langid = {english},
	keywords = {act\_Preservation, act\_Visualizing, goal\_Analysis, obj\_Project, obj\_Research, obj\_Text},
}

@article{neuroth_nachhaltigkeit_2016,
	title = {Nachhaltigkeit von digitalen Forschungsinfrastrukturen},
	volume = {40},
	url = {file:///C:/Users/bertino/Downloads/[Bibliothek%20Forschung%20und%20Praxis]%20Nachhaltigkeit%20von%20digitalen%20Forschungsinfrastrukturen.pdf},
	doi = {DOI 10.1515/bfp-2016-0022},
	abstract = {{DARIAH}-{DE} as a research infrastructure is entering the next important phase. After the preparatory phase, followed by the construction {DARIAH}-{DE} is now preparing the operational phase which has to guarantee on the one hand the stability of all services and tools of this digital research infrastructure and on the other hand a dynamic as well as flexible development based on user needs from the scholarly, information science and {ITexpert} communities. The challenge is now to find the right balance between trust as well as stability and communitydriven expansion and development. This article introduces in different aspects of sustainability of research infrastructures.},
	pages = {264--270},
	number = {2},
	journaltitle = {Bibliothek - Forschung und Praxis},
	author = {Neuroth, Heike and Andrea, Rapp},
	date = {2016},
	keywords = {act\_Archiving, act\_Collaborating, act\_Preservation, goal\_Dissemination, goal\_Storage, meta\_CommunityBuilding, obj\_Infrastructures},
}

@book{veidlinger_digital_2019,
	title = {Digital Humanities and Buddhism. An introduction},
	isbn = {978-3-11-051908-2},
	url = {https://www.degruyter.com/view/product/480584},
	shorttitle = {Digital Humanities and Buddhism.},
	abstract = {{IDH} Religion provides a series of short introductions to specific areas of study at the intersections of digital humanities and religion, offering an overview of current methodologies, techniques, tools, and projects as well as defining challenges and opportunities for further research. This volume explores {DH} and Buddhism in four sections: Theory and Method; Digital Conservation, Preservation and Archiving; Digital Analysis; Digital Resources. It covers themes such as language processing, digital libraries, online lexicography, and ethnographic methods.},
	pagetotal = {234},
	publisher = {De Gruyter},
	author = {Veidlinger, Daniel},
	date = {2019},
	keywords = {goal\_Analysis, goal\_Storage, meta\_GiveOverview, obj\_DigitalHumanities, obj\_Methods, obj\_Tools},
}
@article{aurast_big_2016,
	title = {Big Data und Smart Data in den Geisteswissenschaften},
	volume = {40},
	doi = {DOI: https://doi.org/10.1515/bfp-2016-0033},
	abstract = {With networked large data collections and suitable quantitative methods new research questions become possible. Within the {DARIAH}-{DE} cluster 5 new philologico-critical and historical comparing perspectives enter the modelling of large data collections. Consequently new, tailored analytical tools can be developed. This includes the chance to test in empirical ways theoretical presuppositions and the extension and advancement of the modes of the production of scientific knowledge. The article describes two analytical tools developed within cluster 5: the Dariah-{DKPro}-Wrapper, a software package for the automated linguistical analysis, and Cosmotool, a search- and analysis tool for the exploration of transboundary lives by means of structured and unstructured data collections.},
	number = {2},
	journaltitle = {Bibliothek Forschung und Praxis},
	author = {Aurast, Anna and Gradl, Tobias and Pernes, Stefan and Pielström, Steffen},
	date = {2016},
	keywords = {act\_Collaborating, act\_ContentAnalysis, act\_RelationalAnalysis, goal\_Analysis, goal\_Dissemination, meta\_Teaching, obj\_Data, obj\_Tools},
}

@article{eyers_perils_2013,
	title = {The Perils of the “Digital Humanities”: New Positivisms and the Fate of Literary Theory},
	volume = {23},
	url = {http://www.pomoculture.org/2015/07/08/the-perils-of-the-digital-humanities-new-positivisms-and-the-fate-of-literary-theory/},
	number = {2},
	journaltitle = {Posmodern Culture},
	author = {Eyers, Tom},
	urldate = {2019-08-25},
	date = {2013},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{allison_authorship_2019,
	title = {Authorship After {AI}},
	url = {https://www.publicbooks.org/authorship-after-ai/},
	abstract = {Authorship attribution is helpful if you suspect fraud: for instance, if you believe that Shakespeare wasn’t educated enough to write the plays, or that Charlotte Brontë’s Jane Eyre was really ...},
	journaltitle = {Public Books},
	author = {Allison, Sarah},
	urldate = {2019-06-27},
	date = {2019-06-25},
	langid = {american},
	keywords = {act\_StylisticAnalysis, goal\_Analysis, obj\_Literature, obj\_Persons},
}

@article{underwood_why_2018,
	title = {Why an Age of Machine Learning Needs the Humanities},
	url = {https://www.publicbooks.org/why-an-age-of-machine-learning-needs-the-humanities/},
	abstract = {It isn’t easy to be a citizen in 2018. We are told to watch out for bots and biased ...},
	journaltitle = {Public Books},
	author = {Underwood, Ted},
	urldate = {2019-06-27},
	date = {2018-12-05},
	langid = {american},
	keywords = {{AnalyzeStatistically}, goal\_Analysis, obj\_Computers, obj\_Methods, t\_MachineLearning},
}

@article{gradl_dariah--foderationsarchitektur_2016,
	title = {Die {DARIAH}-{DE}-Föderationsarchitektur – Datenintegration im Spannungsfeld forschungsspezifischer und domänenübergreifender Anforderungen},
	volume = {40},
	url = {https://www.degruyter.com/downloadpdf/j/bfup.2016.40.issue-2/bfp-2016-0027/bfp-2016-0027.pdf},
	doi = {DOI 10.1515/bfp-2016-0027},
	abstract = {This contribution presents an overview of the concepts and components of the {DARIAH}-{DE} federation architecture. After a distinction between requirements of research-specific and cross-domain integration, the creation context and application context of research data are differentiated along with the facilities for their explication within the research architecture, which is shown to support generic and specific queries within the generic search.},
	pages = {222--228},
	number = {2},
	journaltitle = {{BIBLIOTHEK} – Forschung und Praxis},
	author = {Gradl, Tobias and Henrich, Andreas},
	date = {2016},
	keywords = {act\_Annotating, act\_Collaborating, goal\_Dissemination, goal\_Enrichment, meta\_GiveOverview, obj\_Data, obj\_Infrastructures, obj\_Standards},
}

@article{busch_wo_2016,
	title = {Wo bleibt eigentlich der einzelne Fachwissenschaftler? Community building als Aufgabe und Herausforderung für {DH}-Infrastrukturen},
	volume = {40},
	url = {https://www.degruyter.com/downloadpdf/j/bfup.2016.40.issue-2/bfp-2016-0028/bfp-2016-0028.pdf},
	doi = {DOI 10.1515/bfp-2016-0028},
	shorttitle = {Wo bleibt eigentlich der einzelne Fachwissenschaftler?},
	abstract = {Digital Humanities infrastructures tend to lose
sight of their actual clientele: the individual scholar. This
paper describes the efforts {DARIAH}-{DE} undertakes to
counteract such developments. The specially established “Stakeholdergremium Fachgesellschaften” is described, detailing its tasks and objectives illustrated as well as the connections between {DHd} and {DARIAH}-{DE}.},
	pages = {278--282},
	number = {2},
	journaltitle = {{BIBLIOTHEK} – Forschung und Praxis},
	author = {Busch, Anna and Meister, Jan Christoph and Schumacher, Mareike},
	date = {2016},
	keywords = {act\_Collaborating, act\_Sharing, goal\_Dissemination, meta\_CommunityBuilding, meta\_GiveOverview, obj\_Infrastructures},
}

@article{kollatz_raum-zeit-analysen_2016,
	title = {Raum-Zeit-Analysen mit Geo-Browser und Datasheet-Editor},
	volume = {40},
	url = {https://www.degruyter.com/downloadpdf/j/bfup.2016.40.issue-2/bfp-2016-0032/bfp-2016-0032.pdf},
	doi = {DOI 10.1515/bfp-2016-0032},
	abstract = {: Presentation of {DARIAH}-{DE} Geo-Browser and {DARIAH}-{DE} Datasheet Editor as tools for spatio-temporal Visualization and analysis.},
	pages = {229--233},
	number = {2},
	journaltitle = {{BIBLIOTHEK} – Forschung und Praxis},
	author = {Kollatz, Thomas},
	date = {2016},
	keywords = {act\_Visualizing, goal\_Analysis, obj\_Interaction, obj\_Maps, obj\_Tools, obj\_Visualization},
}

@collection{edmond_digital_2020,
	title = {Digital Technology and the Practices of Humanities Research},
	isbn = {978-1-78374-839-6 978-1-78374-840-2 978-1-78374-841-9 978-1-78374-842-6 978-1-78374-843-3 978-1-78374-844-0},
	url = {https://www.openbookpublishers.com/product/1108},
	publisher = {Open Book Publishers},
	editor = {Edmond, Jennifer},
	urldate = {2020-02-04},
	date = {2020-02},
	langid = {english},
	doi = {10.11647/obp.0192},
	keywords = {act\_Publishing, goal\_Dissemination, meta\_GiveOverview, obj\_Research},
}

@article{jacobsen_stylometry_2013,
	title = {Stylometry of paintings using hidden Markov modelling of contourlet transforms},
	volume = {93},
	issn = {0165-1684},
	url = {http://www.sciencedirect.com/science/article/pii/S0165168412003544},
	doi = {10.1016/j.sigpro.2012.09.019},
	series = {Image Processing for Digital Art Work},
	abstract = {Visual stylometry is the task of quantifying artistic style in the visual arts. In this paper we present a method for visual stylometry of paintings from digital reproductions.

Our method is framed around modelling contourlet transforms of the digital reproductions with hidden Markov models. Using the contourlet transform in the field of classification is a new approach motivated by the contourlets' efficiency in representing piecewise smooth contours such as brushstrokes.

To test our method we have used paintings related to the Danish painter Asger Jorn and drawings related to the Flemish artist Pieter Bruegel the Elder. The paintings related to Asger Jorn are recorded in multiple digital images and by two different cameras. With multiple sources we are able to get insight into the robustness of our method against different means of acquisition.

Through a cross-validation of the Jorn images by one of the cameras we are able to correctly classify 39 out of 44 images; based on this classifier we can correctly classify 28 out of 36 images in the other data set.

A cross-validation of the Bruegel images correctly classifies 11 out of 13 images.},
	pages = {579--591},
	number = {3},
	journaltitle = {Signal Processing},
	shortjournal = {Signal Processing},
	author = {Jacobsen, C. R. and Nielsen, M.},
	urldate = {2015-04-08},
	date = {2013-03},
	keywords = {{AnalyzeStatistically}, act\_StylisticAnalysis, goal\_Analysis, obj\_Images, obj\_Music, t\_Stylometry},
}

@book{jannidis_digital_2017,
	title = {Digital Humanities. Eine Einführung},
	url = {https://www.springer.com/de/book/9783476026224},
	shorttitle = {Digital Humanities},
	abstract = {Computerbasierte Verfahren greifen in viele Bereiche der Geistes- und Kulturwissenschaften ein und spielen eine zunehmende Rolle in der universitären Bildung. Dieser Band bietet eine fundierte Einführung in die grundlegenden Konzepte, Methoden und Werkzeuge der Digital Humanities. Sie präsentiert Grundlagen wie Digitalisierung, Aufbau von Datensammlungen, Datenmodellierung und {XML}. Darüber hinaus behandelt sie Anwendungsgebiete wie Digitale Edition, Information Retrieval, Netzwerkanalyse, Geographische Informationssysteme, Simulation ebenso weiterführende Aspekte wie die Rolle der Bibliotheken, Archive und Museen sowie rechtliche und ethische Fragen.},
	pagetotal = {370},
	publisher = {Metzler},
	author = {Jannidis, Fotis and Kohle, Hubertus and Rehbein, Malte},
	date = {2017},
}

@book{semino_corpus_2004,
	location = {London; New York},
	title = {Corpus stylistics: speech, writing and thought presentation in a corpus of English writing},
	isbn = {0-415-28669-7 978-0-415-28669-5 978-0-203-49407-3 0-203-49407-5},
	shorttitle = {Corpus stylistics},
	abstract = {"This book combines stylistic analysis with corpus linguistics in order to provide an account of the phenomenon of speech, writing and thought presentation - commonly referred to as 'speech reporting' or 'discourse presentation'." "Corpus Stylistics shows how stylistics, and text/discourse analysis more generally, can benefit from the use of a corpus methodology. This book will be essential reading for linguists interested in the areas of stylistics and corpus linguistics."--{BOOK} {JACKET}.},
	publisher = {Routledge},
	author = {Semino, Elena},
	date = {2004},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, act\_StructuralAnalysis, act\_StylisticAnalysis},
}

@book{mounier_les_nodate,
	title = {Les humanités numériques. Une histoire critique},
	isbn = {2-7351-2255-7},
	url = {http://www.editions-msh.fr/livre/?GCOI=27351100065270},
	shorttitle = {Les humanités numériques},
	abstract = {Que sont les humanités numériques ?

D'abord une rencontre, au lendemain de la Seconde Guerre mondiale. Celle d'un prêtre jésuite soucieux d’analyser la Somme théologique de Thomas d’Aquin avec les ordinateurs d’{IBM}. Cette collaboration donnera naissance à ce qu’on appellera plus tard les humanités numériques.

Porteuses de l’histoire des technologies, marquée par le développement des technosciences et du complexe militaro-industriel, les humanités numériques conduisent à s’interroger en retour sur ce qui fait la spécificité des humanités. L’union des technologies numériques et des humanités conduit-elle à remettre en cause ce qui les dinstingue traditionnellement ? Le numérique pousse-t-il, par les méthodes et modèles qu’il permet de développer dans ce champ de recherche, à placer les humanités sous la domination de modèles scientifiques qui leur sont étrangers ?

Quels dangers ces approches comportent-elles, en particulier lorsqu’une part croissante des productions culturelles et des interactions sociales est désormais placée sous l’emprise de sociétés commerciales globalisées qui font un usage massif du numérique ?

Dans cet ouvrage, Pierre Mounier nous livre une histoire critique des humanités numériques et propose de redéfinir à la lumière de ces analyses le contrat moral que les humanités peuvent établir avec la société.},
	pagetotal = {180},
	author = {Mounier, Pierre},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{noack_missing_2019,
	title = {Missing Link: Künstliche Intelligenz – der weite Weg zur Kunst aus der Maschine},
	url = {Der KI-Hype ist ungebrochen. Auch in der Kunst finden lernende Algorithmen zunehmend Anwendung. Kulturschaffende müssen aber noch nicht um ihre Jobs bangen.},
	abstract = {Der {KI}-Hype ist ungebrochen. Auch in der Kunst finden lernende Algorithmen zunehmend Anwendung. Kulturschaffende müssen aber noch nicht um ihre Jobs bangen.},
	journaltitle = {heise.de},
	author = {Noack, Pit},
	date = {2019-12-01},
	keywords = {act\_Programming, goal\_Analysis, goal\_Creation, obj\_Computers, obj\_Research},
}

@online{bernhart_wie_2019,
	title = {Wie die Computer dichten lernten},
	url = {https://www.sueddeutsche.de/digital/literatur-marbach-computer-dichtung-poesie-theo-lutz-1.4649251},
	abstract = {Literaturarchiv Marbach: Der Informatiker Theo Lutz erzeugte vor sechzig Jahren die ersten deutschen Gedichte aus dem Computer.},
	titleaddon = {Süddeutsche.de},
	author = {Bernhart, Toni and Richter, Sandra},
	urldate = {2019-10-27},
	date = {2019},
	langid = {german},
	keywords = {act\_Programming, act\_Writing, goal\_Creation, obj\_Literature, obj\_Persons},
}

@collection{lunen_historia_2020,
	location = {New York, {NY}},
	title = {Historia ludens: the playing historian},
	isbn = {978-1-00-069313-3 978-0-429-34561-6 978-1-00-069295-2 978-1-00-069331-7},
	series = {Routledge approaches to history},
	shorttitle = {Historia ludens},
	abstract = {"This book aims to further a debate about aspects of 'playing' and 'gaming' in connection with history. Reaching out to academics, professionals and students alike, it pursues a dedicated interdisciplinary approach. Rather than only focusing on how professionals could learn from academics in history, the book also ponders the question of what academics can learn from gaming and playing for their own practice, such as gamification for teaching, or using 'play' as a paradigm for novel approaches into historical scholarship. 'Playing' and 'gaming' are thus understood as a broad cultural phenomenon that cross-pollinates the theory and practice of history and gaming alike"--},
	pagetotal = {1},
	number = {vol. 30},
	publisher = {Routledge},
	editor = {Lünen, Alexander von and Lewis, Katherine J. and Litherland, Benjamin and Cullum, P. H.},
	date = {2020},
	keywords = {act\_ContentAnalysis, goal\_Analysis, obj\_Games},
}

@article{da_computational_2019,
	title = {The Computational Case against Computational Literary Studies},
	volume = {45},
	issn = {0093-1896},
	url = {https://www.journals.uchicago.edu/doi/abs/10.1086/702594},
	doi = {10.1086/702594},
	pages = {601--639},
	number = {3},
	journaltitle = {Critical Inquiry},
	shortjournal = {Critical Inquiry},
	author = {Da, Nan Z.},
	urldate = {2019-08-25},
	date = {2019},
}

@book{de_santis_crossing_2019,
	location = {Berlin, Boston},
	title = {Crossing Experiences in Digital Epigraphy, From Practice to Discipline},
	isbn = {978-3-11-060719-2},
	url = {https://www.degruyter.com/viewbooktoc/product/506243#},
	publisher = {De Gruyter},
	author = {De Santis, Annamaria and Rossi, Irene},
	urldate = {2019-08-21},
	date = {2019},
	doi = {10.1515/9783110607208},
}

@thesis{funk_elektronisches_2018,
	location = {Köln},
	title = {Elektronisches Publizieren von Digitalen Forschungsdaten am Beispiel des {TextGrid} Repositorys – Umsetzung von Digitalen Publikationsworkflows für die {eHumanities}},
	shorttitle = {Elektronisches Publizieren von Digitalen Forschungsdaten am Beispiel des {TextGrid} Repositorys},
	pagetotal = {97},
	type = {phdthesis},
	author = {Funk, Stefan Edwin},
	date = {2018},
	langid = {german},
}

@article{gorner_digital_2019,
	title = {Digital Humanities: Wenn die Saat des Digitalen aufgeht},
	issn = {0174-4909},
	url = {https://www.faz.net/aktuell/feuilleton/hoch-schule/wenn-die-saat-des-digitalen-aufgeht-digital-humanities-16255522.html},
	shorttitle = {Digital Humanities},
	abstract = {Bedeutet die Zunahme digitaler Interpretationstechniken das Ende einer sinnvollen Textauslegung? Über das Interpretieren von Texten in Zeiten von „Digital Humanities“.},
	journaltitle = {Frankfurter Allgemeine Zeitung ({FAZ})},
	author = {Görner, Rüdiger},
	urldate = {2019-07-01},
	date = {2019},
	langid = {german},
	keywords = {Aristoteles, Erschließungsverfahren, Facebook, Google Inc., {ISIN}\_US30303M1027, {ISIN}\_US38259P5089, Stephen Marche},
}

@collection{schoch_stylometry_2016,
	title = {Stylometry Bibliography},
	url = {https://www.zotero.org/groups/643516/},
	abstract = {The bibliography is based on contributions by Hugh Craig, Joseph Rudman, Patrick Juola as well as Mike Kestemont and Maciej Eder and has been curated by Christof Schöch.
The scope of this bibliography is stylometry defined broadly as the quantitative analysis of literary style. There is a special focus on authorship attribution, but work in linguistic forensics and quantitative stylistics is also included. The scope is open with regard to time, place, type and language of publication.
For more information, see: https://dragonfly.hypotheses.org/1093.},
	publisher = {Zotero},
	editor = {Schöch, Christof},
	date = {2016},
}

@online{schlosser_neuen_2019,
	title = {Die neuen Denkwege der Digital Humanities},
	url = {https://www.wienerzeitung.at/nachrichten/reflexionen/vermessungen/2014825-Die-neuen-Denkwege-der-Digital-Humanities.html},
	abstract = {Die Geisteswissenschaften sollen mit Hilfe der Informationstechnologie ins 21. Jahrhundert geführt werden. Ein Überblick.},
	titleaddon = {Wiener Zeitung Online},
	author = {Schlösser, Hermann},
	urldate = {2019-06-23},
	date = {2019},
	langid = {german},
}

@article{krajewski_digital_2019,
	title = {Digital Humanities: Hilfe für die Hilfswissenschaft},
	issn = {0174-4909},
	url = {https://www.faz.net/1.6131498},
	shorttitle = {Digital Humanities},
	abstract = {Von den Digital Humanities verspricht man sich wahre Wunder. Was oft vergessen wird: Wer sich mit der computerisierten Gesellschaft auskennen möchte, muss auch Codes lesen können – und deren Wirkung verstehen.},
	author = {Krajewski, Markus},
	urldate = {2019-04-12},
	date = {2019},
	langid = {german},
}

@article{franceschi-bicchierai_forensic_2018,
	title = {Forensic Linguist Says 'Lodestar' Can't Tell Us Who Burned Trump in New York Times Op-Ed},
	url = {https://motherboard.vice.com/en_us/article/pa8eev/forensic-linguist-lodestar-trump-anonymous-new-york-times-op-ed},
	journaltitle = {Motherboard},
	author = {Franceschi-Bicchierai, Lorenzo},
	date = {2018},
	keywords = {goal\_Analysis, obj\_Text, t\_Stylometry},
}
@book{loebel_lost_2016,
	location = {Glückstadt},
	title = {Lost in Translation: Leistungsfähigkeit,
Einsatz und Grenzen von Emulatoren bei der Langzeitbewahrung
digitaler multimedialer Objekte},
	isbn = {9783864880681},
	abstract = {Die Erhaltung und das Einlesen
von digitalen Objekten wird in absehbarer Zeit ein Problem werden, da sich diese Objekte auf digitalen
Datenträgern befinden, welche wiederum an spezifische in sich geschlossene
Hardwaresysteme gebunden sind. Mit
genau dieser Thematik befasst sich der
Autor des Buches. Sein Ziel ist es, die
Emulation von digitalen Objekten als
eine mögliche Methode der Langzeiterhaltung zu untersuchen und zu bewerten. Dabei geht er hauptsächlich auf die technischen Aspekte von Emulatoren, die darunterliegenden Originalsysteme und die Emulationsprozesse ein.},
	pagetotal = {184},
	publisher = {Verlag Werner Hülsbusch},
	author = {Loebel, Jens-Martin},
	date = {2016},
	langid = {german},
	keywords = {act\_Collaborating, act\_Conversion, act\_Publishing, obj\_AnyObject},
}

@book{jones_roberto_2016,
	location = {London},
	title = {Roberto Busa, S. J., and the Emergence of Humanities Computing. The Priest and the Punched Cards},
	isbn = {9781138186774},
	pagetotal = {186},
	publisher = {Routledge},
	author = {Jones, Steven E.},
	urldate = {2017-10-20},
	date = {2016},
	keywords = {act\_Conceptualizing, act\_RelationalAnalysis, obj\_DigitalHumanities},
}

@article{beer_how_2017,
	title = {How should we do the history of Big Data?},
	volume = {3},
	rights = {oa},
	issn = {20539517},
	url = {http://journals.sagepub.com/doi/pdf/10.1177/2053951716646135},
	doi = {https://doi.org/10.1177/2053951716646135},
	abstract = {Taking its lead from Ian Hacking’s article ‘How should we do the history of statistics?’, this article reflects on how we might develop a sociologically informed history of Big Data. It argues that within the history of social statistics we have a relatively well developed history of the material phenomenon of Big Data. Yet this article argues that we now need to take the concept of ‘Big Data’ seriously, there is a pressing need to explore the type of work that is being done by that concept. The article suggests a programme for work that explores the emergence of the concept of Big Data so as to track the institutional, organisational, political and everyday adoption of this term. It argues that the term Big Data has the effect of making-up data and, as such, is powerful in framing our understanding of those data and the possibilities that they afford.},
	number = {1},
	journaltitle = {Big Data \& Society},
	author = {Beer, David},
	urldate = {2017-10-20},
	date = {2017},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_Methods},
}

@article{erb_distant_2017,
	title = {Distant Reading and Discourse Analysis},
	volume = {1},
	issn = {2515-2076},
	url = {https://foucaldien.net/articles/10.16995/lefou.16/galley/16/download/},
	doi = {http://doi.org/10.16995/lefou.16},
	abstract = {Can historical discourse analyses be carried out with the aid of computers? In order to examine this question, we compare Franco Moretti's Distant Reading with Foucault's archaeological method. Despite their common origins in the French Annales School, the two approaches differ fundamentally. While Moretti interprets literary data by means of social history, Foucault seeks the immanent meaning of discourses. Our preliminary conclusion: digital archaeology appears to founder on the operationalization of the complex concept of the statement (énoncé).},
	number = {2},
	journaltitle = {Le foucaldien.},
	author = {Erb, Maurice},
	urldate = {2017-10-20},
	date = {2017},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@article{underwood_genealogy_2017,
	title = {A Genealogy of Distant Reading},
	volume = {11},
	url = {http://www.digitalhumanities.org/dhq/vol/11/2/000317/000317.html},
	abstract = {It has recently become common to describe all empirical approaches to literature as subfields of digital humanities. This essay argues that distant reading has a largely distinct genealogy stretching back many decades before the advent of the internet – a genealogy that is not for the most part centrally concerned with computers. It would be better to understand this field as a conversation between literary studies and social science, inititated by scholars like Raymond Williams and Janice Radway, and moving slowly toward an explicitly experimental method. Candor about the social-scientific dimension of distant reading is needed now, in order to refocus a research agenda that can drift into diffuse exploration of digital tools. Clarity on this topic might also reduce miscommunication between distant readers and digital humanists.},
	number = {2},
	journaltitle = {{DHQ}},
	author = {Underwood, Ted},
	urldate = {2017-10-20},
	date = {2017},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_Methods},
}

@article{duffy_who_2016,
	title = {Who needs trust when you know everything? Dealing with information abundance},
	volume = {21},
	issn = {1396-0466},
	url = {http://firstmonday.org/ojs/index.php/fm/article/view/6313},
	doi = {http://dx.doi.org/10.5210/fm.v21i7.6313},
	abstract = {This study finds that trust plays a role in reducing both primary and secondary uncertainty, but not in decision-making. It proposes that trust may be subsumed into information seeking on information-abundant Web sites, and discusses implications for how trust is understood and what it means.},
	number = {7},
	journaltitle = {First Monday},
	author = {Duffy, Andrew Michael},
	urldate = {2017-10-20},
	date = {2016},
	langid = {english},
	keywords = {act\_Collaborating, act\_Modeling, obj\_Artefacts, obj\_Documents, obj\_Infrastructures},
}

@online{noauthor_open_2017,
	title = {The Open Commons of Phenomenology},
	url = {http://ophen.org/},
	abstract = {The Open Commons repository will contain the full corpus of phenomenology by 2020. In June 2017, it numbered 33.000 entries (about 15\% of the total).},
	urldate = {2017-10-20},
	date = {2017},
	langid = {english},
	keywords = {act\_Conversion, act\_Publishing, obj\_Documents},
}

@article{lupton_digital_2017,
	title = {Digital companion species and eating data: Implications for theorising digital data–human assemblages},
	volume = {3},
	url = {http://bds.sagepub.com/cgi/content/abstract/3/1/2053951715619947},
	doi = {https://doi.org/10.1177%2F2053951715619947},
	abstract = {This commentary is an attempt to begin to identify and think through some of the ways in which theory may contribute to understandings of the relationship between humans and digital data. 
I develop an argument that rests largely on the work of two scholars in the field of science and technology studies: Donna Haraway and Annemarie Mol. 
Both authors emphasised materiality and multiple ontologies in their writing. I argue that these concepts have much to offer critical data studies. I employ the tropes of companion species, drawn from Haraway, and eating data, from Mol, and demonstrate how these may be employed to theorise digital data–human assemblages.},
	number = {1},
	journaltitle = {Big Data \& Society},
	author = {Lupton, Deborah},
	date = {2017},
	langid = {english},
	keywords = {act\_Collaborating, act\_Conceptualizing, act\_Identifying, act\_Modeling, obj\_Code, obj\_DigitalHumanities, obj\_Infrastructures},
}

@article{reed_argument_2017,
	title = {The Argument Web: an Online Ecosystem of Tools, Systems and Services for Argumentation},
	volume = {30},
	rights = {oa},
	issn = {2210-5433},
	url = {https://link.springer.com/content/pdf/10.1007%2Fs13347-017-0260-8.pdf},
	doi = {https://doi.org/10.1007/s13347-017-0260-8},
	abstract = {The Argument Web is maturing as both a platform built upon a synthesis of many contemporary theories of argumentation in philosophy and also as an ecosystem in which various applications and application components are contributed by different research groups around the world. It already hosts the largest publicly accessible corpora of argumentation and has the largest number of interoperable and cross compatible tools for the analysis, navigation and evaluation of arguments across a broad range of domains, languages and activity types. Such interoperability is key in allowing innovative combinations of tool and data reuse that can further catalyse the development of the field of computational argumentation. The aim of this paper is to summarise the key foundations, the recent advances and the goals of the Argument Web, with a particular focus on demonstrating the relevance to, and roots in, philosophical argumentation theory.},
	number = {2},
	journaltitle = {Philosophy \& Technology},
	author = {Reed, Chris and et, al},
	urldate = {2017-10-22},
	date = {2017},
	keywords = {act\_Conceptualizing, act\_Modeling, act\_RelationalAnalysis, act\_StructuralAnalysis, obj\_Text},
}

@article{sahle_digital_2015,
	title = {Digital Humanities? Gibt’s doch gar nicht!},
	url = {http://www.zfdg.de/sb001_004},
	abstract = {Die Digital Humanities sind entweder ein Forschungsfeld oder eine Disziplin, möglicherweise auch beides. Sie verfügen jedenfalls über eine gut ausgebaute Infrastruktur der Organisation, Information und Kommunikation und blicken in Bezug auf ihre vielfältigen Forschungsansätze teilweise auf lange Traditionen zurück. Als Bindeglied zwischen den Geisteswissenschaften und der Informatik scheint das Feld heute nicht nur für diese beiden Bereiche, sondern auch für die Organe der Forschungsförderung von besonders hoher Attraktivität zu sein. Neben der Wissenschaft haben selbst die Massenmedien in den letzten Jahren die Digital Humanities entdeckt. Die hohe Anziehungskraft des Feldes hat erfreulich integrative Tendenzen. Allerdings birgt dieser {DH}-›Hype‹ auch Gefahren. Diese reichen von der bloßen Aneignung des Etiketts über explizite Abwehrhaltungen bis hin zu Ignoranz und Verleugnung: »{DH}? Das gibt es doch gar nicht!«},
	journaltitle = {{ZfdG}},
	author = {Sahle, Patrick},
	urldate = {2015-09-16},
	date = {2015},
	keywords = {activity: Assess, obj\_DigitalHumanities},
}

@report{meiners_participatory_2015,
	location = {Göttingen},
	title = {Participatory Design bei der Erstellung einer Virtuellen Forschungsumgebung für die Geschichtswissenschaft},
	url = {urn:nbn:de:gbv:7-dariah-2015-6-6},
	abstract = {Participatory Design ist ein Ansatz aus der Software-Entwicklung, bei dem Benutzer und Entwickler zusammenarbeiten, um sicherzustellen, dass das zu entwickelnde Produkt
den Anforderungen der Zielgruppe entspricht. Ziel ist es, vom Wissen der jeweils anderen Gruppe zu profitieren: Entwickler kennen die Technologien, während auf der anderen Seite die Nutzer über Anwendungswissen und Anforderungen aus dem jeweiligen   Arbeits- oder Forschungsfeld verfügen   und den späteren Anwendungskontext genau beschreiben können.
Im vorliegenden Dokument wird die Anwendung des Participatory Designs im Rahmen der Entwicklung einer Virtuellen Forschungsumgebung ({VRE}) für die
Geschichtswissenschaft gezeigt und die dabei gemachten Erfahrungen vorgestellt.},
	pages = {33},
	number = {13},
	institution = {{DARIAH}-{DE}},
	author = {Meiners, Hanna-Lena and Buddenbohm, Stefan and Thiel, Carsten},
	date = {2015},
	langid = {german},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities, obj\_Infrastructures, obj\_Research},
}

@report{puhl_diskussion_2015,
	location = {Göttingen},
	title = {Diskussion und Definition eines Research Data {LifeCycle} für die digitalen Geisteswissenschaften},
	url = {urn:nbn:de:gbv:7-dariah-2015-4-4},
	abstract = {Das vorliegende Dokument beschreibt den aktuellen Diskussionsstand über ein Referenzmodell eines Forschungsdatenzyklus in den digitalen Geisteswissenschaften. 
Nach einer Bedarfserläuterung
werden Begriffe, Funktionen und Abläufe eines solchen Datenzyklus näher analysiert und definiert.
Das Dokument schließt mit einem   Referenzmodell sowie einem Ausblick auf weiteren Evaluierungsbedarf und daraus resultierende Pläne in dem Forschungsprojekt {DARIAH}-{DE}.},
	pages = {53},
	number = {11},
	institution = {{DARIAH}-{DE}},
	author = {Puhl, Johanna and Andorfer, Peter and Höckendorff, Mareike and Schmunk, Stefan and Stiller, Juliane and Thoden, Klaus},
	date = {2015},
	langid = {german},
	note = {00002},
	keywords = {obj\_DigitalHumanities, obj\_Humanities, obj\_Metadata, obj\_Research},
}

@report{walkowski_digital_2016,
	location = {Göttingen},
	title = {Digital publications beyond digital communication},
	url = {urn:nbn:de:gbv:7-dariah-2016-3-4},
	abstract = {Since more than twenty years different stakeholders involved in scholarly publishing have tried to fundamentally rethink the shape of publications in a digital environment. In contrast to these abundant activities with their highly experimental character the same stakeholders continuously regret that the dominant form of digital publications is still the {PDF}. In the research literature, the situation of digital publications is often compared with the era of the printing press. While this comparison might be
helpful to create awareness about the dimension in which changes are taking place it certainly also blurs significant differences. In the situation that was described before it is even more important to identify the peculiarities of the process of change. Fortunately, the story of digital scholarly publications is long enough to tell it in a way in which these peculiarities become more transparent. The study at hand is an attempt to do so. At its end, it becomes clear that the major issue in digital publishing is not so much connected to the questions how publications look digitally but what we may perceive as a
publication in a digital environment.},
	pages = {23},
	number = {17},
	institution = {{DARIAH}-{DE}},
	author = {Walkowski, Niels-Oliver},
	date = {2016},
	langid = {english},
	note = {00000},
	keywords = {act\_Communicating, act\_Publishing, obj\_DigitalHumanities, obj\_Humanities},
}

@report{andorfer_forschen_2015,
	location = {Göttingen},
	title = {Forschen und Forschungsdaten in den Geisteswissenschaften. Zwischenbericht einer Interviewreihe},
	url = {urn:nbn:de:gbv:7-dariah-2015-3-8},
	abstract = {Im Rahmen der an der Herzog August Bibliothek Wolfenbüttel durchgeführten Interviewreihe:
„Forschen und Forschungsdaten in den Geisteswissenschaften“ wurden {GeistswissenschaftlerInnen} und {StipendiatInnen} zu ihrer Forschungspraxis und ihrem Umgang mit Forschungsdaten befragt. Vorliegender Bericht präsentiert erste Ergebnisse   (Stand Jänner 2015) der auf
http://digital-archiv.at:8081/exist/apps/{DARIAH}-Collection/pages/{InterviewAuswertung}.html
„in Echtzeit“ erfolgenden Auswertung.},
	pages = {18},
	number = {10},
	institution = {{DARIAH}-{DE}},
	author = {Andorfer, Peter},
	date = {2015},
	langid = {german},
	note = {00001},
	keywords = {obj\_DigitalHumanities, obj\_Humanities, obj\_Research},
}

@report{lorenz_interdisziplinare_2015,
	location = {Göttingen},
	title = {Interdisziplinäre E-Publikationen – interdisziplinäre Evaluation? Ein Blick auf die Bewertung fächerübergreifender Forschungsleistungen am Beispiel der Digital Humanities},
	url = {urn:nbn:de:gbv:7-dariah-2015-2-2},
	abstract = {Die Divergenzen zwischen der naturwissenschaftlichen und der geisteswissenschaftlichen Publikationskultur bestehen auch in der elektronischen Publikationspraxis weiter. Mit der zunehmenden interdisziplinären Zusammenarbeit werden die Publikations- und Bewertungsstandards zugleich mehrheitlich als defizitär wahrgenommen. Als eine Bestandsaufnahme skizziert der Beitrag den Entwicklungsstand unterschiedlicher Evalutionsverfahren und gibt im Bereich der Digital Humanities einen Überblick über die verschiedenen internationalen und interdisziplinären Standardisierungsansätze.},
	pages = {11},
	number = {9},
	institution = {{DARIAH}-{DE}},
	author = {Lorenz, Anne Katrin},
	date = {2015},
	note = {00000},
	keywords = {act\_Publishing, obj\_DigitalHumanities, obj\_Humanities},
}

@report{andorfer_forschungsdaten_2015,
	location = {Göttingen},
	title = {Forschungsdaten in den (digitalen) Geisteswissenschaften. Versuch einer Konkretisierung},
	url = {urn:nbn:de:gbv:7-dariah-2015-7-2},
	abstract = {Die Bezeichnung „Forschungsdaten“ zählt zu den Schlüsselbegriffen der Digitalen
Geisteswissenschaften, nicht zuletzt vor den zunehmenden Bemühungen zur Errichtung und Etablierung von Datenzentren, -archiven und Repositorien zur Veröffentlichung und dauerhaften Sicherung von Forschungsdaten. Vorliegendes Papier versucht eine Definition des Begriffes
Forschungsdaten. Im Fokus steht dabei   das Bemühen, diese Bezeichnung im   Vokabular traditionellen geisteswissenschaftlichen Arbeitens   zu verorten. Daran anschließend geht es
außerdem darum, Forschungsdaten von semantisch ähnlichen Bezeichnungen wie Publikation oder Quellen abzugrenzen.},
	pages = {29},
	number = {14},
	institution = {{DARIAH}-{DE}},
	author = {Andorfer, Peter},
	date = {2015},
	langid = {german},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities, obj\_Research},
}

@report{maier_digital_2016,
	location = {Göttingen},
	title = {Digital Humanities und Bibliothek als Kooperationspartner},
	url = {urn:nbn:de:gbv:7-dariah-2016-5-6},
	abstract = {Wissenschaftliche Bibliotheken haben traditionell die Aufgabe, die Wissenschaft hinsichtlich der In-
formationsversorgung und -beschaffung zu unterstützen. Durch die digitalen Entwicklungen und der
Ausdifferenzierung der sogenannten Digital Humanities ({DH}) hat sich das Verständnis dessen, was
Information ist, gewandelt: Das Arbeiten mit digitalen Daten in der Wissenschaft gehört heute zum
Alltag. Hierdurch sind Bibliotheken gefordert, ihr Selbstverständnis und das Aufgabenprofil anzupassen.
In der bibliothekarischen Fachwelt werden seit längerem genau dieses Selbstverständnis sowie das
eigene Berufsbild stark und vor allem kontrovers diskutiert. Diese Diskussion wird aufgegriffen, und
herausgearbeitet, welche Faktoren als Grundlage für die praktische Unterstützung der Digital Humanities
durch wissenschaftliche Bibliotheken geschaffen werden müssen.},
	pages = {29},
	number = {19},
	institution = {{DARIAH}-{DE}},
	author = {Maier, Petra},
	date = {2016},
	langid = {german},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities, obj\_Metadata, obj\_Research},
}

@report{maier_erstellung_2015,
	location = {Göttingen},
	title = {Die Erstellung eines {TEI}-Metadatenschemas für die Auszeichnung von Texten des Klassischen Maya},
	url = {urn:nbn:de:gbv:7-dariah-2015-1-6},
	abstract = {Im Rahmen des 2014 angelaufenen Projektes, Textdatenbank und Wörterbuch des Klassischen Maya ({TWKM}) der Universität Bonn, das die Erforschung und vollständige Entzifferung der Schrift und Sprache des Klassischen Maya zum Ziel hat, werden alle überlieferten Schriftenträger des Klassischen Maya erschlossen und systematisch dokumentiert. Hierfür wurden in einem Teil-Projekt die Inschriftentexte mittels des Datenformats der Text Encoding Initiative ({TEI}) erfasst: Wesentliche Informationen, die durch die Metadaten ausgezeichnet werden mussten, waren die unterschiedlichen Formen und Ausgestaltungen der Inschriftentexte, sowie die Zeitschrift selbst. Das {TEI}-Metadatenschema dieses Teilprojekts stellte eine Arbeitsgrundlage dar, die im weiteren Projektverlauf angepasst und ergänzt werden konnte.},
	pages = {27},
	number = {8},
	institution = {{DARIAH}-{DE}},
	author = {Maier, Petra},
	date = {2015},
	langid = {german},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities, obj\_Metadata, obj\_Text},
}

@report{gnadt_faktoren_2017,
	location = {Göttingen},
	title = {Faktoren und Kriterien für den Impact von {DH}-Tools und Infrastrukturen},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2017-21.pdf},
	abstract = {Der Impact von digitalen Forschungsinfrastrukturen und Tools in der geistes- und kulturwissenschaftli-
chen Forschung ist von zentralem Interesse, wenn es darum geht die Vorteile solcher Entwicklungen zum
einen den {ForscherInnen} selbst, zum anderen aber auch den Förderern zu vermitteln. In diesem Artikel
werden Impactfaktoren und Erfolgskriterien in einem Bewertungskatalog gesammelt und aus Sicht verschiedener Stakeholder, {FachwissenschaftlerInnen}, {DiensteanbieterInnen}, {DiensteentwicklerInnen} und Förderer beurteilt. Dazu werden durchgeführte Umfragen ausgewertet, Kriterien und Maßzahlen aus der Literatur zusammengetragen und die daraus resultierenden Faktoren und Kennzahlen nach eindeutigen Impact-Bereichen katalogisiert. Berücksichtigt werden neben den gängigen quantitativen
Kennzahlen auch qualitative Aspekte, die den Besonderheiten der Geistes- und Kulturwissenschaften Rechnung tragen.},
	pages = {49},
	number = {21},
	institution = {{DARIAH}-{DE}},
	author = {Gnadt, Timo and Schmitt, Viola E. and Stiller, Juliane and Thoden, Klaus},
	date = {2017},
	langid = {german},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities, obj\_Infrastructures, obj\_Tools},
}

@report{bock_einsatz_2016,
	location = {Göttingen},
	title = {Der Einsatz quantitativer Textanalyse in den Geisteswissenschaften: Bericht über den Stand der Forschung},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2016-18.pdf},
	abstract = {Der Beitrag beschreibt den Forschungsstand zum Einsatz quantitativer Verfahren der Textanalyse in den Geisteswissenschaften. Dabei werden für zentrale Verfahren der stilistischen Analyse und themati-
schen Erschließung von Textbeständen jeweils die grundlegenden Konzepte und Intuitionen zu ihrer Wirkungsweise, ihre Anfänge und der heutige Entwicklungsstand beschrieben.},
	pages = {19},
	number = {18},
	institution = {{DARIAH}-{DE}},
	author = {Bock, Sina and Du, Keli and Huber, Michael and Pernes, Stefan and Pielström, Steffen},
	date = {2016},
	langid = {german},
	note = {00000},
	keywords = {Object: Texts, obj\_DigitalHumanities, obj\_Humanities, obj\_Methods, obj\_Text},
}

@report{stollwerk_machbarkeitsstudie_2016,
	location = {Göttingen},
	title = {Machbarkeitsstudie zu Einsatzmöglichkeiten von {OCR}-Software im Bereich 'Alter Drucke' zur Vorbereitung einer vollständigen Digitalisierung deutscher Druckerzeugnisse zwischen 1500 und 1930},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2016-16.pdf},
	abstract = {Forschungsvorhaben sind oft an  spezifische technische Anforderungen  gekoppelt. Diese Studie liefert einen Überblick zu relevanten Anwendungsbereichen der {OCR}, dem Bereich der praxisbezogenen   Massen-{OCR} für Bibliotheken sowie   vergleichbaren  Einrichtungen und dem
anwendungsnahen Teil der {OCR}-Forschung.
Für die systematische und vollständige Digitalisierung von Druckwerken wird in dieser Studie eine Übersicht zur Größe der Bestände bereitgestellt. Zudem wird überprüft, ob mit derzeit marktüblicher Software oder mit derzeit vorhandenen noch nicht marktüblichen Produkten, die
Funktionalität anhand der Anforderungen älterer Drucke angepasst werden kann,   ohne grundlegende Neuentwicklung dabei vorzusehen.
Die Identifizierung von Forschungsfeldern im Bereich der angewandten Informatik, von denen die
Ausdehnung der derzeit möglichen   {OCR}-Verfahren auf bisher nicht behandelbare Korpora zu erwarten ist, wird ebenfalls geleistet. Darüber hinaus werden Forschungsfelder benannt, die eine Ausdehnung auf heute  nicht routinemäßig mit {OCR}-Verfahren behandelbare Schriften  (wie  z.B.
Handschriften) abdecken. 
Im Rahmen von vier Fallstudien sind für diese Zwecke Korpora definiert worden, um im Hinblick auf die Optimierungsmöglichkeiten von {OCR}-Verfahren die Ergebnisse in  Fallstudien zu diskutieren, und um Empfehlungen und Ansätze zu konkretisieren.},
	pages = {97},
	number = {16},
	institution = {{DARIAH}-{DE}},
	author = {Stollwerk, Christoph},
	date = {2016},
	langid = {german},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities},
}
@report{schumacher_big_2016,
	location = {Göttingen},
	title = {Big Data in den Geisteswissenschaften: Konzept für eine Lehr- und Lernmittelsammlung},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2016-15.pdf},
	abstract = {Dieses Working Paper beschreibt die Inhalte, Darstellungsformen und die medialen Umsetzungsmöglichkeiten einer Lehrmittelsammlung zum Thema „Big Data Methodik in den Geistes- und Kulturwissenschaften”. Zudem wird ein   Disseminationskonzept entwickelt, das aufzeigt, auf welche Weise Inhalte, Themen und Instrumente dieses transdisziplinären Bereichs in
den jeweiligen Fachdisziplinen vermittelt werden können. Das hier   vorgestellte Konzept einer Lehr- und Lernmittelsammlung ist auf die Nutzung und Anwendung von Big Data Technologien
und Methoden für geistes- und kulturwissenschaftliche Forschungsfragen ausgerichtet.},
	pages = {26},
	number = {15},
	institution = {{DARIAH}-{DE}},
	author = {Schumacher, Mareike and Held, Marcus and Falk, Claudia and Pernes, Stefan},
	date = {2016},
	langid = {german},
	note = {00000},
	keywords = {obj\_Data, obj\_DigitalHumanities, obj\_Humanities, obj\_Methods},
}

@report{rodriguez_experiments_2014,
	location = {Göttingen},
	title = {Experiments for the design of a help desk system for the {EHRI} project - an Information Retrieval approach},
	url = {http://nbn-resolving.de/urn:nbn:de:gbv:7-dariah-2014-3-2},
	abstract = {This paper describes the experiments realized for the conception of a helpdesk system for the {EHRI} project. The system will help researchers to find Collection Holding Institutions ({CHI}) suitable to support them to resolve questions about Holocaust relevant documentary sources. In this
experiments we model descriptions of archival material in a vector space model, and we show that this model is able to give an answer to real user queries.},
	pages = {14},
	number = {5},
	institution = {{DARIAH}-{DE}},
	author = {Rodriguez, Kepa J.},
	date = {2014},
	langid = {english},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities, obj\_Metadata, t\_InformationRetrieval},
}

@report{klimpel_forschen_2015,
	location = {Göttingen},
	title = {Forschen in der digitalen Welt. Juristische Handreichung für die Geisteswissenschaften},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2015-12.pdf},
	abstract = {Unter dem Schlagwort Digital Humanities finden nicht nur Techniken der quantitativen Analyse vermehrt   Eingang in den Methodenkanon der Geistes- und Kulturwissenschaften, auch ein Umdenken in der Verwaltung von Zugriffs- und Nutzungsrechten wird   erforderlich. Ob die Einführung von Standards sowohl für Zugangsberechtigung zu Forschungsergebnissen als auch für die Verwendung von Forschungsdaten in Form von freien Lizenzen hierfür ein geeignetes Mittel darstellen, soll u. a. mit diesem Dokument eruiert werden. Nach der Einführung woran Rechte entstehen, widmen sich weitere Kapitel dem wissenschaftlichen Arbeiten auf Basis fremder Inhalte, dem wissenschaftlichen Arbeiten als Quelle eigener Rechte, den Rechten der Forschungsinstitution oder Universität, Open Access, sowie der grenzüberschreitenden Forschung und Haftungsfragen.},
	pages = {36},
	number = {12},
	institution = {{DARIAH}-{DE}},
	author = {Klimpel, Paul and Weitzmann, John H.},
	date = {2015},
	langid = {german},
	note = {00000},
	keywords = {Open Access, obj\_DigitalHumanities, obj\_Humanities},
}

@report{beer_datenlizenzen_2014,
	location = {Göttingen},
	title = {Datenlizenzen für geisteswissenschaftliche Forschungsdaten - Rechtliche Bedingungen und Handlungsbedarf},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2014-6.pdf},
	abstract = {Der vorliegende Beitrag schildert den Diskussionsstand zu Lizenzen für geisteswissenschaftliche Forschungsdaten, gibt einen Überblick über deren urheberrechtliche Grundlagen, zeigt Fallbeispiele aus den
Altertumswissenschaften und schließt mit einer Evaluation gängiger Werkzeuge
zur Lizenzerstellung.},
	pages = {49},
	number = {6},
	institution = {{DARIAH}-{DE}},
	author = {Beer, Nikolaos and Herold, Kristin and Heinrich, Maurice and Kolbmann, Wibke and Kollatz, Thomas and Romanello, Matteo and Rose, Sebastian and Schäfer, Felix Falco and Walkowski, Niels-Oliver},
	date = {2014},
	langid = {german},
	note = {00000},
	keywords = {obj\_Data, obj\_DigitalHumanities, obj\_Humanities, obj\_Research},
}

@report{buddenbohm_erfolgskriterien_2014,
	location = {Göttingen},
	title = {Erfolgskriterien für den Aufbau und nachhaltigen Betrieb Virtueller Forschungsumgebungen},
	url = {urn:nbn:de:gbv:7-dariah-2014-5-4},
	abstract = {Virtuelle Forschungsumgebungen ({VREs}) haben sich in vielen Wissenschaftsdisziplinen zu
wichtigen Bestandteilen moderner Forschungsprozesse entwickelt. Dieser gewachsenen Bedeutung müssen die  Betreiber von {VREs} durch funktionierende und effiziente
Verfahren für Aufbau, Betrieb und die Qualitätssicherung gerecht werden. Zur
Unterstützung dieser Prozesse wird ein Lebensphasen-Modell für {VREs} vorgeschlagen, welches insbesondere   auf die erfolgskritischen Punkte für den Übergang in den nachhaltigen Betrieb einer {VRE} eingeht. Im weiteren wird ein Satz von Erfolgskriterien
diskutiert, welches es allen an der {VRE} beteiligten Akteuren (Betreiber, Förderer, Nutzer)ermöglicht, die für ihren Anwendungsfall relevanten Aspekte bereits im Vorfeld der Erstellung einer neuen {VRE} zu identifizieren. Angesichts der Heterogenität von {VREs} wird
dieses Kriterienset im individuellen Fall durch disziplinär spezifische Kriterien ergänzt.
Abgeschlossen werden diese Überlegungen durch Handreichungen zu den
Kostenstrukturen und möglichen Finanzierungsmodellen für {VREs}.},
	pages = {37},
	number = {7},
	institution = {{DARIAH}-{DE}},
	author = {Buddenbohm, Stefan and Enke, Harry and Hofmann, Matthias and Klar, Jochen and Neuroth, Heike and Schwiegelshohn, Uwe},
	date = {2014},
	langid = {german},
	note = {00002},
	keywords = {obj\_DigitalHumanities, obj\_Humanities},
}

@report{beer_interdisciplinary_2014,
	location = {Göttingen},
	title = {Interdisciplinary Interoperability},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2014-3.pdf},
	abstract = {The exchange and reusability of data used for research in the humanities is one of the goals of {DARIAH}. To increase the interoperability of data sets between disciplines we present an overview and recommendations of measures to achieve this. We account for
the finding and fetching of data with legal aspects in mind. This is achieved through standardized methods of discovery and transfer via interfaces on the web. Furthermore, we consider syntactic and semantic interoperability of data for use in different fields of
study. Standardized metadata sets are one way to achieve this and we present some of them in this paper. The importance for scholars to find and be able to process data that is be relevant to their work is the main motivation of this document and for the aspects of the digital humanities covered within. We present options for each of the four aspects that we identified ({APIs} and Protocols, Standards, Identifiers and Licensing).},
	pages = {46},
	number = {3},
	institution = {{DARIAH}-{DE}},
	author = {Beer, Nikolaos and Herold, Kristin and Kolbmann, Wibke and Kollatz, Thomas and Romanello, Matteo and Rose, Sebastian and Walkowski, Niels-Oliver},
	date = {2014},
	langid = {english},
	note = {00002},
	keywords = {obj\_DigitalHumanities, obj\_Humanities},
}

@report{jannidis_dariah-dkpro-wrapper_2016,
	location = {Göttingen},
	title = {{DARIAH}-{DKPro}-Wrapper Output Format ({DOF}) Specification},
	url = {urn:nbn:de:gbv:7-dariah-2016-6-2},
	abstract = {The {DARIAH}-{DKPro}-Wrapper Output Format ({DOF}) is a tab-separated file format, designed to be easily accessible by various analysis tools and scripting languages. It is based on a modular linguistic processing pipeline, which includes a range of analysis capabilities and has been fitted to book-length documents. Both processing pipeline and output format have been developed in cooperation by
the Department of Literary Computing, Würzburg (“Lehrstuhl Für Computerphilologie Und Neuere
Deutsche Literaturgeschichte. Universität Würzburg”
2016) and the Ubiquitous Knowledge Processing Lab, Darmstadt (“Ubiquitous Knowledge Processing Lab. Technische Universität Darmstadt” 2016) as
part of {DARIAH}-{DE}, Digital Research Infrastructure for the Arts and Humanities (“{DARIAH}-{DE}” 2016].},
	pages = {14},
	number = {20},
	institution = {{DARIAH}-{DE}},
	author = {Jannidis, Fotis and Pernes, Stefan and Pielström, Steffen and Reger, Isabella and Reimers, Nils and Vitt, Thorsten},
	date = {2016},
	langid = {english},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities},
}

@report{sahle_dh_2013,
	location = {Göttingen},
	title = {{DH} Studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-1.pdf},
	abstract = {Der Status der Digital Humanities als eigenständiger wissenschaftlicher Bereich ist seit
langem in der Diskussion. Ein wichtiger Aspekt in der fortschreitenden Verfestigung als
Disziplin oder Fach ist die Vermittlung von {DH} in der Lehre. Dies geschieht bereits auf allen Ebenen der Ausbildung: von einzelnen Kursen und Modulen, über angestimmte Angebote, Zertifikate und Summer Schools, bis hin zu {BA}-, {MA}- und Promotionsstudiengängen.
Für den Erfolg dieser Lehrprogramme, für die Verwertbarkeit der Abschlüsse für die Studierenden und die Attraktivität der Absolventen auf dem Arbeitsmarkt ist ihre
Erkennbarkeit als {DH}-Ausbildungen mit ganz bestimmten Lehrinhalten und Zielkompetenzen von besonderer Bedeutung. Kern- und Referenzcurricula für die verschiedenen
Spielarten von {DH}-Programmen wären ein geeignetes Mittel, um die Kohärenz der
Ausbildungen zu verbessern und sichtbar zu machen. Auf dem Weg dorthin werden
zunächst eine empirische Sichtung der bestehenden Angebote, ein analytisches Raster zu ihrer Untersuchung, ein Modell der verschiedenen Grundtypen, eine erste Zusammenstellung der typischen Lehrinhalte und Zielkompetenzen sowie Überlegungen zum
Aufbau neuer Studienprogramme benötigt. Das vorliegende Papier versucht in diesem Sinne die Grundlagen für gemeinsame Kern- und Referenzcurricula zu legen, die dann innerhalb der Fachgemeinschaft der {DH}-Ausbildung weiter diskutiert werden müssen.
Der vorliegende Bericht enthält außerdem im Anhang ein Konzeptpapier von Manfred Thaller: “Vorüberlegungen zu einem Referenzcurriculum, das Zwiebelschalenmodell und
der Nürnberger Informatikkern”.},
	pages = {39},
	number = {1},
	institution = {{DARIAH}-{DE}},
	author = {Sahle, Patrick},
	date = {2013},
	langid = {german},
	note = {00002},
	keywords = {obj\_DigitalHumanities, obj\_Humanities},
}

@report{schoch_quantitative_2013,
	location = {Göttingen},
	title = {Quantitative Text Analysis for Literary History – Report on a {DARIAH}-{DE} Expert Workshop},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-2.pdf},
	abstract = {The workshop on Quantitative Text Analysis for Literary History was the first in a series of {DARIAH}-{DE} expert workshops and took place from November 22 to 23 at the University of Würzburg, Germany. It brought together experts in the computational analysis of collections of literary texts from France, Germany, Poland and the {US}. This report provides some context on the {DARIAH} expert workshops and inroduces the specific goals of the workshop reported on here. Then, it summarizes the major issues raised by the participants in their initial statements and debated in the ensuing discussions. Finally, it describes the key results from the workshop, which include advances int he areas of data, tools and methods for quantitative text analysis.},
	pages = {13},
	number = {2},
	institution = {{DARIAH}-{DE}},
	author = {Schöch, Christof and Jannidis, Fotis},
	date = {2013},
	langid = {english},
	note = {00001},
	keywords = {goal\_Analysis, obj\_DigitalHumanities, obj\_Humanities},
}

@book{bartscherer_switching_nodate,
	title = {Switching Codes},
	url = {http://www.press.uchicago.edu/ucp/books/book/chicago/S/bo6027946.html},
	abstract = {Half a century into the digital era, the profound impact of information technology on intellectual and cultural life is universally acknowledged but still poorly understood. The sheer complexity of the technology coupled with the rapid pace of change makes it increasingly difficult to establish common ground and to promote thoughtful discussion. Responding to this challenge, Switching Codes brings together leading American and European scholars, scientists, and artists—including Charles Bernstein, Ian Foster, Bruno Latour, Alan Liu, and Richard Powers—to consider how the precipitous growth of digital information and its associated technologies are transforming the ways we think and act. Employing a wide range of forms, including essay, dialogue, short fiction, and game design, this book aims to model and foster discussion between {IT} specialists, who typically have scant training in the humanities or traditional arts, and scholars and artists, who often understand little about the technologies that are so radically transforming their fields. Switching Codes will be an indispensable volume for anyone seeking to understand the impact of digital technology on contemporary culture, including scientists, educators, policymakers, and artists, alike.},
	author = {Bartscherer, Thomas},
	urldate = {2017-05-03},
	note = {00037},
}

@incollection{viprey_philologie_2005,
	location = {Geneva},
	title = {Philologie numérique et herméneutique intégrative},
	pages = {51--68},
	booktitle = {Sciences du texte et analyse de discours : enjeux d’une interdisciplinarité},
	publisher = {Slatkine},
	author = {Viprey, Jean-Marie},
	editor = {Adam, Jean-Michel and Heidmann, Ute},
	date = {2005},
	note = {00035},
}

@article{mayaffre_lhermeneutique_2002,
	title = {L’herméneutique numérique},
	url = {http://halshs.archives- ouvertes.fr/hal-00586512/},
	pages = {1--11},
	issue = {numéro spécial},
	journaltitle = {L'Astrolabe. Recherche littéraire et informatique},
	author = {Mayaffre, Damon},
	date = {2002},
	note = {00020},
}

@incollection{mayaffre_philologie_2007,
	location = {Toulouse},
	title = {Philologie et/ou herméneutique numérique : nouveaux concepts pour de nouvelles pratiques ?},
	url = {http://www.revue-texto.net/Parutions/Livres-E/Albi-2006/Mayaffre.pdf},
	booktitle = {Corpus en Lettres et Sciences sociales. Des documents numériques à l'interprétation, actes du {XXVIIe} colloque d’Albi},
	publisher = {Presses Univ. du Mirail},
	author = {Mayaffre, Damon},
	editor = {Ballabriga, Michel and Rastier, François},
	date = {2007},
	note = {00010},
}

@article{loescher_digital_2017,
	location = {Berlin},
	title = {Digital Humanities. Garagenbastler der Geisteswissenschaften},
	url = {http://www.tagesspiegel.de/wissen/digital-humanities-garagenbastler-der-geisteswissenschaften/19721710.html},
	abstract = {Den Digital Humanities fehlt ein sinnstiftendes Manifest: Es gilt, die Lücke zwischen Datensammlern und Gelehrten zu schließen. Ein Gastbeitrag.},
	journaltitle = {Tagesspiegel},
	author = {Loescher, Jens},
	date = {2017-05-01},
	note = {00000},
}

@book{hahn_dh-handbuch_2015,
	location = {Göttingen},
	title = {{DH}-Handbuch /Version 1.0},
	rights = {https://creativecommons.org/licenses/by/4.0/deed.de},
	isbn = {{ISBN} 978-3-7375-6818-0},
	url = {https://handbuch.tib.eu/w/DH-Handbuch},
	abstract = {Ziel dieses Buchs ist, einen konzentrierten Überblick über das Feld der Digital Humanities ({DH}) anzubieten. Für Einsteiger und mögliche {AntragstellerInnen} stellen sich häufig die folgenden Fragen:

    Was sind die Digital Humanities?
    Was sind relevante Forschungsfragen?
    Mithilfe welcher Tools lassen sich fachspezifische, aber auch fächerübergreifende Fragen beantworten?
    Was müssen Geisteswissenschaftler beim Umgang mit Daten beachten?
    Wie sehen erfolgreiche Projekte in den Digital Humanities aus? 

Neben Lösungswegen und Ressourcen zu typischen Fragen werden auch Projekte und Werkzeuge detailliert vorgestellt, um vorhandene Kenntnisse aufzufrischen und neue Aspekte der Digital Humanities kennenzulernen. Die Nähe zur fachwissenschaftlichen Praxis steht dabei im Vordergrund. Wir hoffen, mit diesem Handbuch auch Einsteigern die Digital Humanities nahebringen zu können und die Neugierde auf digitale Methoden und deren Möglichkeiten für die geisteswissenschaftliche Forschung zu wecken.},
	pagetotal = {131},
	publisher = {{DARIAH}-{DE}},
	author = {Hahn, Helene and Kalman, Tibor and Kolbmann, Wibke and Kollatz, Thomas and Neuschäfer, Steffen and Pielström, Steffen and Puhl, Johanna and Stiller, Juliane and Tonne, Danah},
	date = {2015},
	langid = {german},
	note = {pdf: http://bit.do/{DH}-Handbuch},
	keywords = {act\_Communicating, obj\_DigitalHumanities},
}

@online{j.w.s.w._revenge_2017,
	title = {Revenge of the maths mob. Why literature is the ultimate big-data challenge},
	url = {http://www.economist.com/blogs/prospero/2017/03/revenge-maths-mob},
	titleaddon = {Prospero (The Economist)},
	author = {{J.W.S.W.}},
	date = {2017},
}

@article{dignazio_feminist_nodate,
	title = {Feminist Data Visualization},
	volume = {{IEEE} {VIS} 2017, October 1-6, 2017},
	url = {http://www.kanarinka.com/wp-content/uploads/2015/07/IEEE_Feminist_Data_Visualization.pdf},
	abstract = {In this paper, we begin to outline how feminist theory may be productively applied to information visualization research and practice. Other technology- and design-oriented fields such as Science and Technology Studies, Human-Computer Interaction, Digital Humanities, and Geography/{GIS} have begun to incorporate feminist principles into their research. Feminism is not (just) about women, but rather draws our attention to questions of epistemology – who is included in dominant ways of producing and communicating knowledge and whose perspectives are marginalized. We describe potential applications of feminist theory to influence the information design process as well as to shape the outputs from that process.},
	pages = {5},
	issue = {http://ieeevis.org/ (tbd)},
	journaltitle = {{IEEE} Vis conference on Feminist Data Visualization},
	author = {D'Ignazio, Catherine and Klein, Lauren F.},
	langid = {english},
	note = {einführend:
https://civic.mit.edu/feminist-data-visualization},
	keywords = {act\_Collaborating, act\_Visualizing, activity: Assess, activity: Gather, activity: Reflect, obj\_Data/Databases, obj\_DigitalHumanities, obj\_Humanities, obj\_Maps},
}

@article{mauch_evolution_2015,
	title = {The evolution of popular music: {USA} 1960-2010},
	volume = {2},
	issn = {2054-5703},
	url = {http://rsos.royalsocietypublishing.org/cgi/doi/10.1098/rsos.150081},
	doi = {10.1098/rsos.150081},
	shorttitle = {The evolution of popular music},
	pages = {150081--150081},
	number = {5},
	journaltitle = {Royal Society Open Science},
	author = {Mauch, M. and {MacCallum}, R. M. and Levy, M. and Leroi, A. M.},
	urldate = {2017-01-17},
	date = {2015-05-06},
	langid = {english},
	keywords = {act\_StylisticAnalysis, goal\_Analysis, obj\_Music},
}

@book{burnard_quest-ce_2015,
	location = {Marseille},
	title = {Qu'est-ce que la Text Encoding Initiative?},
	rights = {{CC} {BY}-{NC}-{ND} 3.0},
	isbn = {978-2-8218-5581-6},
	url = {http://books.openedition.org/oep/1297},
	series = {Encyclopédie numérique},
	publisher = {{OpenEdition} Press},
	author = {Burnard, Lou},
	urldate = {2016-12-19},
	date = {2015-10-28},
	langid = {french},
	keywords = {x\_astree},
}

@online{mani_how_2016,
	title = {How {AI} is revolutionising the role of the literary critic – Inderjeet Mani {\textbar} Aeon Essays},
	url = {https://aeon.co/essays/how-ai-is-revolutionising-the-role-of-the-literary-critic},
	abstract = {Artificial intelligence sheds new light on classic texts. Literary theorists who don’t embrace it face obsolescence},
	titleaddon = {Aeon},
	author = {Mani, Inderjeet},
	urldate = {2016-12-06},
	date = {2016},
}

@article{greenwood_how_2016,
	title = {How Well Can Computers Read Fiction?},
	issn = {1072-7825},
	url = {https://www.theatlantic.com/technology/archive/2016/11/how-well-can-computers-read-fiction/508133/},
	abstract = {Computational tools have the ability to analyze books’ emotional arcs, but it’s unclear what they can really find out about literature.},
	journaltitle = {The Atlantic},
	author = {Greenwood, Veronique},
	urldate = {2016-11-27},
	date = {2016-11-18},
}

@article{bouchet_luniversite_2016,
	title = {L’Université face au déferlement numérique},
	rights = {Les ami•e•s de Variations},
	issn = {1968-3960},
	url = {https://variations.revues.org/740},
	doi = {10.4000/variations.740},
	abstract = {L’université française et la science qu’elle produit connaissent des mutations rapides. Elles sont lancées dans une course effrénée à l’innovation, sans cesse stimulée par les injonctions de l’État et des milieux économiques, ainsi que par la mode des classements internationaux, tel celui de Shanghai (Charle, Soulié, 2008). Depuis les années 1980, les innovations et les trajectoires technoscientifiques sont de plus en plus modelées par un nouveau régime de production néolibéral des sciences a...},
	number = {19},
	journaltitle = {Variations. Revue internationale de théorie critique},
	author = {Bouchet, Thomas and Carnino, Guillaume and Jarrige, François},
	urldate = {2016-11-25},
	date = {2016-04-07},
	langid = {french},
}

@article{winterhalter_licence_2016,
	title = {Licence to Mine? Ein Überblick über Rahmenbedingungen von Text and Data Mining und den aktuellen Stand der Diskussion},
	volume = {4},
	rights = {Copyright (c) 2016 027.7 Zeitschrift für Bibliothekskultur / Journal for Library Culture},
	issn = {2296-0597},
	url = {http://0277.ch/ojs/index.php/cdrs_0277/article/view/153},
	doi = {10.12685/027.7-4-2-153},
	shorttitle = {Licence to Mine?},
	abstract = {Der Artikel gibt einen Überblick über die Möglichkeiten der Anwendung von Text and Data Mining ({TDM}) und ähnlichen Verfahren auf der Grundlage bestehender Regelungen in Lizenzverträgen zu kostenpflichtigen elektronischen Ressourcen, die Debatte über zusätzliche Lizenzen für {TDM} am Beispiel von Elseviers {TDM} Policy und den Stand der Diskussion über die Einführung von Schrankenregelungen im Urheberrecht für {TDM} zu nichtkommerziellen wissenschaftlichen Zwecken.The article gives a survey about the potential application of text and data mining ({TDM}) or similar techniques on the basis of given licence agreements for subscription-based electronic resources. It also resumes the debate about the supplemental licence amendments for {TDM} that has arisen over the introduction of Elsevier’s {TDM} Policy. Finally, it describes the current discussions about the possible implementation of copyright exemptions for {TDM} within the context of non-commercial scientific research.},
	pages = {48--59},
	number = {2},
	journaltitle = {027.7 Zeitschrift für Bibliothekskultur / Journal for Library Culture},
	author = {Winterhalter, Christian},
	urldate = {2016-11-11},
	date = {2016-11-11},
	langid = {german},
}
@article{reichwein_distant_2016,
	title = {Distant Reading. Wie Big Data die Literaturwissenschaften erobert},
	url = {https://www.welt.de/wissenschaft/article156735737/Wie-Big-Data-die-Literaturwissenschaften-erobert.html},
	journaltitle = {welt.de},
	author = {Reichwein, Marc},
	date = {2016-07-06},
}

@article{blei_probabilistic_2012,
	title = {Probabilistic topic models},
	volume = {55},
	issn = {00010782},
	url = {http://cacm.acm.org/magazines/2012/4/147361-probabilistic-topic-models/fulltext},
	doi = {10.1145/2133806.2133826},
	abstract = {As our collective knowledge continues to be digitized and stored—in the form of news, blogs, Web pages, scientific articles, books, images, sound, video, and social networks—it becomes more difficult to find and discover what we are looking for. We need new computational tools to help organize, search, and understand these vast amounts of information.},
	pages = {77--84},
	number = {4},
	journaltitle = {Communications of the {ACM}},
	author = {Blei, David M.},
	urldate = {2012-04-25},
	date = {2012-04-01},
	langid = {english},
	keywords = {bigdata, meta\_GiveOverview},
}

@article{breitling_bamberg_2011,
	title = {Bamberg vierdimensional - Ausbau und Ergänzung des digitalen Planungsmodells durch die Rekonstruktion der mittelalterlichen Stadt},
	volume = {17},
	url = {http://www.uni-bamberg.de/fileadmin/uni/verwaltung/presse/Publikationen/uni.vers/univers_forschung_2011/03_Bamberg_vierdimensional.pdf},
	abstract = {Seit der Gründung des Bistums Bamberg im Jahre 1007 hat die Domstadt wechselvolle und
ereignisreiche Zeiten durchlebt. Das Projekt „4D-Stadtmodell Bamberg um 1300“ möchte ein
Stück dieser Stadtgeschichte wieder lebendig werden lassen – und regt damit nicht nur zu
einer neuen Auseinandersetzung mit dem baulichen Kulturerbe Bambergs an. Es hilft auch
bei der touristischen Erschließung der Eigenschaften, die das Städtchen an der Regnitz zum
{UNESCO}-Welterbe gemacht haben.},
	pages = {6--10},
	issue = {Mai 2011},
	journaltitle = {uni.vers, Magazin der Otto-Friedrich-Universität Bamberg},
	author = {Breitling, Stefan and Schramm, Karl-Heinz},
	date = {2011},
	langid = {german},
	keywords = {act\_Visualizing, obj\_Architecture},
}

@article{gotz_wie_2014,
	title = {Wie die {IT} die Geisteswissenschaften verändert},
	url = {http://www.deutschlandfunk.de/forschungsmethoden-wie-die-it-die-geisteswissenschaften.1148.de.html?dram:article_id=292172},
	journaltitle = {Deutschlandfunk: Aus Kultur- und Geisteswissenschaften},
	author = {Götz, Eva-Maria},
	date = {2014},
}

@article{argamon_interpreting_2008,
	title = {Interpreting Burrows's Delta: Geometric and Probabilistic Foundations},
	volume = {23},
	url = {http://llc.oxfordjournals.org/content/23/2/131.abstract},
	doi = {10.1093/llc/fqn003},
	shorttitle = {Interpreting Burrows's Delta},
	abstract = {While Burrows's intuitive and elegant ‘Delta’ measure for authorship attribution has proven to be extremely useful for authorship attribution, a theoretical understanding of its operation has remained somewhat obscure. In this article, I address this issue by introducing a geometric interpretation of Delta, which further allows us to interpret Delta as a probabilistic ranking principle. This interpretation gives us a better understanding of the method's fundamental assumptions and potential limitations, as well as leading to several well-founded variations and extensions.},
	pages = {131 --147},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Argamon, Shlomo},
	urldate = {2011-07-26},
	date = {2008},
	langid = {english},
	keywords = {*****, t\_Stylometry},
}

@book{baayen_word_2001,
	location = {Dordrecht; Boston},
	title = {Word frequency distributions},
	isbn = {0-7923-7017-1 978-0-7923-7017-8 1-4020-0927-5 978-1-4020-0927-3},
	abstract = {This book is a comprehensive introduction to the statistical analysis of word frequency distributions, intended for computational linguists, corpus linguists, psycholinguists, and researchers in the field of quantitative stylistics. It aims to make these techniques more accessible for non-specialists, both theoretically, by means of a careful introduction to the underlying probabilistic and statistical concepts, and practically, by providing a program library implementing the main models for word frequency distributions.},
	publisher = {Kluwer Academic},
	author = {Baayen, R. Harald},
	date = {2001},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@book{bader_simson_1991,
	location = {Tübingen},
	title = {Simson bei Delila: computerlinguistische Interpretation des Textes Ri 13 - 16},
	volume = {3},
	isbn = {3-7720-1952-8},
	series = {Textwissenschaft, Theologie, Hermeneutik, Linguistik, Literaturanalyse, Informatik},
	pagetotal = {468},
	publisher = {Francke},
	author = {Bader, Winfried},
	date = {1991},
	langid = {german},
	note = {{PhD} Thesis: Tübingen, 1989.},
	keywords = {goal\_Analysis, goal\_Interpretation},
}

@book{aiden_uncharted:_2013,
	location = {New York},
	title = {Uncharted: big data as a lens on human culture},
	isbn = {978-1-59448-745-3},
	shorttitle = {Uncharted},
	abstract = {"One of the most exciting developments from the world of ideas in decades, presented with panache by two frighteningly brilliant, endearingly unpretentious, and endlessly creative young scientists." - Steven Pinker, author of The Better Angels of Our Nature Our society has gone from writing snippets of information by hand to generating a vast flood of 1s and 0s that record almost every aspect of our lives: who we know, what we do, where we go, what we buy, and who we love. This year, the world will generate 5 zettabytes of data. (That's a five with twenty-one zeros after it.) Big data is revolutionizing the sciences, transforming the humanities, and renegotiating the boundary between industry and the ivory tower. What is emerging is a new way of understanding our world, our past, and possibly, our future. In Uncharted, Erez Aiden and Jean-Baptiste Michel tell the story of how they tapped into this sea of information to create a new kind of telescope: a tool that, instead of uncovering the motions of distant stars, charts trends in human history across the centuries. By teaming up with Google, they were able to analyze the text of millions of books. The result was a new field of research and a scientific tool, the Google Ngram Viewer, so groundbreaking that its public release made the front page of The New York Times, The Wall Street Journal, and The Boston Globe, and so addictive that Mother Jones called it "the greatest timewaster in the history of the internet." Using this scope, Aiden and Michel-and millions of users worldwide-are beginning to see answers to a dizzying array of once intractable questions. How quickly does technology spread? Do we talk less about God today? When did people start "having sex" instead of "making love"? At what age do the most famous people become famous? How fast does grammar change? Which writers had their works most effectively censored by the Nazis? When did the spelling "donut" start replacing the venerable "doughnut"? Can we predict the future of human history? Who is better known-Bill Clinton or the rutabaga? All over the world, new scopes are popping up, using big data to quantify the human experience at the grandest scales possible. Yet dangers lurk in this ocean of 1s and 0s-threats to privacy and the specter of ubiquitous government surveillance. Aiden and Michel take readers on a voyage through these uncharted waters"--},
	pagetotal = {288},
	publisher = {Riverhead Books},
	author = {Aiden, Erez and Michel, Jean-Baptiste},
	date = {2013},
	keywords = {bigdata, goal\_Analysis, obj\_Data, obj\_Text},
}

@book{adolphs_introducing_2006,
	location = {London \& New York},
	title = {Introducing electronic text analysis : a practical guide for language and literary studies},
	isbn = {978-0-415-32021-4},
	url = {http://www.routledge.com/textbooks/0415320216},
	shorttitle = {Introducing electronic text analysis},
	abstract = {Introducing Electronic Text Analysis is a practical and much needed introduction to corpora - bodies of linguistic data. Written specifically for students studying this topic for the first time, the book begins with a discussion of the underlying principles of electronic text analysis. It then examines how these corpora enhance our understanding of literary and non-literary works. In the first section the author introduces the concepts of concordance and lexical frequency, concepts which are then applied to a range of areas of language study. Key areas examined are the use of on-line corpora to complement traditional stylistic analysis, and the ways in which methods such as concordance and frequency counts can reveal a particular ideology within a text. Presenting an accessible and thorough understanding of the underlying principles of electronic text analysis, the book contains abundant illustrative examples and a glossary with definitions of main concepts. It will also be supported by a companion website with links to on-line corpora so that students can apply their knowledge to further study. The accompanying website to this book can be found at http://www.routledge.com/textbooks/0415320216},
	publisher = {Routledge},
	author = {Adolphs, Svenja},
	date = {2006},
	keywords = {act\_ContentAnalysis, goal\_Analysis, meta\_GiveOverview},
}

@online{anderson_end_2008,
	title = {The End of Theory: The Data Deluge Makes the Scientific Method Obsolete},
	url = {http://www.wired.com/science/discoveries/magazine/16-07/pb_theory},
	shorttitle = {The End of Theory},
	abstract = {Get the latest in science news, including space, physics, planet earth, discoveries, {NASA}, satellites, and space travel from Wired.com},
	titleaddon = {{WIRED}},
	author = {Anderson, Chris},
	urldate = {2013-07-24},
	date = {2008},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Research},
}

@online{gabasova_star_2015,
	title = {The Star Wars social network},
	url = {http://evelinag.com/blog/2015/12-15-star-wars-social-network},
	titleaddon = {Evelina Gabasova's Blog},
	author = {Gabasova, Evelina},
	date = {2015},
	keywords = {act\_NetworkAnalysis, goal\_Analysis, obj\_Video},
}

@article{urbanus_reading_2015,
	title = {Reading the Invisible Ink},
	url = {http://archaeology.org/issues/202-1601/trenches/3948-trenches-martellus-map},
	journaltitle = {Archaeology},
	author = {Urbanus, Jason},
	date = {2015},
	keywords = {act\_DataRecognition, goal\_Analysis, obj\_Maps, obj\_Object},
}

@article{david_m._berry_post-digital_nodate,
	title = {Post-Digital Humanities: Computation and Cultural Critique in the Arts and Humanities},
	volume = {vol. 49},
	url = {http://er.educause.edu/articles/2014/5/postdigital-humanities-computation-and-cultural-critique-in-the-arts-and-humanities},
	abstract = {Today we live in computational abundance whereby our
everyday lives and the environment that surrounds us
are suffused with digital technologies. This is a world of
anticipatory technology and contextual computing that
uses smart diffused computational processing to create a
fine web of computational resources that are embedded into
the material world. Thus, the historical distinction between
the digital and the non-digital becomes increasingly blurred,
to the extent that to talk about the digital presupposes an experiential
disjuncture that makes less and less sense.},
	issue = {no. 3 (May/June 2014)},
	journaltitle = {{EduCauseReview}},
	author = {{David M. Berry}},
	urldate = {2015-09-29},
	langid = {american},
	keywords = {Object: Digital Humanities, activity: Assess},
}

@online{inke_geoffrey_rockwell_stefan_sinclair_et_al.:http://www.artsrn.ualberta.ca/inke/_digital_nodate,
	title = {Digital Humanities Pedagogy: Encouraging Critical Analysis of Digital Technology in the Classroom},
	url = {http://inke.ca/2015/01/15/digital-humanities-pedagogy-encouraging-critical-analysis-of-digital-technology-in-the-classroom/},
	abstract = {In the words of Paul Fyfe, “the goal is to keep students’ attention on the critical labor that digital resources seem to dissolve” in order to enable students to both use digital tools productively for education as well as making them conscientious users of technology in general. I now think that there will be a significant learning curve for students in Digital Humanities classrooms. Critical thought is not something which is encouraged by mobile apps or social media platforms, and in fact, their ease of use not only does away with the need for instructions of any kind but also with analysis. Beginning to reflect on something which is, for this generation of students, ubiquitous will feel somewhat counter-intuitive. Using it for educational purposes will be even more so. But after having considered the question, I believe that it is imperative that students are empowered with the educational tools necessary to be discerning data consumers who use digital tools as a means for contemplation as well as information.},
	author = {{INKE (Geoffrey Rockwell, Stéfan Sinclair et al.:http://www.artsrn.ualberta.ca/inke/)}},
	urldate = {2015-09-29},
	keywords = {Object: Digital Humanities, activity: Assess, object: Infrastructure},
}

@article{marwick_reproducibility:_nodate,
	title = {Reproducibility:  How computers broke science – and what we can do to fix it},
	rights = {Creative Commons Attribution {NoDerivatives}},
	url = {https://theconversation.com/how-computers-broke-science-and-what-we-can-do-to-fix-it-49938},
	abstract = {Reproducibility is one of the cornerstones of science. Made popular by British scientist Robert Boyle in the 1660s, the idea is that a discovery should be reproducible before being accepted as scientific knowledge.

In essence, you should be able to produce the same results I did if you follow the method I describe when announcing my discovery in a scholarly publication. Since the introduction of the personal computer – and the point-and-click software programs that have evolved to make it more user-friendly – reproducibility of much research has become questionable, if not impossible. Too much of the research process is now shrouded by the opaque use of computers that many researchers have come to depend on. This makes it almost impossible for an outsider to recreate their results.

Recently, several groups have proposed similar solutions to this problem. Together they would break scientific data out of the black box of unrecorded computer manipulations so independent readers can again critically assess and reproduce results. Researchers, the public, and science itself would benefit.},
	journaltitle = {the conversation},
	author = {Marwick, Ben},
	langid = {english},
	keywords = {Object: Code, activity: Assess},
}

@article{blanke_big_nodate,
	title = {The (Big) Data-security assemblage: Knowledge and critique},
	volume = {2 / 2015 (July - December 2015)},
	issn = {20539517},
	url = {http://bds.sagepub.com/content/2/2/2053951715609066.full.pdf+html},
	doi = {10.1177/2053951715609066, Oct 2015},
	abstract = {The Snowden revelations and the emergence of ‘Big Data’ have rekindled questions about how security practices are deployed in a digital age and with what political effects. While critical scholars have drawn attention to the social, political and legal challenges to these practices, the debates in computer and information science have received less analytical attention. This paper proposes to take seriously the critical knowledge developed in information and computer science and reinterpret their debates to develop a critical intervention into the public controversies concerning data-driven security and digital surveillance. The paper offers a two-pronged contribution: on the one hand, we challenge the credibility of security professionals’ discourses in light of the knowledge that they supposedly mobilize; on the other, we argue for a series of conceptual moves around data, human–computer relations, and algorithms to address some of the limitations of existing engagements with the Big Data-security assemblage.},
	journaltitle = {Big Data \& Society},
	author = {Blanke, Tobias and Aradau, Claudia},
	keywords = {Object: Data, Object: Metadata, Object: People, Object: Texts, activity: Assess},
}

@book{frabetti_software_nodate,
	title = {Software Theory. A Cultural and Philosophical Study.},
	isbn = {1783481978},
	url = {http://www.rowmaninternational.com/books/software-theory},
	series = {Media Philosophy},
	abstract = {This book engages directly in close readings of technical texts and computer code in order to show how software works. It offers an analysis of the cultural, political, and philosophical implications of software technologies that demonstrates the significance of software for the relationship between technology, philosophy, culture, and society.},
	pagetotal = {220},
	publisher = {Rowman \& Littlefield},
	author = {Frabetti, Frederica},
	langid = {english},
	keywords = {activity: Assess, obj\_Code},
}

@article{osborne_special_nodate,
	title = {Special Issue: Transdisciplinary Problematics},
	volume = {32 (5-6)},
	issn = {02632764},
	url = {http://tcs.sagepub.com/content/32/5-6.toc},
	abstract = {The articles situate current debates about transdisciplinarity within the deeper history of academic disciplinarity, in its difference from the notions of inter- and multi- disciplinarity. It offers a brief typology and history of established conceptions of transdisciplinarity within science and technology studies. It then goes on to raise the question of the conceptual structure of transdisciplinary generality in the huma- nities, with respect to the incorporation of the 19th- and 20th-century German and French philosophical traditions into the anglophone humanities, under the name of ‘theory’. It identifies two distinct – dialectical and anti-dialectical, or dialectical and transversal – transdisciplinary trajectories. It locates the various contributions to the special issue of which it is the introduction within this conceptual field, drawing attention to the distinct contribution of the French debates about structuralism and its aftermath – those by Serres, Foucault, Derrida, Guattari and Latour, in particular. It concludes with an appendix on Foucault’s place within current debates about disciplinarity and academic disciplines.},
	issue = {September-November 2015},
	journaltitle = {Theory, Culture \& Society},
	author = {Osborne, Peter},
	langid = {english},
	keywords = {activity: Assess, obj\_DigitalHumanities},
}

@article{presner_critical_2012,
	title = {Critical Theory and the Mangle of Digital Humanities},
	url = {http://www.toddpresner.com/wp-content/uploads/2012/09/Presner_2012_DH_FINAL.pdf},
	abstract = {As the various fields of the digital humanities have begun to mature and gain institutional traction, a debate has started to coalesce around the relationship between the "critical" function of the humanities and the "building" and "making" claims of the digital humanities (Ramsay; Mandell; Liu 2011 and 2012). While "making" was obviously central to the formation of the artifacts and objects of study in the humanities (whether musical compositions, works of art, literary texts, or films), the institutional and disciplinary formations of the humanities have largely focused their intellectual energies on criticism and interpretation. It would not be much of an exaggeration to say this has been true of humanistic fields of inquiry for centuries, if not millennia, ranging from Platonic impulses to unveil illusions and Enlightenment ideals of critical rationality to the Marxist-inflected social and cultural criticism of the Frankfurt School, not to mention more contemporary modes of deconstructive critique in fields such as post-colonialism, feminism, critical race theory, and cultural studies},
	pages = {20},
	journaltitle = {The Humanities and the Digital (ed. David Theo Goldberg and Patrik Svensson), {MIT} Press},
	author = {Presner, Todd},
	date = {2012},
	langid = {english},
	keywords = {act\_Conceptualizing, act\_Crowdsourcing, act\_Theorizing, obj\_DigitalHumanities, obj\_Infrastructures, obj\_Institutions},
}

@book{tennison_beginning_2005,
	location = {Berkeley, {CA} : New York},
	title = {Beginning {XSLT} 2.0: from novice to professional},
	isbn = {978-1-59059-324-0},
	shorttitle = {Beginning {XSLT} 2.0},
	abstract = {This followup to Jeni Tennison's Beginning {XSLT} has been updated to accomodate the revised {XSLT} standard. Part one of this book introduces {XML} and {XSLT} at a comfortable pace, and gradually demonstrates techniques for generating {HTML} (plus other formats), from {XML}. In part two, Tennison applies theory to real-life {XSLT} capabilities - including generating graphics.

Each chapter includes step-by-step examples, plus review questions at the end, to help you grasp the discussed features.},
	pagetotal = {824},
	publisher = {Apress ; Distributed to the book trade in the United States by Springer-Verlag},
	author = {Tennison, Jeni},
	date = {2005},
	langid = {english},
	keywords = {act\_Modeling},
}

@article{johanna_drucker_performative_nodate,
	title = {Performative Materiality and Theoretical Approaches to Interface},
	volume = {Volume 7 Number 1},
	url = {http://www.digitalhumanities.org/dhq/vol/7/1/000143/000143.html},
	abstract = {This article outlines a critical framework for a theory of performative materiality and its potential application to interface design from a humanistic perspective. Discussions of the materiality of digital media have become richer and more complex in the last decade, calling the literal, physical, and networked qualities of digital artifacts and systems to attention. This article extends those discussions by reconnecting them to a longer history of investigations of materiality and the specificity of media in critical theory and aesthetics. In addition, it introduces the concept of performative materiality, the enacted and event-based character of digital activity supported by those literal, physical conditions, and introduces the theoretical concerns that attach to that rubric. Performative materiality is based on the conviction that a system should be understood by what it does, not only how it is structured. As digital humanities matures, it can benefit from a re-engagement with the mainstream principles of critical theory on which a model of performative materiality is based. The article takes these ideas into a more focused look at how we might move towards integrating this model and critical principles into a model of humanistic interface design.},
	number = {2013},
	journaltitle = {{DHQ}: Digital Humanities Quarterly},
	author = {{Johanna Drucker}},
	urldate = {2015-09-29},
	keywords = {Object: Infrastructures, activity: Assess},
}

@online{mitchell_whitelaw_generous_nodate,
	title = {Generous Interfaces for Digital Cultural Collections},
	url = {http://www.digitalhumanities.org/dhq/vol/9/1/000205/000205.html},
	abstract = {Decades of digitisation have made a wealth of digital cultural material available online. Yet search — the dominant interface to these collections — is incapable of representing this abundance. Search is ungenerous: it withholds information, and demands a query. This paper argues for a more generous alternative: rich, browsable interfaces that reveal the scale and complexity of digital heritage collections. Drawing on related work and precedents from information retrieval and visualisation, as well as critical humanistic approaches to the interface, this paper documents and analyses practical experiments in generous interfaces developed in collaboration with Australian cultural institutions.},
	titleaddon = {{DHQ}: Digital Humanities Quarterly},
	author = {{Mitchell Whitelaw}},
	urldate = {2015-09-24},
	keywords = {activity: Assess, obj\_Infrastructures},
}

@online{christina_manzo_et_al._by_nodate,
	title = {By the People, For the People: Assessing the Value of Crowdsourced, User-Generated Metadata},
	url = {http://www.digitalhumanities.org/dhq/vol/9/1/000204/000204.html},
	abstract = {With the growing volume of user-generated classification systems arising from media tagging-based platforms (such as Flickr and Tumblr) and the advent of new crowdsourcing platforms for cultural heritage collections, determining the value and usability of crowdsourced, "folksonomic," or user-generated, "freely chosen keywords"  [21st Century Lexicon] for libraries, museums and other cultural heritage organizations becomes increasingly essential. The present study builds on prior work investigating the value and accuracy of folksonomies by: (1) demonstrating the benefit of user-generated "tags" - or unregulated keywords typically meant for personal organizational purposes - for facilitating item retrieval and (2) assessing the accuracy of descriptive metadata generated via a game-based crowdsourcing application. In this study, participants (N = 16) were first tasked with finding a set of five images using a search index containing either a combination of folksonomic and controlled vocabulary metadata or only controlled vocabulary metadata. Data analysis revealed that participants in the folksonomic and controlled vocabulary search condition were, on average, six times faster to search for each image (M = 25.08 secs) compared to participants searching with access only to controlled vocabulary metadata (M = 154.1 secs), and successfully retrieved significantly more items overall. Following this search task, all participants were asked to provide descriptive metadata for nine digital objects by playing three separate single-player tagging games. Analysis showed that 88\% of participant-provided tags were judged to be accurate, and that both tagging patterns and accuracy levels did not significantly differ between groups of professional librarians and participants outside of the Library Science field. These findings illustrate the value of folksonomies for enhancing item "findability," or the ease with which a patron can access materials, and the ability of librarians and general users alike to contribute valid, meaningful metadata. This could significantly impact the way libraries and other cultural heritage organizations conceptualize the tasks of searching and classification.},
	type = {{DHQ}: Digital Humanities Quarterly:},
	author = {{Christina Manzo et al.}},
	urldate = {2015-09-24},
	keywords = {activity: Collaborate},
}
@article{tkacz_wikipedia_nodate,
	title = {Wikipedia and the Politics of Mass Collaboration},
	volume = {Vol. 2 (2) 2012},
	url = {https://platformjmc.files.wordpress.com/2015/04/platformvol2issue2_tkacz.pdf},
	abstract = {Working together to produce socio-technological objects, based on emergent platforms
of economic production, is of great importance in the task of political transformation and the
creation of new subjectivities. Increasingly, “collaboration” has become a veritable buzzword
used to describe the human associations that create such new media objects. In the language
of “Web 2.0”, “participatory culture”, “user-generated content”, “peer production” and
the “produser”, first and foremost we are all collaborators. In this paper I investigate recent
literature that stresses the collaborative nature of Web 2.0, and in particular, works that
address the nascent processes of peer production. I contend that this material positions such
projects as what Chantal Mouffe has described as the “post-political”; a fictitious space far
divorced from the clamour of the everyday. I analyse one Wikipedia entry to demonstrate the
distance between this post-political discourse of collaboration and the realities it describes,
and finish by arguing for a more politicised notion of collaboration.},
	pages = {40--53},
	issue = {Collaborative Media and Networked Publics},
	journaltitle = {Platform. Journal of Media and Communication.},
	author = {Tkacz, Nathaniel},
	langid = {english},
	keywords = {Activity: Annotate, Object: Infrastructures, activity: Assess, activity: Enrich, object: Data/Databases, object: Knowledge},
}

@article{liu_what_nodate,
	title = {What is the meaning of the digital humanities to the humanities?},
	volume = {128},
	url = {http://www.mlajournals.org/doi/pdf/10.1632/pmla.2013.128.2.409},
	abstract = {Yet even if we were to complete our hypothetical ethnographer’s chart [of the digital humanities], it would not adequately explain the digital humanities. This is because we would leave unexplained the relation of the digital humanities to the humanities generally. My thesis is that an understanding of the digital humanities can only rise to the level of an explanation if we see that the underlying issue is the disciplinary identity not of the digital humanities but of the humanities themselves. For the humanities, the digital humanities exceed (though they include) the functional role of instrument or service, the pioneer role of innovator, the ensemble role of an “additional field,” and even such faux-political roles assigned to new fields as challenger, reformer, and (less positively) fifth column.},
	pages = {409--23},
	number = {2013},
	journaltitle = {{PMLA} 128},
	author = {Liu, Alan},
	langid = {english},
}

@article{best_surface_nodate,
	title = {Surface Reading: An Introduction},
	volume = {108},
	url = {http://www.jstor.org/stable/10.1525/rep.2009.108.1.1},
	doi = {DOI:10.1525/rep.2009.108.1.1},
	series = {Representations, University of California Press},
	abstract = {In the text-based disciplines, psychoanalysis and Marxism have had a major influence on how we read, and this has been expressed most consistently in the practice of symptomatic reading, a mode of interpretation that assumes that a text's truest meaning lies in what it does not say, describes textual surfaces as superfluous, and seeks to unmask hidden meanings. For symptomatic readers, texts possess meanings that are veiled, latent, all but absent if it were not for their irrepressible and recurring symptoms. Noting the recent trend away from ideological demystification, this essay proposes various modes of "surface reading" that together strive to accurately depict the truth to which a text bears witness. Surface reading broadens the scope of critique to include the kinds of interpretive activity that seek to understand the complexity of literary surfaces---surfaces that have been rendered invisible by symptomatic reading.},
	pages = {1--21},
	number = {1},
	author = {Best, Stephen and Marcus, Sharon},
	keywords = {act\_ContentAnalysis, act\_Query/Retrieve, act\_RelationalAnalysis, act\_Theorizing, obj\_Text},
}

@article{svensson_global_nodate,
	title = {Global Plants and Digital Letters: Epistemological Implications of Digitising the Directors’ Correspondence at the Royal Botanic Gardens, Kew},
	volume = {6},
	rights = {{CC} {BY}-{NC}-{ND} 3.0},
	issn = {2201-1919},
	url = {http://environmentalhumanities.org/arch/vol6/6.4.pdf},
	abstract = {Digitisation is presenting new possibilities and challenges for the use of collections in both the humanities and the sciences. However, digitisation is also another layer in a longer process of selections shaping the collection—something which must be analysed on a case-by-case basis. This paper considers the epistemological implications of the digitisation of the Directors’ Correspondence ({DC}) collection (1841-1928) at the Royal Botanic Gardens, Kew, made available through the Global Plants database. In order to avoid a polarised analysis of the end-products of archive and database, the selection process shaping this collection is traced from the writing of the letters and their reception into the {DC} at {RBG}, Kew, to the digitisation with corresponding metadata and the end-user searching the database. Particular attention is given the digitisation process and the knowledge produced by the project digitisers, as they combine close reading and database searches in writing the summaries of the letters for the metadata. This analysis of the {DC} engages with wider discussions about digitisation by emphasising the importance of taking a longer historical perspective, with particular attention to moments of selection, and highlighting the knowledge generated by those involved in the digitisation process. By doing so, the result is not a clear trajectory but a combination of losses and gains, disconnections and reconnections. Care is therefore needed to avoid replicating the invisible losses of extractive approaches to knowledge production, particularly in the context of collection-based biodiversity conservation.},
	journaltitle = {Environmental Humanities},
	author = {Svensson, Anna},
	langid = {english},
	keywords = {Object: Archives, Object: Infrastructures, activity: Assess, activity: Collect, activity: Reflect, object: Data/Databases},
}

@report{maziere_antoine_google_nodate,
	title = {Google Borders},
	url = {http://fabelier.org/towards-google-borders/},
	abstract = {We built a tool enabling users to map, worldwide the suggestions made by Google for a specific query. Through local suggestion, the user is given some knowledge about the other users with whom he is associated to. Given that {GA} associates users by the domain they use, those domains define the borders within which this knowledge is acquired about other users. Our tool allows one to abstract himself from these borders and acquire knowledge about any possible sub-set or combination of sub-sets of users.

We propose a new method and system for cultural trends analysis based on Google auto-complete suggestion, presenting a toolkit enabling any user to collect and analyze associations between queries, suggestions and various regions of the world. We report unexpected observations about several behavioural and geographical trends along with promising uses.},
	pages = {20},
	author = {Maziere, Antoine, Samuel, Huron},
	langid = {english},
	note = {http://doi.acm.org/10.1145/2464464.2464525},
	keywords = {Object: Language, activity: Assess, activity: Compare, activity: Gather, object: Data/Databases},
}

@article{wolfe_annotations_2008,
	title = {Annotations and the collaborative digital library: Effects of an aligned annotation interface on student argumentation and reading strategies},
	volume = {3},
	issn = {1556-1607, 1556-1615},
	url = {http://link.springer.com/article/10.1007/s11412-008-9040-x},
	doi = {10.1007/s11412-008-9040-x},
	shorttitle = {Annotations and the collaborative digital library},
	abstract = {Recent research on annotation interfaces provides provocative evidence that anchored, annotation-based discussion environments may lead to better conversations about a text. However, annotation interfaces raise complicated tradeoffs regarding screen real estate and positioning. It is argued that solving this screen real estate problem requires limiting the number of annotations displayed to users. In order to understand which annotations have the most learning value for students, this paper presents two complementary studies examining the effects of annotations on students performing a reading-to-write task. The first study used think-aloud protocols and a within-subjects methodology, finding that annotations appeared to provoke students to reflect more critically upon the primary text. This effect was particularly strong when students encountered pairs of annotations presenting different viewpoints on the same section of text. Student interviews suggested that annotations were most helpful when they caused the reader to consider and weigh conflicting viewpoints. The second study used a between-subjects methodology and a more naturalistic task to provide complementary evidence that annotations encourage more reflective responses to a text. This study found that students who received annotated materials both perceived themselves and were perceived by instructors as less reliant on unreflective summary strategies than students who received the same content but in a different format. These findings indicate that the learning value of an annotation lies in its ability to provoke students to consider and weigh new perspectives on the primary text. When selected effectively, annotations provide a critical scaffolding that can support students’ critical thinking and argumentation activities. Collaborative digital libraries and applications for the Web 2.0 should be designed with this learning framework in mind.},
	pages = {141--164},
	number = {2},
	author = {Wolfe, Joanna},
	urldate = {2014-03-26},
	date = {2008-06-01},
	note = {00036 bibtex: Wolfe2008Annotations 
biblatexdata[journaltitle=International Journal of Computer-Supported Collaborative Learning;langid=english;shortjournal=Computer Supported Learning]},
	keywords = {Activity: Annotate, Object: Texts},
}

@article{gius_informatik_nodate,
	title = {Informatik und Hermeneutik. Zum Mehrwert interdisziplinärer Textanalyse},
	url = {http://fvmww.diphda.uberspace.de/informatik-und-hermeneutik-zum-mehrwert-interdisziplin%C3%A4rer-textanalyse},
	doi = {nN},
	abstract = {Der Beitrag verhandelt die methodologischen Konsequenzen der interdisziplinären Zusammenarbeit zwischen Geisteswissenschaft und Informatik im Kontext des {heureCLÉA}-Projekts. Ziel von {heureCLÉA} ist es, eine ›digitale Heuristik‹ zur narratologischen Analyse literarischer Texte zu entwickeln, mit der automatisiert (1) bislang nur manuell durchführbare Annotationsaufgaben bis zu einem bestimmten Komplexitätsniveau durchgeführt und (2) statistisch auffällige Textphänomene als Kandidaten für eine anschließende Detailanalyse durch den menschlichen Nutzer identifiziert werden können.

Bei diesem Projekt müssen die disziplinären Herangehensweisen in besonderem Maß berücksichtigt werden. Im vorliegenden Beitrag werden die Ansätze dargestellt, die das bestehende Spannungsfeld zwischen (nicht-deterministischer) geisteswissenschaftlicher Hermeneutik und (deterministischer) Informatik produktiv nutzen und so über die konkrete Fragestellung des Projekts hinauswirken.
------
This paper discusses the methodological effects of the interdisciplinary cooperation between humanities scholars and computer scientists in the context of the project {heureCLÉA}. The goal of {heureCLÉA} is to develop a ›digital heuristic‹ that supports the narratological analysis of literary texts by (1) performing automatically annotation tasks up to a certain level of complexity that previously could only be carried out manually, and by (2) identifying statistically salient text phenomena for the subsequent detailed analysis by the human user.

In this project, it was essential to devote special attention to the specific disciplinary approaches. In this paper, we illustrate the ways in which the tensions between (non-deterministic) hermeneutics in the humanities and (deterministic) computer science can be productively employed and thus have an effect beyond the concrete research question.},
	issue = {Sonderband},
	author = {Gius, Evelyn and Jacke, Janina},
	langid = {german},
	keywords = {Meta: Theorizing, Narratology, Object: Digital Humanities, activity: Enrich, activity: Reflect},
}

@article{ernst_vom_nodate,
	title = {Vom Urheber zur Crowd, vom Werk zur Version, vom Schutz zur Öffnung? Kollaboratives Schreiben und Bewerten in den Digital Humanities},
	url = {http://fvmww.diphda.uberspace.de/vom-urheber-zur-crowd-vom-werk-zur-version-vom-schutz-zur-%C3%B6ffnung-kollaboratives-schreiben-und},
	doi = {nN},
	series = {Zeitschrift für {DH}},
	abstract = {Open-Review-Plattformen und Wikis verändern die wissenschaftliche Erkenntnisproduktion, Soziale Medien und Wissenschaftsblogs die Wissenschaftskommunikation. Diese digitalen Publikationsweisen stellen die Veröffentlichungspraxen und Autorschaftskonzepte der Geisteswissenschaften praktisch und kategorial vor einige Probleme. Daher ist es notwendig, den Begriff ›wissenschaftliche Autorschaft‹ angesichts neuer Formen des kollaborativen Schreibens einer Revision zu unterziehen und die Wissenschaftlichkeit der neuen Medienformate zu reflektieren. Dies kann exemplarisch an selbstkontrollierten digitalen Veröffentlichungsinfrastrukturen und Modellen eines offeneren Immaterialgüterrechts untersucht werden.

Open-review projects and wikis are changing academic knowledge production, just as academic blogs and social media are influencing scholarly communication. These digital forms of publishing challenge the usual practices of publishing and the concepts of authorship within the humanities, both practically and categorically. Therefore, it is necessary to revise the concept of ›scholarly authorship‹ by differentiating new forms of academic collaborative writing and to reflect on the academic quality of the new media formats within the humanities. This can be shown when analyzing self-curated digital publishing infrastructures and open license models of intellectual property.},
	issue = {Sonderband},
	author = {Ernst, Thomas},
	langid = {german},
	keywords = {Object: Digital Humanities, Open Access, act\_Collaborating, activity: Enrich, activity: Reflect},
}

@report{puig_digital_nodate,
	title = {Digital studies: Issues of organology for individuation in collaborative practices},
	url = {http://www.iri.centrepompidou.fr/wp-content/uploads/2011/02/PatrimoineHumanites.Puig-E1.pdf},
	abstract = {The research program of {IRI} (Institut de recherche et d'innovation) is deliberately focused on Digital Studies, which intends to goes beyond the current term Digital Humanities, as the issue is not primarily to equip the human and social sciences with digital tools but to study and design situations in which digital technologies profoundly modify the epistemology of disciplines and skills. More precisely, our approach is focused on a general organology (organon - tool in Greek; organology studies the psychic, artificial and social tools, which evolve within their mutual dependencies (http://www.arsindustrialis.org/glossary)) – including the technological, social and cultural – in the digital context. If the map plays an important role in geographic knowledge or the floor plan in an architectural way of thinking, or furthermore if the evolution of books (from analog to digital printing) directly influences literature, what will be the influence of the digital organology of the Web on all our knowledge?
Our approach is both theoretical and practical. We wish to tackle these epistemic issues while adapting and designing new organons. For the purpose of this action-research we identified four topics: 1) the figure of the amateur in the current context of the economy of contribution, 2) the organization of metadata, 3) the new industrial context for collaborative and contributive knowledge diffusion, and finally 4) the body and gesture intelligence in perceptive loops that are currently largely bypassed by digital organology.},
	pages = {12},
	institution = {{IRI} (http://www.iri.centrepompidou.fr/)},
	author = {Puig, Vincent},
	keywords = {act\_Collaborating, activity: Reflect, object: Data/Databases},
}

@article{collar_networks_2015,
	title = {Networks in Archaeology: Phenomena, Abstraction, Representation},
	volume = {22},
	issn = {1072-5369, 1573-7764},
	url = {http://link.springer.com/article/10.1007/s10816-014-9235-6},
	doi = {10.1007/s10816-014-9235-6},
	shorttitle = {Networks in Archaeology},
	abstract = {The application of method and theory from network science to archaeology has dramatically increased over the last decade. In this article, we document this growth over time, discuss several of the important concepts that are used in the application of network approaches to archaeology, and introduce the other articles in this special issue on networks in archaeology. We argue that the suitability and contribution of network science techniques within particular archaeological research contexts can be usefully explored by scrutinizing the past phenomena under study, how these are abstracted into concepts, and how these in turn are represented as network data. For this reason, each of the articles in this special issue is discussed in terms of the phenomena that they seek to address, the abstraction in terms of concepts that they use to study connectivity, and the representations of network data that they employ in their analyses. The approaches currently being used are diverse and interdisciplinary, which we think are evidence of a healthy exploratory stage in the application of network science in archaeology. To facilitate further innovation, application, and collaboration, we also provide a glossary of terms that are currently being used in network science and especially those in the applications to archaeological case studies.},
	pages = {1--32},
	number = {1},
	journaltitle = {Journal of Archaeological Method and Theory},
	shortjournal = {J Archaeol Method Theory},
	author = {Collar, Anna and Coward, Fiona and Brughmans, Tom and Mills, Barbara J.},
	urldate = {2015-08-05},
	date = {2015-01-30},
	langid = {english},
	keywords = {act\_NetworkAnalysis, act\_Theorizing, obj\_ANTHROPOLOGY, obj\_Archaeology, obj\_Network science, obj\_Relational archaeology},
}

@article{jacqueline_wernimont_feminisms_2015,
	title = {Feminisms in Digital Humanities},
	volume = {9.2},
	url = {http://www.digitalhumanities.org/dhq/preview/index.html},
	abstract = {Introduction to the special issue of Digital Humanities Quarterly on Feminisms and {DH}, which offers both background on the origins of the special issue and an overview of the pieces therein.},
	author = {{Jacqueline Wernimont}},
	urldate = {2015-09-14},
	date = {2015},
	langid = {english},
	keywords = {act\_Theorizing, obj\_DigitalHumanities},
}

@article{berg_rotkappchen_2014,
	title = {Rotkäppchen 2.0},
	url = {https://www.humboldt-foundation.de/web/kosmos-titelthema-102-1.html},
	abstract = {Ob Märchenforschung, Linguistik oder Archäologie: Immer mehr Geisteswissenschaftler nutzen die Methoden der Digital Humanities. Das führt zu neuen Erkenntnissen, aber auch zu Widerstand.},
	number = {102},
	journaltitle = {Humboldt Kosmos: Titelthema "Digital Humanities - Märchenhafte Chance oder Modetrend?"},
	author = {Berg, Lilo},
	date = {2014},
}

@article{merriman_science_2015,
	title = {A Science of Literature},
	url = {http://bostonreview.net/books-ideas/ben-merriman-moretti-jockers-digital-humanities},
	journaltitle = {Boston Review},
	author = {Merriman, Ben},
	date = {2015},
}

@article{gamerman_data_2015,
	title = {Data Miners Dig for Answers About Harper Lee, Truman Capote and ‘Go Set a Watchman’},
	url = {http://blogs.wsj.com/speakeasy/2015/07/15/go-set-a-watchman-harper-lee-truman-capote/},
	abstract = {Hours after “Go Set a Watchman” went on sale Tuesday, the developers of a computerized text-analysis tool ran that long-awaited novel and Lee’s Pulitzer-Prize winning “To Kill a Mockingbird” through an algorithm that searched for signs of heavy editing, frequent rewriting and other influences. The findings, which attempt to shed light on a book that has sparked world-wide attention by an author who has famously declined to discuss her work, show Lee as the undisputed author of both novels but suggest that her style as a writer was more consistent in “Watchman” than “Mockingbird.”

“‘Watchman’ is more her than ‘Mockingbird’ is,” said Jan Rybicki, who with fellow literature scholar Maciej Eder studied the texts and wrote up their analysis for The Wall Street Journal. Rybicki and Eder used software they released in 2013 to compare word patterns across different books. Rybicki’s scholarship includes attempting to distinguish a translator’s voice from an author’s in translated texts, while Eder recently took on Greek literature, arguing in a coming paper that Aeschylus didn’t write the classical tragedy “Prometheus Bound.”},
	journaltitle = {Wall Street Journal},
	author = {Gamerman, Ellen},
	date = {2015-07-15},
	keywords = {goal\_Analysis, obj\_Text},
}

@article{muhanna_hacking_2015,
	title = {Hacking the Humanities},
	url = {http://www.newyorker.com/culture/culture-desk/hacking-the-humanities},
	journaltitle = {The New Yorker},
	author = {Muhanna, Elias},
	date = {2015},
}

@book{deuff_les_2014,
	location = {Limoges},
	title = {Les temps des humanités digitales},
	isbn = {9782364051225},
	abstract = {Les humanités digitales se situent à la croisée de l informatique, des arts, des lettres et des sciences humaines et sociales. Elles s enracinent dans un mouvement en faveur de la diffusion, du partage et de la valorisation du savoir. Avec leur apparition, les universités, les lieux de savoir et les chercheurs vivent une transformation importante de leur mode de travail. Cela entraîne une évolution des compétences et des pratiques. Cet ouvrage explique les origines des humanités digitales et ses évolutions. Il décrit leurs réussites, leurs potentialités, leur rapport à la technique et comment elles transforment les sciences humaines, la recherche et l enseignement. Il examine les enjeux des nouveaux formats, modes de lecture, et des outils de communication et de visualisation. Ce livre permet d aller plus loin dans vos pratiques et vos réflexions. Le temps des humanités digitales est venu !},
	pagetotal = {176},
	publisher = {{FYP} {EDITIONS}},
	author = {Deuff, Olivier Le},
	date = {2014},
}

@article{borgman_digital_2009,
	title = {The Digital Future is Now: A Call to Action for the Humanities},
	volume = {3},
	url = {http://www.digitalhumanities.org/dhq/vol/3/4/000077/000077.html},
	abstract = {The digital humanities are at a critical moment in the transition from a specialty area to a full-fledged community with a common set of methods, sources of evidence, and infrastructure – all of which are necessary for achieving academic recognition. As budgets are slashed and marginal programs are eliminated in the current economic crisis, only the most articulate and productive will survive. Digital collections are proliferating,
but most remain difficult to use, and digital scholarship remains a backwater in most humanities departments with respect to hiring, promotion, and teaching practices. Only the scholars themselves are in a position to move the field forward. Experiences of the sciences in their initiatives for cyberinfrastructure and {eScience} offer valuable lessons.
Information- and data-intensive, distributed, collaborative, and multi-disciplinary research is now the norm in the sciences, while remaining experimental in the humanities. Discussed here are six factors for comparison, selected for their implications for the future of digital scholarship in the humanities: publication practices, data, research methods, collaboration, incentives, and learning. Drawing upon lessons gleaned from these comparisons, humanities scholars are “called to action” with five questions to
address as a community: What are data? What are the infrastructure requirements? Where are the social studies of digital humanities? What is the humanities laboratory of the 21st century? What is the value proposition for digital humanities in an era of declining budgets?},
	pages = {§1--82},
	number = {4},
	journaltitle = {Digital Humanities Quarterly},
	shortjournal = {{DHQ}},
	author = {Borgman, Christine},
	date = {2009},
	langid = {english},
	keywords = {*****, meta\_GiveOverview, obj\_DigitalHumanities},
}

@online{arxiv_machine_2015,
	title = {The Machine Vision Algorithm Beating Art Historians at Their Own Game},
	url = {http://www.technologyreview.com/view/537366/the-machine-vision-algorithm-beating-art-historians-at-their-own-game/},
	abstract = {Classifying a painting by artist and style is tricky for humans; spotting the links between different artists and styles is harder still. So it should be impossible for machines, right?},
	titleaddon = {{MIT} Technology Review},
	author = {{arXiv}, Emerging Technology From the},
	urldate = {2015-06-03},
	date = {2015-05-11},
	keywords = {goal\_Analysis, obj\_Images, t\_MachineLearning},
}

@article{saleh_large-scale_2015,
	title = {Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature},
	url = {http://arxiv.org/abs/1505.00855},
	shorttitle = {Large-scale Classification of Fine-Art Paintings},
	abstract = {In the past few years, the number of fine-art collections that are digitized and publicly available has been growing rapidly. With the availability of such large collections of digitized artworks comes the need to develop multimedia systems to archive and retrieve this pool of data. Measuring the visual similarity between artistic items is an essential step for such multimedia systems, which can benefit more high-level multimedia tasks. In order to model this similarity between paintings, we should extract the appropriate visual features for paintings and find out the best approach to learn the similarity metric based on these features. We investigate a comprehensive list of visual features and metric learning approaches to learn an optimized similarity measure between paintings. We develop a machine that is able to make aesthetic-related semantic-level judgments, such as predicting a painting's style, genre, and artist, as well as providing similarity measures optimized based on the knowledge available in the domain of art historical interpretation. Our experiments show the value of using this similarity measure for the aforementioned prediction tasks.},
	journaltitle = {{arXiv}:1505.00855 [cs]},
	author = {Saleh, Babak and Elgammal, Ahmed},
	urldate = {2015-06-03},
	date = {2015-05-04},
	eprinttype = {arxiv},
	eprint = {1505.00855},
	keywords = {act\_Annotating, goal\_Analysis, obj\_Images, t\_MachineLearning},
}

@online{udell_when_2015,
	title = {When Open Access is the norm, how do scientists work together online?},
	url = {http://blogs.plos.org/scicomm/2015/04/13/hello-world/#.VVDFitLnjqA.twitter},
	shorttitle = {When Open Access is the norm, how do scientists work together online?},
	titleaddon = {{PLOS} Scicomm Blog},
	author = {Udell, Jon},
	urldate = {2015-05-13},
	date = {2015},
	keywords = {act\_Communicating, act\_Publishing, goal\_Collaboration},
}

@collection{siemens_companion_2004,
	location = {Oxford},
	edition = {Hardcover},
	title = {A Companion to Digital Humanities},
	isbn = {1405103213},
	url = {http://www.digitalhumanities.org/companion/},
	series = {Blackwell Companions to Literature and Culture},
	abstract = {This Companion offers a thorough, concise overview of the emerging field of humanities computing. Contains 37 original articles written by leaders in the field. Addresses the central concerns shared by those interested in the subject. Major sections focus on the experience of particular disciplines in applying computational methods to research problems; the basic principles of humanities computing; specific applications and methods; and production, dissemination and archiving. Accompanied by a website featuring supplementary materials, standard readings in the field and essays to be included in future editions of the Companion.},
	publisher = {Blackwell Publishing Professional},
	editor = {Siemens, Ray and Unsworth, John and Schreibman, Susan},
	urldate = {2010-05-17},
	date = {2004},
	langid = {english},
	keywords = {*****, X-{CHECK}, act\_Publishing, meta\_GiveOverview, meta\_Theorizing, t\_Encoding},
}

@book{hulle_making_2011,
	title = {The making of Samuel Beckett's stirrings still/ soubresauts and comment dire/what is the word},
	url = {http://librarytitles.ebrary.com/Doc?id=10781907},
	author = {Hulle, Dirk van and Stéphane de Schrevel, Gent},
	urldate = {2015-05-12},
	date = {2011},
}
@book{hulle_manuscript_2008,
	location = {Gainesville},
	title = {Manuscript genetics, Joyce's know-how, Beckett's nohow},
	isbn = {9780813032009  0813032008},
	publisher = {University Press of Florida},
	author = {Hulle, Dirk van},
	date = {2008},
}

@article{blumenthal_stanford_2015,
	title = {Stanford Literary Lab Maps ‘Emotions in Victorian London’},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2015/04/14/books/stanford-literary-lab-maps-emotions-in-victorian-london.html},
	abstract = {This online project is part of a growing movement in the humanities to harness digital technology for cultural analysis — like treating books as data to create “literary geography.”},
	journaltitle = {The New York Times},
	author = {Blumenthal, Ralph},
	urldate = {2015-04-14},
	date = {2015-04-13},
	keywords = {act\_ContentAnalysis, goal\_Analysis, obj\_Text, t\_Mapping, t\_SentimentAnalysis},
}

@article{battersby_new_2015,
	title = {A new Shakespeare play has just been discovered},
	url = {http://www.independent.co.uk/arts-entertainment/theatre-dance/news/fake-shakespeare-play-double-falsehood-is-genuine-after-all-10167657.html},
	abstract = {A lost play once claimed to be by Shakespeare but subsequently poo-pooed as a forgery, is now “strongly” believed to be genuine according to new research.},
	journaltitle = {The Independent},
	author = {Battersby, Matilda},
	urldate = {2015-04-11},
	date = {2015-04-11},
	langid = {english},
	keywords = {act\_StylisticAnalysis, goal\_Analysis, obj\_Text},
}

@inproceedings{qi_new_2011,
	title = {A new method for visual stylometry on impressionist paintings},
	doi = {10.1109/ICASSP.2011.5946912},
	abstract = {A new emerging field, that of visual stylometry of art, proposes to apply image analysis and machine learning tools to high-resolution digital images of artwork in order to assist art connoisseurs in determining the painting's likely creator. The premise is that each artist's brushwork is likely to contain features that are characteristic of the artist's unique habitual physical movements; these features could be identified and characterized through machine learning. In this paper, we describe a new technique for this problem. We extract, as features for our classifier, parameters of both Hidden Markov Tree models and linear predictor models of the painting's wavelet coefficients. We then use the {FINE} dimensionality reduction technique [1] to produce an unsupervised low-dimensional embedding of the data. Tests on two dataset consisting of over 100 high-resolution digital images of impressionist paintings by Van Gogh and contemporaries shows good separation between paintings of Van Gogh and others is achieved via this unsupervised process. We further show (through comparison with the alternative) that our method benefits greatly from (1) using only background sections of each painting in our analysis, (2) the {FINE} technique, and (3) the use of both {HMT} and linear predictor features together in the same analysis. All three of these technique choices are new in this paper. We hope that our method can be a tool, used carefully in conjunction with the connoisseur's expertise and other examinations, to determine a painting's true authorship.},
	eventtitle = {2011 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {2036--2039},
	booktitle = {2011 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Qi, Hanchao and Hughes, S.},
	date = {2011-05},
	keywords = {act\_StylisticAnalysis, goal\_Analysis, obj\_Music},
}

@article{backer_musical_2005,
	title = {On musical stylometry—a pattern recognition approach},
	volume = {26},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865504003393},
	doi = {10.1016/j.patrec.2004.10.016},
	series = {In Memoriam: Azriel Rosenfeld},
	abstract = {In this short communication we describe some experiments in which methods of statistical pattern recognition are applied for musical style recognition and disputed musical authorship attribution.

Values of a set of 20 features (also called “style markers”) are measured in the scores of a set of compositions, mainly describing the different sonorities in the compositions. For a first study over 300 different compositions of Bach, Handel, Telemann, Mozart and Haydn were used and from this data set it was shown that even with a few features, the styles of the various composers could be separated with leave-one-out-error rates varying from 4\% to 9\% with the exception of the confusion between Mozart and Haydn which yielded a leave-one-out-error rate of 24\%. A second experiment included 30 fugues from J.S. Bach, W.F. Bach and J.L. Krebs, all of different style and character. With this data set of compositions of undisputed authorship, the F minor fugue for organ, {BWV} 534 (of which Bach’s authorship is disputed) then was confronted. It could be concluded that there is experimental evidence that J.L. Krebs should be considered in all probability as the composer of the fugue in question.},
	pages = {299--309},
	number = {3},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Backer, Eric and Kranenburg, Peter van},
	urldate = {2015-04-08},
	date = {2005-02},
	keywords = {act\_StylisticAnalysis, goal\_Analysis, obj\_Music},
}

@article{wilkens_digital_2015,
	title = {Digital Humanities and Its Application in the Study of Literature and Culture},
	volume = {67},
	issn = {0010-4124, 1945-8517},
	url = {http://complit.dukejournals.org/content/67/1/11},
	doi = {10.1215/00104124-2861911},
	abstract = {“Digital Humanities and Its Application in the Study of Literature and Culture” examines the relationship between comparative literary and cultural studies, systems theory and model building, and recent work in digital humanities. Areas of specific application include the topology of German literature, modernist poetics in China, Japan, and the United States, and the geography of nineteenth-century fiction, as well as a range of associated computational methods. Wilkens argues that computational work represents a unique opportunity for comparatists interested in large-scale cultural analysis and that digital humanities would benefit from increased participation by comparatists.},
	pages = {11--20},
	number = {1},
	journaltitle = {Comparative Literature},
	shortjournal = {Comparative Literature},
	author = {Wilkens, Matthew},
	urldate = {2015-03-21},
	date = {2015-03-01},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, obj\_Text},
}

@article{unsworth_computational_2011,
	title = {Computational Work with Very Large Text Collections},
	rights = {{TEI} Consortium 2011 (Creative Commons Attribution-{NoDerivs} 3.0 Unported License);http://creativecommons.org/licenses/by-nd/3.0/},
	url = {http://jtei.revues.org/215},
	abstract = {This essay will address the challenges and possibilities presented to the Text Encoding Initiative, particularly in the area of interoperability, by the very large text collections (on the order of millions of volumes) being made available for computational work in environments where the texts can be reprocessed into new representations, in order to be manipulated with analytical tools.  It will also consider {TEI}’s potential role in the design of these environments, these representations, and these tools.  The argument of the piece is that interoperability is a process as well as a state, that it requires mechanisms that would sustain it, and that {TEI} is one of those mechanisms.},
	issue = {Issue 1},
	journaltitle = {Journal of the Text Encoding Initiative},
	author = {Unsworth, John},
	editor = {Hawkins, Kevin and Rehbein, Malte and Bauman, Syd},
	urldate = {2011-06-08},
	date = {2011-06-08},
	langid = {english},
	note = {The “I” in {TEI} sometimes stands for interchange, but it never stands for interoperability.  Interchange is the activity of reciprocating or exchanging, especially with respect to information (according to Wordnet), or if you prefer the Oxford English Dictionary, it is “the act of exchanging reciprocally; giving and receiving with reciprocity.”  It’s an old word, its existence attested as early as 1548.  Interoperability is a much newer word with what appears to be military provenance, dating bac (...)},
	keywords = {{AnalyzeStatistically}, bigdata, t\_Interoperability},
}

@online{underwood_what_2012,
	title = {What can topic models of {PMLA} teach us about the history of literary scholarship?},
	url = {http://tedunderwood.com/2012/12/14/what-can-topic-models-of-pmla-teach-us-about-the-history-of-literary-scholarship/},
	abstract = {Of all our literary-historical narratives it is the history of criticism itself that seems most wedded to a stodgy history-of-ideas approach—narrating ch...},
	titleaddon = {The Stone and the Shell},
	author = {Underwood, Ted and Goldstone, Andrew},
	urldate = {2012-12-16},
	date = {2012-12-14},
	langid = {english},
	keywords = {bigdata},
}

@article{rybicki_collaborative_2014,
	title = {Collaborative authorship: Conrad, Ford and Rolling Delta},
	volume = {29},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/29/3/422},
	doi = {10.1093/llc/fqu016},
	shorttitle = {Collaborative authorship},
	abstract = {Based on Burrows's measure of stylometric difference that uses frequencies of most frequent words, Rolling Delta is a method for revealing stylometric signals of two (or more) authors in a collaborative text. It is applied here to study the texts written jointly by Joseph Conrad and Ford Madox Ford, producing results that generally confirm the usual critical consensus on the visibility of the two author's hand. It also confirms that Ford's claims to a sizeable fragment in Nostromo are unfounded.},
	pages = {422--431},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Rybicki, Jan and Hoover, David and Kestemont, Mike},
	urldate = {2014-09-22},
	date = {2014-09-01},
	langid = {english},
	keywords = {act\_StylisticAnalysis, goal\_Analysis, obj\_Text},
}

@article{rockwell_what_2003,
	title = {What is Text Analysis, Really?},
	volume = {18},
	url = {http://llc.oxfordjournals.org/content/18/2/209.abstract},
	doi = {10.1093/llc/18.2.209},
	abstract = {The author revisits the question of what text analysis could be. He traces the tools from their origin in the concordance. He argues that text‐analysis tools produce new texts generated from queries through processes implemented on the computer. These new texts come from the decomposition of original texts and recomposition into hybrid new works for interpretation. The author ends by presenting a portal model for how text‐analysis tools can be made available to the community.},
	pages = {209 --219},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Rockwell, Geoffrey},
	urldate = {2011-12-14},
	date = {2003-06},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, meta\_GiveOverview},
}

@book{ramsay_reading_2011,
	location = {Urbana  Ill.},
	title = {Reading machines : toward an algorithmic criticism},
	isbn = {9780252036415},
	url = {http://www.press.uillinois.edu/books/catalog/75tms2pw9780252036415.html},
	shorttitle = {Reading machines},
	abstract = {Computers can handle vast amounts of data, allowing for the comparison of texts in ways that were previously too overwhelming for individuals, but they may also assist in enhancing the entirely necessary role of subjectivity in critical interpretation. Reading Machines discusses the importance of this new form of text analysis conducted with the assistance of computers. Ramsay suggests that the rigidity of computation can be enlisted by intuition, subjectivity, and play.},
	publisher = {University of Illinois Press},
	author = {Ramsay, Stephen},
	date = {2011},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, bigdata, meta\_Theorizing},
}

@inproceedings{ramsay_hermeneutics_2010,
	location = {Brown University},
	title = {The Hermeneutics of Screwing Around; or What You Do with a Million Books},
	url = {http://quod.lib.umich.edu/d/dh/12544152.0001.001/1:5/--pastplay-teaching-and-learning-history-with-technology?g=dculture;rgn=div1;view=fulltext;xc=1},
	abstract = {There is a never-ending, unsatiated search for truth and wisdom. Nonetheless, that has not stopped philosophers and the like from attempting to create a declared path to what is known. It is this same train of thought that aids professors create class curricula and syllabi. All one can do is survey a field while presenting a topic that in turn will resonate. It is in this state of unease with the unknown that new ways of knowing are devised. This state of flux and anxiety can also be linked to the early days of the Internet and its seemingly chaotic order. As with what happened with print media, there was an attempt to create sense out of the chaos using guides and tools such as Google. The author describes how this turmoil is present in his field of literary studies, the debates about canonicity and who is in and who is out. With the vast amount of material that exists, one may never even be close to creating a reliable guide to a cannon and may only ever be able to create a random sampling. The author describes that there are some solutions that have been proposed such as accepting our ignorance and the fact that we will never know it all. He continues with a discussion of how using a library in different ways for specific research can follow the premise discussed above. Following this is a discussion of searching and browsing and how the two terms are different. The author concludes the chapter by stating that there are too many books for one to ever read and one should neither try to read them all nor pretend that they have but one’s personal path through the vast archive is important},
	eventtitle = {{CUHG} Talk},
	author = {Ramsay, Stephen},
	date = {2010-04-16},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Theorizing},
}

@inproceedings{kirschenbaum_remaking_2007,
	title = {The Remaking of Reading: Data Mining and the Digital Humanities},
	url = {http://www.cs.umbc.edu/∼hillol/NGDM07/abstracts/talks/MKirschenbaum.pdf},
	abstract = {This paper discusses applications of data mining in the seemingly unlikely field of literary criticism. While the underlying techniques are traditional—Naïve Bayes, {SVM}—literary criticism, and the ” digital humanities” more generally, differ from other domains in that they rarely admit ground truth into their discussions. Instead, data mining and machine learning are best understood in terms of ” provocation”—the potential for outlier results to surprise a reader into attending to some aspect of a text not previously deemed significant—as well as ” notreading” or ” distant reading,” the automated search for patterns across a much wider corpus than could be read and assimilated via traditional humanistic methods of ” close reading.” At a moment when a widely publicized report by the National Endowment for the Arts concluded reading itself was ” at risk,” large online text collections (Google Books, the Open Content Alliance) are making millions of texts available in machine-readable form. Data mining is part of this remaking of reading.},
	booktitle = {{NGDM} 07: National Science Foundation Symposium on Next Generation of Data Mining and Cyber-Enabled Discovery for Innovation},
	author = {Kirschenbaum, Matthew},
	date = {2007-10},
	langid = {english},
	keywords = {*****, {AnalyzeStatistically}, goal\_Analysis, meta\_Theorizing},
}

@book{kirschenbaum_digital_2010,
	location = {Washington  D.C.},
	title = {Digital forensics and born-digital content in cultural heritage collections},
	isbn = {9781932326376},
	url = {http://www.clir.org/pubs/reports/reports/pub149/pub149.pdf},
	abstract = {While the purview of digital forensics was once specialized to fields of law enforcement, computer security, and national defense, the increasing ubiquity of computers and electronic devices means that digital forensics is now used in a wide variety of cases and circumstances. Most records today are born digital, and libraries and other collecting institutions increasingly receive computer storage media as part of their acquisition of "papers" from writers, scholars, scientists, musicians, and public figures. This poses new challenges to librarians, archivists, and curators—challenges related to accessing and preserving legacy formats, recovering data, ensuring authenticity, and maintaining trust. The methods and tools developed by forensics experts represent a novel approach to these demands. For example, the same forensics software that indexes a criminal suspect's hard drive allows the archivist to prepare a comprehensive manifest of the electronic files a donor has turned over for accession.

This report introduces the field of digital forensics in the cultural heritage sector and explores some points of convergence between the interests of those charged with collecting and maintaining born-digital cultural heritage materials and those charged with collecting and maintaining legal evidence.},
	publisher = {Council on Library and Information Resources},
	author = {Kirschenbaum, Matthew and Ovenden, Richard and Redwine, Gabriela},
	date = {2010},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview},
}

@misc{moller_digital_2015,
	title = {Digital Humanities. Wie die Digitalisierung die Wissenschaft verändert},
	url = {http://www.deutschlandradiokultur.de/digital-humanities-wie-die-digitalisierung-die-wissenschaft.976.de.html?dram:article_id=313420},
	titleaddon = {Zeitfragen},
	publisher = {Deutschlandradio Kultur},
	author = {Möller, Cristian},
	date = {2015-03-05},
	langid = {german},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@book{juola_authorship_2008,
	title = {Authorship Attribution},
	isbn = {160198118X},
	pagetotal = {116},
	publisher = {Now Publishers,},
	author = {Juola, Patrick},
	date = {2008-03-06},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview, t\_Stylometry},
}

@article{jockers_comparative_2010,
	title = {A comparative study of machine learning methods for authorship attribution},
	volume = {25},
	url = {http://llc.oxfordjournals.org/content/25/2/215.abstract},
	doi = {10.1093/llc/fqq001},
	abstract = {We compare and benchmark the performance of five classification methods, four of which are taken from the machine learning literature, in a classic authorship attribution problem involving the Federalist Papers. Cross-validation results are reported for each method, and each method is further employed in classifying the disputed papers and the few papers that are generally understood to be coauthored. These tests are performed using two separate feature sets: a “raw” feature set containing all words and word bigrams that are common to all of the authors, and a second “pre-processed” feature set derived by reducing the raw feature set to include only words meeting a minimum relative frequency threshold. Each of the methods tested performed well, but nearest shrunken centroids and regularized discriminant analysis had the best overall performances with 0/70 cross-validation errors.},
	pages = {215--223},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Jockers, Matthew L. and Witten, Daniela M.},
	urldate = {2011-12-14},
	date = {2010-06},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, meta\_Theorizing, t\_MachineLearning, t\_Stylometry},
}

@incollection{hoover_searching_2008,
	location = {Amsterdam},
	title = {Searching for Style in Modern American Poetry},
	abstract = {This essay examines a corpus of Modern American Poetry using Zeta and Iota, two recently developed measures of textual difference (Burrows 2007), and some alternatives to them that move beyond the most frequent words of the text, which have been the traditional focus of computational stylistics. These measures concentrate our attention on moderately frequent or rare characteristic words, most of which are content words. These highly characteristic words lead us back to the text, back to questions of interpretation and style, and highlighting such words in texts emphasizes their dense concentration and helps us to visualize how and why intuitive perceptions of stylistic difference are possible.},
	pages = {211--227},
	booktitle = {Directions in Empirical Literary Studies: Essays in Honor of Willie van Peer},
	publisher = {John Benjamins},
	author = {Hoover, David L.},
	editor = {Zyngier, Sonia},
	date = {2008},
	langid = {english},
	keywords = {bigdata},
}

@article{crane_what_2006,
	title = {What Do You Do with a Million Books?},
	volume = {12},
	issn = {1082-9873},
	url = {http://www.dlib.org/dlib/march06/crane/03crane.html},
	doi = {10.1045/march2006-crane},
	abstract = {Introduction

The Greek historian Herodotus has the Athenian sage Solon estimate the lifetime of a human being at c. 26,250 days (Herodotus, The Histories, 1.32). If we could read a book on each of those days, it would take almost forty lifetimes to work through every volume in a single million book library. The continuous tradition of written European literature that began with the Iliad and Odyssey in the eighth century {BCE} is itself little more than a million days old. While libraries that contain more than one million items are not unusual, print libraries never possessed a million books of use to any one reader. The great libraries that took shape in the nineteenth and twentieth centuries were meta-structures, whose catalogues and finding aids allowed readers to create their own customized collections, building on the fixed classification schemes and disciplinary structures that took shape in the nineteenth century.

The digital libraries of the early twenty-first century can be searched and their contents transmitted around the world. They can contain time-based media, images, quantitative data, and a far richer array of content than print, with visualization technologies blurring the boundaries between library and museum. But our digital libraries remain filled with digital incunabula – digital objects whose form remains firmly rooted in traditions of print, with {HTML} and {PDF} largely mimicking the limitations of their print predecessors.

Vast collections based on image books – raw digital pictures of books with searchable but uncorrected text from {OCR} – could arguably retard our long-term progress, reinforcing the hegemony of structures that evolved to minimize the challenges of a world where paper was the only medium of distribution and where humans alone could read.1 Already the books in a digital library are beginning to read one another and to confer among themselves before creating a new synthetic document for review by their human readers.2},
	number = {3},
	journaltitle = {D-Lib Magazine},
	shortjournal = {D-Lib Magazine},
	author = {Crane, Gregory},
	urldate = {2011-08-26},
	date = {2006-03},
	langid = {english},
	keywords = {*****, {AnalyzeStatistically}, act\_Discovering},
}

@article{cohen_babel_2006,
	title = {From Babel to Knowledge: Data Mining Large Digital Collections},
	volume = {12},
	issn = {1082-9873},
	url = {http://chnm.gmu.edu/essays-on-history-new-media/essays/?essayid=40},
	doi = {10.1045/march2006-cohen},
	pages = {6--19},
	number = {3},
	journaltitle = {D-Lib Magazine},
	author = {Cohen, Daniel J.},
	urldate = {2009-12-12},
	date = {2006-03},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview, obj\_OnlineContent},
}

@article{leroi_digitizing_2015,
	title = {Digitizing the Humanities},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2015/02/14/opinion/digitizing-the-humanities.html},
	abstract = {The digital humanities are being built by a new breed of scholar; one who can investigate Cicero’s use of the word “lascivium” and code in Python.},
	journaltitle = {The New York Times},
	author = {Leroi, Armand Marie},
	urldate = {2015-02-15},
	date = {2015-02-13},
	keywords = {obj\_DigitalHumanities},
}

@article{mackenzie_literature_2013,
	title = {Literature by the Numbers. Critical reading gets even better when you use your computer.},
	url = {http://nautil.us/issue/6/secret-codes/literature-by-the-numbers},
	shorttitle = {Literature by the Numbers},
	abstract = {“Literature is the opposite of data,” wrote novelist Stephen Marche in the Los Angeles Times Review of Books in October 2012.\&\#8230;},
	number = {6},
	journaltitle = {Nautilus},
	author = {Mackenzie, Dana},
	urldate = {2015-01-15},
	date = {2013},
	keywords = {obj\_Text},
}

@collection{jisc_ebooks_2014,
	title = {Ebooks in Education: Realising the Vision},
	isbn = {9781909188372},
	url = {http://www.ubiquitypress.com/site/books/detail/10/ebooks-in-education/},
	shorttitle = {Ebooks in Education},
	publisher = {Ubiquity Press},
	editor = {{Jisc} and Woodward, Hazel},
	urldate = {2015-01-15},
	date = {2014-11-28},
	keywords = {act\_Teaching/Learning, obj\_Text},
}
@article{percino_instrumentational_2014,
	title = {Instrumentational Complexity of Music Genres and Why Simplicity Sells},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pone.0115255},
	doi = {10.1371/journal.pone.0115255},
	abstract = {Listening habits are strongly influenced by two opposing aspects, the desire for variety and the demand for uniformity in music. In this work we quantify these two notions in terms of instrumentation and production technologies that are typically involved in crafting popular music. We assign an ‘instrumentational complexity value’ to each music style. Styles of low instrumentational complexity tend to have generic instrumentations that can also be found in many other styles. Styles of high complexity, on the other hand, are characterized by a large variety of instruments that can only be found in a small number of other styles. To model these results we propose a simple stochastic model that explicitly takes the capabilities of artists into account. We find empirical evidence that individual styles show dramatic changes in their instrumentational complexity over the last fifty years. ‘New wave’ or ‘disco’ quickly climbed towards higher complexity in the 70s and fell back to low complexity levels shortly afterwards, whereas styles like ‘folk rock’ remained at constant high instrumentational complexity levels. We show that changes in the instrumentational complexity of a style are related to its number of sales and to the number of artists contributing to that style. As a style attracts a growing number of artists, its instrumentational variety usually increases. At the same time the instrumentational uniformity of a style decreases, i.e. a unique stylistic and increasingly complex expression pattern emerges. In contrast, album sales of a given style typically increase with decreasing instrumentational complexity. This can be interpreted as music becoming increasingly formulaic in terms of instrumentation once commercial or mainstream success sets in.},
	pages = {e115255},
	number = {12},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Percino, Gamaliel and Klimek, Peter and Thurner, Stefan},
	urldate = {2015-01-04},
	date = {2014-12-31},
	keywords = {act\_StructuralAnalysis, goal\_Analysis, obj\_Music},
}

@article{underwood_cents_2014,
	title = {Cents and Sensibility},
	issn = {1091-2339},
	url = {http://www.slate.com/articles/business/moneybox/2014/12/thomas_piketty_on_literature_balzac_austen_fitzgerald_show_arc_of_money.single.html},
	abstract = {Trust Thomas Piketty on economic inequality. Ignore what he says about literature.},
	journaltitle = {Slate},
	author = {Underwood, Ted and Long, Hoyt and So, Richard Jean},
	urldate = {2014-12-31},
	date = {2014-12-10},
	langid = {american},
	keywords = {act\_ContentAnalysis, obj\_Literature},
}

@book{demantowsky_geschichte_2014,
	location = {Berlin, Boston},
	title = {Geschichte lernen im digitalen Wandel},
	isbn = {978-3-486-85866-2},
	url = {http://www.degruyter.com/view/product/231648},
	publisher = {De Gruyter Oldenbourg},
	author = {Demantowsky, Marko and Pallaske, Christoph},
	urldate = {2014-12-16},
	date = {2014},
}

@article{rybicki_deeper_2011,
	title = {Deeper Delta across genres and languages: do we really need the most frequent words?},
	volume = {26},
	url = {http://llc.oxfordjournals.org/content/early/2011/07/14/llc.fqr031.abstract},
	doi = {10.1093/llc/fqr031},
	shorttitle = {Deeper Delta across genres and languages},
	abstract = {This article examines the success of authorship attribution of Burrows’s Delta in several corpora representing a variety of languages and genres. Contrary to the approaches of our predecessors, who only investigated the attributive effectiveness of the very top of the list of the most frequent words, hundreds of possible combinations of word vectors were tested in this study, not solely starting with the most frequent word in each corpus. The results show that Delta works best for prose in English and German and less well for agglutinative languages such as Polish or Latin.},
	pages = {315--321},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	author = {Rybicki, Jan and Eder, Maciej},
	urldate = {2011-07-26},
	date = {2011-07-14},
	langid = {english},
	keywords = {*****, {AnalyzeStatistically}, bigdata, t\_Stylometry},
}

@article{stamatatos_survey_2009,
	title = {A survey of modern authorship attribution methods},
	volume = {60},
	issn = {1532-2882},
	url = {http://dl.acm.org/citation.cfm?id=1527090.1527102},
	doi = {10.1002/asi.v60:3},
	abstract = {Authorship attribution supported by statistical or computational methods has a long history starting from the 19th century and is marked by the seminal study of Mosteller and Wallace (1964) on the authorship of the disputed “Federalist Papers.” During the last decade, this scientific field has been developed substantially, taking advantage of research advances in areas such as machine learning, information retrieval, and natural language processing. The plethora of available electronic texts (e.g., e-mail messages, online forum messages, blogs, source code, etc.) indicates a wide variety of applications of this technology, provided it is able to handle short and noisy text from multiple candidate authors. In this article, a survey of recent advances of the automated approaches to attributing authorship is presented, examining their characteristics for both text representation and text classification. The focus of this survey is on computational requirements and settings rather than on linguistic or literary issues. We also discuss evaluation methodologies and criteria for authorship attribution studies and list open questions that will attract future work in this area. © 2009 Wiley Periodicals, Inc.},
	pages = {538--556},
	number = {3},
	journaltitle = {J. Am. Soc. Inf. Sci. Technol.},
	author = {Stamatatos, Efstathios},
	urldate = {2011-12-14},
	date = {2009-03},
	langid = {english},
	keywords = {*****, {AnalyzeStatistically}, meta\_GiveOverview, t\_Stylometry},
}

@article{salt_statistical_1974,
	title = {Statistical Style Analysis of Motion Pictures},
	volume = {28},
	issn = {00151386, 15338630},
	url = {http://www.jstor.org/discover/10.2307/1211438?uid=3737592&uid=2129&uid=2&uid=70&uid=4&sid=21105321419953},
	doi = {10.2307/1211438},
	pages = {13--22},
	number = {1},
	journaltitle = {Film Quarterly},
	author = {Salt, Barry},
	urldate = {2014-11-29},
	date = {1974-10},
}

@online{surf_vre_2011,
	title = {{VRE} Starters Kit},
	url = {https://www.surf.nl/en/knowledge-and-innovation/knowledge-base/2011/vre-virtual-research-eenvironment-starters-kit.html},
	titleaddon = {{SURF} Wiki},
	author = {{SURF}},
	date = {2011},
	langid = {english},
	keywords = {meta\_ProjectManagement, obj\_Infrastructures},
}

@unpublished{bobley_why_2008,
	title = {Why the Digital Humanities?},
	abstract = {Adapted from a presentation given to members of the National Council on the 
Humanities about the recent creation of the {NEH}’s Office of Digital Humanities.},
	note = {National Council on the Humanities},
	author = {Bobley, Brett},
	urldate = {2010-01-13},
	date = {2008-07-30},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{rosenbloom_towards_2012,
	title = {Towards a Conceptual Framework for the Digital Humanities},
	volume = {6},
	url = {http://ict.usc.edu/pubs/Towards%20a%20Conceptual%20Framework%20for%20Digital%20Humanities.pdf},
	abstract = {The concept of a great scientific domain broadens what is normally considered to be within
the purview of science while identifying four such domains – the physical, life, social and
computing sciences – and suggesting that the humanities naturally fit within the sciences
as part of an expanded social domain. The relational architecture that has been developed
to aid in understanding disciplinary combinations across great scientific domains then
guides an exploration of the structure and content of the digital humanities in terms of a
space of relationships between computing and the humanities.},
	number = {2},
	author = {Rosenbloom, Paul S.},
	urldate = {2013-07-01},
	date = {2012},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@article{meister_think_2002,
	title = {"Think Big": Disziplinarität als wissenschaftstheoretische Benchmark der Computerphilologie},
	url = {http://computerphilologie.uni-muenchen.de/jg02/meister2.html},
	abstract = {The article's point of departure is a clarification of the principle conditions that would have to be fulfilled by Computer Philology in order to qualify as a discipline. This is followed by reviewing steps taken with a view to the possible institutionalization of Computer Philology at Hamburg University. The above theoretical considerations, as well as the reference to the practical example will form the backdrop for reviewing three articles presented in recent years which have aimed to answer the question »What is Computer Philology?«. The contributions under discussion are seen to differ mainly with regard to the implications which they hold for Computer Philology's development into a discipline proper. The article's conclusion is that we ought to »Think Big» in Computer Philology and aspire to meet the criteria for disciplinarity which are considered an important scholarly benchmark.

Der Terminus ›Computerphilologie‹ bezeichnet ein noch junges Phänomen innerhalb des philologischen Wahrnehmungshorizontes. Was jemand, der sich als ›Computerphilologe‹ bezeichnet, betreibt, darüber hat man eine ungefähre Vorstellung (er oder sie traktiert Texte mit dem Computer); was man sich jedoch unter Computerphilologie als solcher vorzustellen hat, ist eine Frage, die traditionell arbeitende Philologen zumeist ratlos lässt, handelt es sich doch um keine bereits fest etablierte oder gar institutionalisierte – ja, was nun: Disziplin? Methodik? Schule?},
	pages = {19--49},
	journaltitle = {Jahrbuch für Computerphilologie 4 (2002)},
	author = {Meister, Jan Christoph},
	date = {2002},
	langid = {german},
	note = {Meister, Jan Christoph: "Think Big": Disziplinarität als wissenschaftstheoretische Benchmark der Computerphilologie. In: Jahrbuch für Computerphilologie 4 (2002), S. 19-49.},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@online{rhody_topic_2013,
	title = {Topic Modeling and Figurative Language},
	url = {http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody/},
	abstract = {Located at the center of Jorie Graham’s collection The End of Beauty, “Self Portrait as Hurray and Delay” crafts a portrait of the artist, poised at a precarious moment in which thought begins to take shape. Like Penelope, Graham entertains the illusion, if only momentarily, of a choice between bringing a creative impulse into form or allowing it to come undone. A weaver of language, Graham subtly, deftly, but unsuccessfully attempts to delay the inevitable moment in poetic creation in which complexity of thought adopts form through language, and so realized is also reduced. In The End of Beauty, the beginning of the creative act signals an inevitable descent into meaning — language’s ultimate impulse.},
	titleaddon = {Journal of Digital Humanities},
	author = {Rhody, Lisa M.},
	urldate = {2013-04-15},
	date = {2013-04-07},
	langid = {english},
}

@online{mittell_thoughts_2012,
	title = {Thoughts on Blogging for Tenure},
	url = {http://justtv.wordpress.com/2012/01/04/thoughts-on-blogging-for-tenure/},
	abstract = {I recently was contacted by Stephen Olsen from the {MLA}, who is coordinating a pre-conference workshop entitled “Evaluating Digital Work for Tenure and Promotion: A Workshop for Evaluators and Candidates” taking place on the 5th of January at this year’s convention. For the session, they are organizing a number of case studies of digital work that they will discuss in terms of how a promotions committee or reviewers would approach them, and my blog was suggested as a possible example. (As it turns out, the suggestion came from my Provost at Middlebury, Alison Byerly, who is participating on the workshop – and I know how fortunate we are not only to have top administrators who are humanists, which I believe is somewhat rare in talking with colleagues elsewhere, but who are also interested \& engaged in thinking about new forms of scholarship.)},
	titleaddon = {Just {TV}},
	author = {Mittell, Jason},
	date = {2012-01-04},
	langid = {english},
}

@book{tufte_visual_2001,
	location = {Cheshire, Conn},
	edition = {2nd ed},
	title = {The visual display of quantitative information},
	isbn = {0961392142},
	url = {http://www.edwardtufte.com/tufte/books_vdqi},
	abstract = {The classic book on statistical graphics, charts, tables. Theory and practice in the design of data graphics, 250 illustrations of the best (and a few of the worst) statistical graphics, with detailed analysis of how to display data for precise, effective, quick analysis. Design of the high-resolution displays, small multiples. Editing and improving graphics. The data-ink ratio. Time-series, relational graphics, data maps, multivariate designs. Detection of graphical deception: design variation vs. data variation. Sources of deception. Aesthetics and data graphical displays.

This is the second edition of The Visual Display of Quantitative Information. Recently published, this new edition provides excellent color reproductions of the many graphics of William Playfair, adds color to other images, and includes all the changes and corrections accumulated during 17 printings of the first edition.},
	pagetotal = {197},
	publisher = {Graphics Press},
	author = {Tufte, Edward R.},
	date = {2001},
	langid = {english},
	keywords = {act\_Visualizing},
}

@incollection{crane_tools_2009,
	location = {Washington, {DC}},
	title = {Tools for Thinking: {ePhilology} and Cyberinfrastructure},
	url = {http://www.clir.org/pubs/reports/pub145/pub145.pdf},
	abstract = {New media and technologies are providing opportunities to transform research, teaching, and learning in the humanities. As scholarship becomes increasingly digital and interdisciplinary, challenges emerge with respect to organizing, engineering, and deploying the technologies needed to operate at a very large scale. The search for solutions will require collaboration across disciplines—in the humanities, humanistic social sciences, and technology.},
	pages = {16--26},
	booktitle = {Working Together or Apart: Promoting the Next Generation of Digital Scholarship},
	publisher = {{CLIR}},
	author = {Crane, Gregory and Babeu, Alison and Bamman, David and Cerrato, Lisa and Singhal, Rashimi},
	date = {2009},
	langid = {english},
	keywords = {meta\_Advocating, meta\_Assessing, obj\_Infrastructures},
}

@online{suda_top_2012,
	title = {The top 20 data visualisation tools},
	url = {http://www.netmagazine.com/features/top-20-data-visualisation-tools},
	abstract = {From simple charts to complex maps and infographics, Brian Suda's round-up of the best – and mostly free – tools has everything you need to bring your data to life},
	titleaddon = {Netmagazine},
	author = {Suda, Brian},
	date = {2012-09-17},
	langid = {english},
	keywords = {act\_Visualizing},
}

@book{feldman_text_2007,
	location = {Cambridge ; New York},
	title = {The text mining handbook: advanced approaches in analyzing unstructured data},
	isbn = {0521836573},
	url = {http://www.amazon.de/The-Text-Mining-Handbook-Unstructured/dp/0521836573},
	shorttitle = {The text mining handbook},
	abstract = {Text mining is a new and exciting area of computer science research that tries to solve the crisis of information overload by combining techniques from data mining, machine learning, natural language processing, information retrieval, and knowledge management. Similarly, link detection – a rapidly evolving approach to the analysis of text that shares and builds upon many of the key elements of text mining – also provides new tools for people to better leverage their burgeoning textual data resources. The Text Mining Handbook presents a comprehensive discussion of the state-of-the-art in text mining and link detection. In addition to providing an in-depth examination of core text mining and link detection algorithms and operations, the book examines advanced pre-processing techniques, knowledge representation considerations, and visualization approaches. Finally, the book explores current real-world, mission-critical applications of text mining and link detection in such varied fields as M\&A business intelligence, genomics research and counter-terrorism activities.},
	pagetotal = {410},
	publisher = {Cambridge University Press},
	author = {Feldman, Ronen},
	editora = {Sanger, James},
	editoratype = {collaborator},
	date = {2007},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@online{weingart_topic_2011,
	title = {Topic Modeling and Network Analysis},
	url = {http://www.scottbot.net/HIAL/?p=221},
	abstract = {According to Google Scholar, David Blei’s first topic modeling paper has received 3,540 citations since 2003. Everybody’s talking about topic models. Seriously, I’m afraid of visiting my parents this Hanukkah and hearing them ask “Scott… what’s this topic modeling I keep hearing all about?” They’re powerful, widely applicable, easy to use, and difficult to understand — a dangerous combination.

Since shortly after Blei’s first publication, researchers have been looking into the interplay between networks and topic models. This post will be about that interplay, looking at how they’ve been combined, what sorts of research those combinations can drive, and a few pitfalls to watch out for. I’ll bracket the big elephant in the room until a later discussion, whether these sorts of models capture the semantic meaning for which they’re often used. This post also attempts to introduce topic modeling to those not yet fully (converted) aware of its potential.},
	titleaddon = {the scottbot irregular},
	author = {Weingart, Scott},
	urldate = {2012-04-18},
	date = {2011-11-15},
	langid = {english},
}

@article{rybicki_stylistics_2013,
	title = {The stylistics and stylometry of collaborative translation: Woolf’s Night and Day in Polish},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/early/2013/05/26/llc.fqt027},
	doi = {10.1093/llc/fqt027},
	shorttitle = {The stylistics and stylometry of collaborative translation},
	abstract = {The study investigates to what extent traditional stylistics and non-traditional stylometry can co-operate in the study of translations in terms of translatorial style. Stylistic authorship attribution methods based on a multivariate analysis of most-frequent-word frequencies are used in attempts at identifying translators. While these methods usually identify the author of the original rather than the translator, a case study is presented of the Polish translation of a single novel by Virginia Woolf, Night and Day, in which one translator took over from the other; the point of this takeover has been successfully identified with the above-mentioned methods.},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Rybicki, Jan and Heydel, Magda},
	urldate = {2013-06-12},
	date = {2013-05-27},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@inproceedings{card_structure_1997,
	title = {The Structure of the Information Visualization Design Space},
	url = {http://www.cs.ubc.ca/~tmm/courses/old533/readings/card96structure.pdf},
	doi = {10.1109/INFVIS.1997.636792},
	abstract = {Research on information visualization has reached the
place where a number of successful point designs have
been proposed and a number of techniques of been
discovered. It is now appropriate to begin to describe and
analyze portions of the design space so as to understand
the differences among designs and to suggest new
possibilities. This paper proposes an organization of the
information visualization literature and illustrates it with a
series of examples. The result is a framework for
designing new visualizations and augmenting existing
designs},
	pages = {92--99},
	booktitle = {Proceedings of {IEEE} Symposium on Information Visualization 1997},
	author = {Card, Stuart K. and Mackinlay, Jock},
	date = {1997},
	langid = {english},
}

@book{argamon_structure_2010,
	location = {Heidelberg; New York},
	title = {The structure of style : algorithmic approaches to understanding manner and meaning},
	isbn = {9783642123368  3642123368  9783642123375  3642123376},
	url = {http://www.springer.com/computer/database+management+%26+information+retrieval/book/978-3-642-12336-8},
	shorttitle = {The structure of style},
	abstract = {Style is a fundamental and ubiquitous aspect of the human experience: Everyone instantly and constantly assesses people and things according to their individual styles, academics establish careers by researching musical, artistic, or architectural styles, and entire industries maintain themselves by continuously creating and marketing new styles. Yet what exactly style is and how it works are elusive: We certainly know it when we see it, but there is no shared and clear understanding of the diverse phenomena that we call style.

The Structure of Style explores this issue from a computational viewpoint, in terms of how information is represented, organized, and transformed in the production and perception of different styles. New computational techniques are now making it possible to model the role of style in the creation of and response to human artifacts—and therefore to develop software systems that directly make use of style in useful ways.

Argamon, Burns, and Dubnov organize the research they have collected in this book according to the three roles that computation can play in stylistics. The first section of the book, Production, provides conceptual foundations by describing computer systems that create artifacts—musical pieces, texts, artworks—in different styles. The second section, Perception, explains methods for analyzing different styles and gleaning useful information, viewing style as a form of communication. The final section, Interaction, deals with reciprocal interaction between style producers and perceivers, in areas such as interactive media, improvised musical accompaniment, and game playing.

The Structure of Style is written for researchers and practitioners in areas including information retrieval, computer art and music, digital humanities, computational linguistics, and artificial intelligence, who can all benefit from this comprehensive overview and in-depth description of current research in this active interdisciplinary field.},
	publisher = {Springer},
	author = {Argamon, Shlomo and Burns, Kevin and Dubnov, Shlomo},
	date = {2010},
	langid = {english},
	keywords = {t\_Stylometry},
}

@online{weingart_topic_2012,
	title = {Topic Modeling for Humanists: A Guided Tour},
	url = {http://www.scottbot.net/HIAL/?p=19113},
	shorttitle = {Topic Modeling for Humanists},
	abstract = {It’s that time again! Somebody else posted a really clear and enlightening description of topic modeling on the internet. This time it was Allen Riddell, and it’s so good that it inspired me to write this post about topic modeling that includes no actual new information, but combines a lot of old information in a way that will hopefully be useful. If there’s anything I’ve missed, by all means let me know and I’ll update accordingly.},
	titleaddon = {the scottbot irregular},
	author = {Weingart, Scott},
	urldate = {2012-07-26},
	date = {2012-07-25},
	langid = {english},
}

@article{clement_story_2009,
	title = {The Story of One: Humanity scholarship with visualization and text analysis},
	volume = {1},
	url = {http://www.academia.edu/966499/The_Story_of_One_Humanity_scholarship_with_visualization_and_text_analysis},
	abstract = {Most critiques of The Making of Americas (Paris 1925) by Gertrude Stein contend that the text deconstructs the role narrative plays in determining identity by using indeterminacy to challenge readerly subjectivity. The current perception of Making as a postmodern text relies on the notion that there is a tension created by frustrated expectations that result from the text’s progressive disbandment of story and plot as the narrative unweaves into seemingly chaotic, meaningless rounds of repetitive words and phrases. Yet, a new perspective that is facilitated by digital tools and based on the highly structured nature of the text suggests that these instabilities can be resolved by the same seemingly nonsensical, non-narrative structures. Seeing the manner in which the structure of the text makes meaning in conversation with narrative alleviates perceived instabilities in the discourse. The discourse about identity formation is engaged—not dissolved in indeterminacy—to the extent that the reader can read the composition.},
	pages = {8485},
	number = {43},
	journaltitle = {Relation 10},
	author = {Clement, T. and Plaisant, C. and Vuillemot, R.},
	date = {2009},
	langid = {english},
	keywords = {act\_Visualizing, t\_Narratology},
}

@thesis{evert_statistics_2004,
	title = {The statistics of word cooccurrences: word pairs and collocations},
	url = {http://elib.uni-stuttgart.de/opus/volltexte/2005/2371/pdf/Evert2005phd.pdf},
	institution = {Stuttgart},
	type = {phdthesis},
	author = {Evert, Stefan},
	editora = {Heid, Ulrich},
	editoratype = {collaborator},
	date = {2004},
	langid = {english},
	note = {Online-Ressource
Stuttgart, Univ., Diss, 2004},
	keywords = {{AnalyzeStatistically}, act\_Visualizing, obj\_Language},
}
@online{templeton_topic_2011,
	title = {Topic Modeling in the Humanities: An Overview},
	url = {http://mith.umd.edu/topic-modeling-in-the-humanities-an-overview/},
	shorttitle = {en},
	abstract = {In a recent post to this blog, Sayan Bhattacharyya described his contributions to the Woodchipper project in the context of a broader discussion about corpus-based approaches to humanities research. Topic modeling, the statistical technology undergirding Woodchipper, has garnered increasing attention as a tool of hermeneutic empowerment, a method for drawing structure out of a corpus on the basis of minimal critical presuppositions. In this post I map out a basic genealogy of topic modeling in the humanities, from the highly cited paper that first articulated Latent Dirichlet Allocation ({LDA}) to recent work at {MITH}.},
	titleaddon = {{MITH} Blog},
	author = {Templeton, Clay},
	date = {2011},
	keywords = {meta\_GiveOverview},
}

@book{yule_statistical_1944,
	location = {Cambridge [Eng.]},
	title = {The Statistical Study of Literary Vocabulary},
	url = {http://www.jstor.org/discover/10.2307/3717870?uid=3737864&uid=2134&uid=2&uid=70&uid=4&sid=21105175796093},
	publisher = {University Press},
	author = {Yule, G},
	date = {1944},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Theorizing, obj\_Literature},
}

@article{liu_state_2011,
	title = {The state of the digital humanities. A report and a critique},
	volume = {11},
	url = {http://ahh.sagepub.com/content/11/1-2/8.abstract},
	doi = {10.1177/1474022211427364},
	abstract = {The scholarly field of the digital humanities has recently expanded and integrated its fundamental concepts, historical coverage, relationship to social experience, scale of projects, and range of interpretive approaches. All this brings the overall field (including the related area of new media studies) to a tipping point where it has the potential not just to facilitate the work of the humanities but to represent the state of the humanities at large in its changing relation to higher education in the postindustrial state. Are the digital humanities up to this larger task?},
	pages = {8 --41},
	number = {1},
	journaltitle = {Arts and Humanities in Higher Education},
	author = {Liu, Alan},
	urldate = {2012-02-08},
	date = {2011},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@online{blevins_topic_2010,
	title = {Topic Modeling Martha Ballard’s Diary},
	url = {http://historying.org/2010/04/01/topic-modeling-martha-ballards-diary/},
	abstract = {In A Midwife’s Tale, Laurel Ulrich describes the challenge of analyzing Martha Ballard’s exhaustive diary, which records daily entries over the course of 27 years: “The problem is not that the diary is trivial but that it introduces more stories than can be easily recovered and absorbed.” (25) This fundamental challenge is the one I’ve tried to tackle by analyzing Ballard’s diary using text mining. There are advantages and disadvantages to such an approach – computers are very good at counting the instances of the word “God,” for instance, but less effective at recognizing that “the Author of all my Mercies” should be counted as well. The question remains, how does a reader (computer or human) recognize and conceptualize the recurrent themes that run through nearly 10,000 entries?},
	titleaddon = {Historying},
	author = {Blevins, Cameron},
	date = {2010-04},
	langid = {english},
}

@article{rudman_state_1998,
	title = {The State of Authorship Attribution Studies: Some Problems and Solutions},
	volume = {31},
	url = {http://link.springer.com/article/10.1023%2FA%3A1001018624850},
	abstract = {he statement, ’’Results of most non-traditional authorship attribution studies are not universally accepted as definitive,'' is explicated. A variety of problems in these studies are listed and discussed: studies governed by expediency; a lack of competent research; flawed statistical techniques; corrupted primary data; lack of expertise in allied fields; a dilettantish approach; inadequate treatment of errors. Various solutions are suggested: construct a correct and complete experimental design; educate the practitioners; study style in its totality; identify and educate the gatekeepers; develop a complete theoretical framework; form an association of practitioners.},
	pages = {351--365},
	journaltitle = {Computers and the Humanities},
	author = {Rudman, Joseph},
	date = {1998},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview, t\_Stylometry},
}

@article{walker_song_2009,
	title = {The Song Decoders},
	url = {http://www.nytimes.com/2009/10/18/magazine/18Pandora-t.html?pagewanted=all},
	abstract = {On first listen, some things grab you for their off-kilter novelty. Like the story of a company that has hired a bunch of “musicologists,” who sit at computers and listen to songs, one at a time, rating them element by element, separating out what sometimes comes to hundreds of data points for a three-minute tune. The company, an Internet radio service called Pandora, is convinced that by pouring this information through a computer into an algorithm, it can guide you, the listener, to music that you like. The premise is that your favorite songs can be stripped to parts and reverse-engineered.},
	journaltitle = {The New York Times},
	author = {Walker, Rob},
	date = {2009-10-14},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, act\_Query/Retrieve, goal\_Dissemination, obj\_Music},
}

@article{gretarsson_topicnets:_2012,
	title = {{TopicNets}: Visual Analysis of Large Text Corpora with Topic Modeling},
	volume = {3},
	issn = {2157-6904},
	url = {http://doi.acm.org/10.1145/2089094.2089099},
	doi = {10.1145/2089094.2089099},
	shorttitle = {{TopicNets}},
	abstract = {We present {TopicNets}, a Web-based system for visual and interactive analysis of large sets of documents using statistical topic models. A range of visualization types and control mechanisms to support knowledge discovery are presented. These include corpus- and document-specific views, iterative topic modeling, search, and visual filtering. Drill-down functionality is provided to allow analysts to visualize individual document sections and their relations within the global topic space. Analysts can search across a dataset through a set of expansion techniques on selected document and topic nodes. Furthermore, analysts can select relevant subsets of documents and perform real-time topic modeling on these subsets to interactively visualize topics at various levels of granularity, allowing for a better understanding of the documents. A discussion of the design and implementation choices for each visual analysis technique is presented. This is followed by a discussion of three diverse use cases in which {TopicNets} enables fast discovery of information that is otherwise hard to find. These include a corpus of 50,000 successful {NSF} grant proposals, 10,000 publications from a large research center, and single documents including a grant proposal and a {PhD} thesis.},
	pages = {23:1--23:26},
	number = {2},
	journaltitle = {{ACM} Trans. Intell. Syst. Technol.},
	author = {Gretarsson, Brynjar and O'Donovan, John and Bostandjiev, Svetlin and Höllerer, Tobias and Asuncion, Arthur and Newman, David and Smyth, Padhraic},
	urldate = {2012-04-19},
	date = {2012-02},
	langid = {english},
	keywords = {act\_Visualizing},
}

@inproceedings{kilgariff_sketch_2004,
	location = {Lorient, France},
	title = {The Sketch Engine},
	url = {http://www.sketchengine.co.uk/},
	abstract = {The Sketch Engine is for anyone wanting to research how words behave. It is a Corpus Query System. It lets you see a concordance for any word, phrase or grammatical construction, in one of the corpora that we provide, or in a corpus of your own. Its unique feature are word sketches, one-page, automatic, corpus-derived summaries of a word's grammatical and collocational behaviour.},
	eventtitle = {Euralex},
	pages = {105--116},
	booktitle = {Proceedings Euralex},
	author = {Kilgariff, Adam and Rychly, Pavel and Smrz, Pavel and Tugwell, David},
	date = {2004},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Tools},
}

@article{short_role_2006,
	title = {The Role of Humanities Computing: Experiences and Challenges},
	volume = {21},
	url = {http://llc.oxfordjournals.org/content/21/1/15.short},
	doi = {10.1093/llc/fqi043},
	shorttitle = {The Role of Humanities Computing},
	pages = {15 --27},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	author = {Short, Harold},
	urldate = {2012-02-08},
	date = {2006-04},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@incollection{argamon_rest_2010,
	title = {The Rest of the Story: Finding Meaning in Stylistic Variation},
	rights = {©2010 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-12336-8, 978-3-642-12337-5},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-12337-5_5},
	shorttitle = {The Rest of the Story},
	abstract = {The computational analysis of the style of natural language texts, computational stylistics, seeks to develop automated methods to (1) effectively distinguish texts with one stylistic character from those of another, and (2) give a meaningful representation of the differences between textual styles. Such methods have many potential applications in areas including criminal and national security forensics, customer relations management, spam/scam filtering, and scholarly research. In this chapter, we propose a framework for research in computational stylistics, based on a functional model of the communicative act. We illustrate the utility of this framework via several case studies.},
	pages = {79--112},
	booktitle = {The Structure of Style},
	publisher = {Springer Berlin Heidelberg},
	author = {Argamon, Shlomo and Koppel, Moshe},
	editor = {Argamon, Shlomo and Burns, Kevin and Dubnov, Shlomo},
	urldate = {2013-05-08},
	date = {2010-01-01},
	langid = {english},
	keywords = {*****, {AnalyzeStatistically}, meta\_GiveOverview},
}

@article{mcminn_tracking_2011,
	title = {Tracking the use of engineering conference papers: citation influence of the Stapp Car Crash Conference},
	volume = {30},
	issn = {0160-4953},
	url = {http://scholarlykitchen.sspnet.org/2012/08/28/bury-your-writing-why-do-academic-book-chapters-fail-to-generate-citations/},
	doi = {10.1108/01604951111127443},
	shorttitle = {Tracking the use of engineering conference papers},
	abstract = {I recently wrote a chapter for an upcoming book about academic and professional publishing. I’ve also written chapters in the past for other academic books about publishing. Writing a chapter is always a worthwhile experience — I like to write, I like the topics I’m selected to tackle, and I like the editors. But even with the books in hand, these chapters also seem to disappear under the waves like anvils, never to be heard from again.},
	pages = {76--85},
	number = {2},
	journaltitle = {Collection Building},
	author = {{McMinn}, H. Stephen and Fleming, Kathleen},
	urldate = {2012-08-29},
	date = {2011},
	langid = {english},
}

@article{cronon_public_2012,
	title = {The Public Practice of History in and for a Digital Age},
	url = {http://www.historians.org/Perspectives/issues/2012/1201/The-Public-Practice-of-History-in-and-for-a-Digital-Age.cfm},
	abstract = {History, like the world itself, is changing in ways that none of us yet fully understands. Some of the changes look pretty exciting, some pretty scary, but all require our engagement if history is to remain relevant to the times in which we live.},
	journaltitle = {{AHA}},
	author = {Cronon, William},
	date = {2012-01},
	langid = {english},
}

@article{flanders_productive_2009,
	title = {The Productive Unease of 21st-century Digital Scholarship},
	volume = {3},
	url = {http://digitalhumanities.org/dhq/vol/3/3/000055.html},
	abstract = {Despite prevailingly progressive narratives surrounding the impact of digital technology on modern academic culture, the field of digital humanities is characterized at a deeper level by a more critical engagement with technology. This engagement, which I characterize as a kind of "productive unease", is focused around issues of representation, medium, and structures of scholarly communication.},
	number = {3},
	journaltitle = {Digital Humanities Quarterly},
	shortjournal = {{DHQ}},
	author = {Flanders, Julia},
	urldate = {2009-12-10},
	date = {2009},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@report{zorich_transitioning_2012,
	location = {New York},
	title = {Transitioning to a Digital World. Art History, Its Research Centers, and Digital Scholarship},
	url = {http://www.kressfoundation.org/news/Article.aspx?id=35338},
	abstract = {The Kress Foundation has a longstanding interest in art history research centers both here and abroad. For half a century the Foundation has supported the important programs of many such centers, most notably through our institutional pre-doctoral art history fellowship program. Kress also has a strong interest in the emerging digital humanities, and especially in digital art history. In order to help us all understand better the current state of digital art history and how it relates to our traditional art history research centers, we have sponsored a study of the subject. In partnership with the Roy Rosenzweig Center for History and New Media at George Mason University – a key node in the emerging network of the digital humanities under the inspired leadership of Daniel Cohen - we invited Diane Zorich to undertake this study. Ms. Zorich, a cultural heritage consultant with extensive experience in the digital humanities and art history fields, has now completed this study. We are pleased to share the final report, Transitioning to a Digital World: Art History, Its Research Centers, and Digital Scholarship, with all communities that take an interest in art history, the digital humanities, humanities research centers, and scholarship in the humanities in general.},
	institution = {The Samuel H. Kress Foundation and The Roy Rosenzweig Center for History and New Media, George Mason University},
	author = {Zorich, Diane M.},
	date = {2012-05},
	langid = {english},
	keywords = {meta\_Assessing, meta\_GiveOverview, obj\_Institutions, obj\_Research},
}

@book{kaufer_power_2004,
	location = {New Jersey \& London},
	title = {The Power of Words: Unveiling the Speaker and Writer’s Hidden Craft},
	url = {http://www.amazon.de/The-Power-Words-Unveiling-Speaker/dp/0805847839},
	abstract = {n 1888, Mark Twain reflected on the writer's special feel for words to his correspondent, George Bainton, noting that "the difference between the almost-right word and the right word is really a large matter." We recognize differences between a politician who is "willful" and one who is "willing" even though the difference does not cross word-stems or parts of speech. We recognize that being "held up" evokes different experiences depending upon whether its direct object is a meeting, a bank, or an example. Although we can notice hundreds of examples in the language where small differences in wording produce large reader effects, the authors of The Power of Words argue that these examples are random glimpses of a hidden systematic knowledge that governs how we, as writers or speakers, learn to shape experience for other human beings. Over the past several years, David Kaufer and his colleagues have developed a software program for analyzing writing called {DocuScope}. This book illustrates the concepts and rhetorical theory behind the software analysis, examining patterns in writing and showing writers how their writing works in different categories to accomplish varying objectives. Reflecting the range and variety of audience experience that contiguous words of surface English can prime, the authors present a theory of language as an instrument of rhetorically priming audiences and a catalog of English strings to implement the theory. The project creates a comprehensive map of the speaker and writer's implicit knowledge about predisposing audience experience at the point of utterance. The book begins with an explanation of why studying language from the standpoint of priming--not just meaning--is vital to non-question begging theories of close reading and to language education in general. The remaining chapters in Part I detail the steps taken to prepare a catalog study of English strings for their properties as priming instruments. Part {II} describes in detail the catalog of priming categories, including enough examples to help readers see how individual words and strings of English fit into the catalog. The final part describes how the authors have applied the catalog of English strings as priming tools to conduct textual research.},
	publisher = {Lawrence Erlbaum Associates},
	author = {Kaufer, David and Ishizaki, Suguru and Butler, Brian and Collins, Jeff},
	date = {2004},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, act\_Visualizing},
}

@article{mccarty_tree_2006,
	title = {Tree, Turf, Centre, Archipelago—or Wild Acre? Metaphors and Stories for Humanities Computing,},
	volume = {21},
	url = {http://llc.oxfordjournals.org/content/21/1/1.abstract},
	doi = {10.1093/llc/fqi066},
	shorttitle = {Tree, Turf, Centre, Archipelago—or Wild Acre?},
	abstract = {The social acceptability of computing to the humanities is no longer a serious problem, although its role in research is sometimes overlooked or must be kept decorously out of sight. The real problem is that in an academic world largely defined by disciplinary turf-polity, possibilities for it are severely constricted. As was true in the early days of computer science, humanities computing is still likely to be seen, judged and funded not as an integral practice but piecemeal, in the widely differing terms of the disciplines to which it is applied. In this essay, I go after antiquated figures of thought responsible for this blinkered, piecemeal view. Reasoning from the evident importance of geopolitical metaphors to our operative conception of disciplinarity, I look down under, and back in time, for different, less constricting metaphors and draw out of them a different professional myth to live by.},
	pages = {1 --13},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	author = {{McCarty}, Willard},
	urldate = {2011-11-10},
	date = {2006-04},
	langid = {english},
	keywords = {meta\_Assessing, meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{tehrani_phylogeny_2013,
	title = {The Phylogeny of Little Red Riding Hood},
	volume = {8},
	url = {http://dx.doi.org/10.1371/journal.pone.0078871},
	doi = {10.1371/journal.pone.0078871},
	abstract = {Researchers have long been fascinated by the strong continuities evident in the oral traditions associated with different cultures. According to the ‘historic-geographic’ school, it is possible to classify similar tales into “international types” and trace them back to their original archetypes. However, critics argue that folktale traditions are fundamentally fluid, and that most international types are artificial constructs. Here, these issues are addressed using phylogenetic methods that were originally developed to reconstruct evolutionary relationships among biological species, and which have been recently applied to a range of cultural phenomena. The study focuses on one of the most debated international types in the literature: {ATU} 333, ‘Little Red Riding Hood’. A number of variants of {ATU} 333 have been recorded in European oral traditions, and it has been suggested that the group may include tales from other regions, including Africa and East Asia. However, in many of these cases, it is difficult to differentiate {ATU} 333 from another widespread international folktale, {ATU} 123, ‘The Wolf and the Kids’. To shed more light on these relationships, data on 58 folktales were analysed using cladistic, Bayesian and phylogenetic network-based methods. The results demonstrate that, contrary to the claims made by critics of the historic-geographic approach, it is possible to identify {ATU} 333 and {ATU} 123 as distinct international types. They further suggest that most of the African tales can be classified as variants of {ATU} 123, while the East Asian tales probably evolved by blending together elements of both {ATU} 333 and {ATU} 123. These findings demonstrate that phylogenetic methods provide a powerful set of tools for testing hypotheses about cross-cultural relationships among folktales, and point towards exciting new directions for research into the transmission and evolution of oral narratives.},
	pages = {e78871},
	number = {11},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Tehrani, Jamshid J.},
	urldate = {2014-08-30},
	date = {2013-11-13},
	langid = {english},
	keywords = {*****, act\_ContentAnalysis, act\_RelationalAnalysis, goal\_Analysis, obj\_Text},
}

@online{konig_twitter_2012,
	title = {Twitter in der Wissenschaft: Ein Leitfaden für Historiker/innen},
	url = {http://dhdhi.hypotheses.org/1072},
	abstract = {Twitter eignet sich in vielfacher Hinsicht als Informations- und Distributionskanal in den Wissenschaften. Dennoch gibt es gerade im deutschsprachigen Bereich noch wenige Historiker/innen, die auf Twitter aktiv sind, vielleicht auch, weil nicht klar ist, welches Potential und welche Einsatzmöglichkeiten Twitter hat und wie man den Dienst tatsächlich verwendet. Dieser Blogbeitrag versteht sich daher als praktische Hilfestellung für Historiker/innen, die Twitter zu wissenschaftlichen Zwecken einsetzen wollen. Die hier aufgeführten Anwendungsbeispiele und Tipps sind dabei lediglich als Hinweise zu verstehen, die dem eigenen, kreativen Umgang mit Twitter keinesfalls im Weg stehen sollen. Es versteht sich von selbst, dass ein solcher Leitfaden nicht vollständig sein kann. Um Ergänzungen in den Kommentaren wird daher ausdrücklich gebeten.},
	titleaddon = {Digital Humanities am {DHIP}},
	author = {König, Mareike},
	date = {2012-08-21},
	langid = {german},
}

@article{beaudouin_metrometer:_1996,
	title = {The Metrometer: a Tool for Analysing French Verse},
	volume = {11},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/11/1/23},
	doi = {10.1093/llc/11.1.23},
	shorttitle = {The Metrometer},
	abstract = {In this article, we present the ‘metrometer’, a computing tool, capable of identifying, in any kind of French text input, metrical components, that is syllables. The choice of the syllable is very natural: French poetry is known to be mainly syllabic. More specifically, our software performs a complete transcription of the input into the corresponding sequence of phonemes, using a transcribing module consistent with the specific phonology of classical poetry, which allows the marking and counting of metrical syllables. Our system outputs as well with a series of useful markers (syntactic tags, word boundaries, etc. ). This tool has been developed and tested on a corpus containing all P. Corneille‘s and J. Racine’s plays, totaling about 80,000 verses, and has proved to be almost error free on these data. The metrometer shall provide a very effective tool to tackle the study of rhythm in French poetry with quantitative methods.},
	pages = {23--31},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Beaudouin, Valérie and Yvon, Francois},
	urldate = {2012-09-23},
	date = {1996-04-01},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@inproceedings{havre_themeriver:_2000,
	title = {{ThemeRiver}: Visualizing Theme Changes over Time},
	url = {http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=885098&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D885098},
	abstract = {{ThemeRiverTM} is a prototype system that visualizes thematic variations over time within a large collection of documents. The “river” flows from left to right through time, changing width to depict changes in thematic strength of temporally associated documents. Colored “currents” flowing within the river narrow or widen to indicate decreases or increases in the strength of an individual topic or a group of topics in the associated documents. The river is shown within the context of a timeline and a corresponding textual presentation of external events},
	eventtitle = {{IEEE} Symposium on Information Visualization},
	pages = {115--123},
	booktitle = {{INFOVIS} 2000 Proceedings},
	author = {Havre, S. and Hetzler, B. and Nowell, L.},
	date = {2000},
	langid = {english},
	keywords = {act\_Visualizing, obj\_Tools},
}

@article{schulz_mechanic_2011,
	title = {The Mechanic Muse - What Is Distant Reading?},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2011/06/26/books/review/the-mechanic-muse-what-is-distant-reading.html?ref=review},
	abstract = {Ars longa,” the ancient saying goes, “vita brevis.” Art is long, life short, and the problem is intensifying. As the literary ars lurches exponentially more longa — accommodating the printing press, “Gravity’s Rainbow,” Google Books — our collective {TBR} pile towers ever more vertiginously overhead. Which raises a question: What are we mortal beings supposed to do with all these books?},
	journaltitle = {The New York Times},
	author = {Schulz, Kathryn},
	urldate = {2011-06-28},
	date = {2011-06-24},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@article{svensson_landscape_2010,
	title = {The Landscape of Digital Humanities},
	volume = {4},
	url = {http://digitalhumanities.org/dhq/vol/4/1/000080/000080.html},
	number = {1},
	journaltitle = {Digital Humanities Quarterly},
	shortjournal = {{DHQ}},
	author = {Svensson, Patrik},
	urldate = {2011-08-30},
	date = {2010},
	langid = {english},
	keywords = {meta\_Advocating, meta\_GiveOverview, obj\_DigitalHumanities},
}

@collection{ryan_johns_2014,
	location = {Baltimore},
	title = {The Johns Hopkins Guide to Digital Media},
	url = {https://jhupbooks.press.jhu.edu/content/johns-hopkins-guide-digital-media},
	abstract = {The study of what is collectively labeled "New Media"—the cultural and artistic practices made possible by digital technology—has become one of the most vibrant areas of scholarly activity and is rapidly turning into an established academic field, with many universities now offering it as a major. The Johns Hopkins Guide to Digital Media is the first comprehensive reference work to which teachers, students, and the curious can quickly turn for reliable information on the key terms and concepts of the field.},
	publisher = {Johns Hopkins Univ. Press},
	editor = {Ryan, Marie-Laure and Emerson, Lori and Robertson, Benjamin J.},
	urldate = {2014-05-07},
	date = {2014},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_Audio, obj\_Text, obj\_Video},
}

@article{zimmer_jargon_2011,
	title = {The Jargon of the Novel, Computed (The Mechanic Muse)},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2011/07/31/books/review/the-mechanic-muse-the-jargon-of-the-novel-computed.html?_r=2&ref=review},
	journaltitle = {The New York Times},
	author = {Zimmer, Ben},
	urldate = {2011-08-01},
	date = {2011-07-29},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}},
}

@article{chenhall_impact_1968,
	title = {The Impact of Computers on Archaeological Theory: An Appraisal and Projection},
	volume = {3},
	url = {http://www.jstor.org/discover/10.2307/30203971?uid=3737864&uid=2134&uid=2&uid=70&uid=4&sid=21105175796093},
	pages = {15--24},
	number = {1},
	journaltitle = {Computers and the Humanities},
	author = {Chenhall, Robert G},
	date = {1968-09},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Artefacts, obj\_DigitalHumanities, obj\_Tools},
}
@book{mani_imagined_2010,
	location = {Lincoln},
	title = {The imagined moment: time, narrative, and computation},
	isbn = {9780803229778  0803229771},
	url = {http://www.nebraskapress.unl.edu/product/Imagined-Moment,674226.aspx},
	shorttitle = {The imagined moment},
	abstract = {Time is a key aspect of narrative. It can advance a story, illuminate its role in our daily lives, and help us understand how events unfold. In this groundbreaking interdisciplinary work, Inderjeet Mani uses recent developments in linguistics and computer science to analyze the use of time in narrative form.
 
The Imagined Moment outlines directions for an emerging discipline of “corpus narratology,” an approach involving the computer analysis and interpretation of multimillion-word collections of narrative text. This approach, Mani explains, could alter the very foundations of narrative theory. Accordingly, he develops a computer representation for timelines and applies it to a variety of literary works. Among these are such classics as One Hundred Years of Solitude, “A Hunger Artist,” Swann’s Way, Jealousy, Candide, and “The Short Happy Life of Francis Macomber.” Along the way, Mani considers stories embedded in temporal cycles; the cognitive processes involved in the construal of events in time; the modeling of narrative progression in terms of changes in readers’ evaluation of characters; the study of variations of tempo in fiction; and time in computer-mediated forms of storytelling.},
	publisher = {University of Nebraska Press},
	author = {Mani, Inderjeet},
	date = {2010},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@article{parry_humanities_2010,
	title = {The Humanities Go Google},
	issn = {0009-5982},
	url = {http://chronicle.com/article/The-Humanities-Go-Google/65713/},
	journaltitle = {The Chronicle of Higher Education},
	author = {Parry, Marc},
	urldate = {2013-03-21},
	date = {2010-05-28},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@article{guldi_history_2012,
	title = {The History of Walking and the Digital Turn: Stride and Lounge in London, 1808–1851},
	volume = {84},
	issn = {00222801, 15375358},
	url = {http://www.jstor.org/stable/info/10.1086/663350},
	doi = {10.1086/663350},
	shorttitle = {The History of Walking and the Digital Turn},
	pages = {116--144},
	number = {1},
	journaltitle = {The Journal of Modern History},
	author = {Guldi, Joanna},
	urldate = {2012-04-17},
	date = {2012-03},
	langid = {english},
	keywords = {{AnalyzeStatistically}, X-{CHECK}},
}

@incollection{oakes_great_2012,
	location = {Amsterdam; Philadelphia},
	title = {The great mystery of the (almost) invisible translator.},
	isbn = {9789027203564  9027203563  9789027274786  9027274789},
	url = {https://benjamins.com/#catalog/books/scl.51.09ryb/details},
	abstract = {Machine-learning stylometric distance methods based on most-frequent-word frequencies are well-accepted and successful in authorship attribution. This study investigates the results of one of these methods, Burrows’s Delta, when applied to translations. Basing the empirical results on a number of corpora of literary translations, it shows that, except for some few highly adaptative translations, Delta usually fails to identify the translator and identifies the author of the original instead.},
	pages = {231--248},
	booktitle = {Quantitative methods in corpus-based translation studies : a practical guide to descriptive translation research},
	publisher = {John Benjamins Pub. Co.},
	author = {Rybicki, Jan},
	editor = {Oakes, Michael P and Ji, Meng},
	date = {2012},
	langid = {english},
}

@article{cohen_future_2005,
	title = {The Future of Preserving the Past},
	volume = {2},
	url = {http://chnm.gmu.edu/essays-on-history-new-media/essays/?essayid=39},
	abstract = {This article was originally published in {CRM}: The Journal of Heritage Stewardship 2, 2 (Summer, 2005): 6–19 and is reprinted here with permission.},
	pages = {6--19},
	number = {2},
	journaltitle = {{CRM}: The Journal of Heritage Stewardship},
	author = {Cohen, Daniel J.},
	urldate = {2009-12-12},
	date = {2005},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{holmes_federalist_1995,
	title = {The Federalist Revisited: New Directions in Authorship Attribution},
	volume = {10},
	url = {http://llc.oxfordjournals.org/content/10/2/111.abstract},
	doi = {10.1093/llc/10.2.111},
	shorttitle = {The Federalist Revisited},
	abstract = {The Federalist Papers, twelve of which are claimed by both Alexander Hamilton and James Madison, have long been used as a testing-ground for authorship attribution techniques despite the fact that the styles of Hamilton and Madison are unusually similar. This paper assesses the value of three novel stylometric techniques by applying them to the Federalist problem. The techniques examined are a multivariate approach to vocabulary richness, analysis of the frequencies of occurrence of sets of common high-frequency words, and use of a machine-learning package based on a ‘genetic algorithm’ to seek relational expressions characterizing authorial styles. All three approaches produce encouraging results to what is acknowledged to be a difficult problem.},
	pages = {111 --127},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Holmes, D I and Forsyth, R S},
	date = {1995-01-01},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview},
}

@inproceedings{shneiderman_eyes_1996,
	title = {The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations},
	url = {http://www.cs.ubc.ca/~tmm/courses/old533/readings/shneiderman96eyes.pdf},
	doi = {10.1109/VL.1996.545307},
	abstract = {A useful starting point for designing advanced graphical user interfaces is the visual information seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. The paper offers a task by data type taxonomy with seven data types (one, two, three dimensional data, temporal and multi dimensional data, and tree and network data) and seven tasks (overview, zoom, filter, details-on-demand, relate, history, and extracts)},
	pages = {336 -- 343},
	booktitle = {Proceedings of Visual Languages 1996},
	author = {Shneiderman, Ben},
	date = {1996},
	langid = {english},
}

@article{burnard_evolution_2013,
	title = {The Evolution of the Text Encoding Initiative: From Research Project to Research Infrastructure},
	rights = {{TEI} Consortium 2013 (Creative Commons Attribution-{NoDerivs} 3.0 Unported License)},
	issn = {2162-5603},
	url = {http://jtei.revues.org/811},
	doi = {10.4000/jtei.811},
	shorttitle = {The Evolution of the Text Encoding Initiative},
	abstract = {It is twenty-five years since the Text Encoding Initiative was first launched as a research project following an international conference funded by the {US} National Endowment for the Humanities. This article describes some key stages in its subsequent evolution from research project into research infrastructure. The {TEI}'s changing nature, we suggest, is partly a consequence of its close and highly responsive relation with an active user community, which may also explain both its longevity and its effectiveness as a part of the digital humanities research infrastructure.},
	issue = {Issue 5},
	journaltitle = {Journal of the Text Encoding Initiative},
	author = {Burnard, Lou},
	editora = {Blanke, Tobias and Romary, Laurent},
	editoratype = {collaborator},
	urldate = {2013-10-17},
	date = {2013-06-21},
	langid = {english},
	keywords = {goal\_Enrichment, meta\_GiveOverview, t\_Encoding},
}

@article{holmes_evolution_1998,
	title = {The Evolution of Stylometry in Humanities Scholarship},
	volume = {13},
	url = {http://llc.oxfordjournals.org/content/13/3/111.abstract},
	doi = {10.1093/llc/13.3.111},
	abstract = {This paper traces the historical development of the use of statistical methods in the analysis of literary style. Commencing with stylometry's early origins, the paper looks at both successful and unsuccessful applications, and at the internal struggles as statisticians search for a proven methodology. The growing power of the computer and the ready availability of machine-readable texts are transforming modern stylometry, which has now attracted the attention of the media. Stylometry's interaction with more traditional literary scholarship is also discussed.},
	pages = {111 --117},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	author = {Holmes, David I.},
	urldate = {2011-12-08},
	date = {1998},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview, t\_Stylometry},
}

@article{fink_evolution_2006,
	title = {The Evolution of Order in the Chapter Lengths of Trollope's Novels},
	volume = {21},
	url = {http://llc.oxfordjournals.org/content/21/3/275.abstract},
	doi = {10.1093/llc/fqi041},
	abstract = {This paper presents the results of a statistical analysis performed on the forty-seven novels of Anthony Trollope. It is shown that there is a trend in the distribution of chapter lengths for each novel towards a state of increased evenness. Some possible causes of the trend are considered, although the most probable explanation is conjectural.},
	pages = {275 --282},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	author = {Fink, Peter},
	urldate = {2011-10-05},
	date = {2006},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Literature},
}

@article{stein_effect_1975,
	title = {The effect of adrenaline and of alpha- and beta-adrenergic blocking agents on {ATP} concentration and on incorporation of 32Pi into {ATP} in rat fat cells},
	volume = {24},
	issn = {0006-2952},
	url = {https://profiles.uonbi.ac.ke/rachelmusoke/publications/effect-adrenaline-and-alpha-and-beta-adrenergic-blocking-agents-atp-concen},
	pages = {1659--1662},
	number = {18},
	journaltitle = {Biochemical pharmacology},
	shortjournal = {Biochem. Pharmacol.},
	author = {Stein, J M},
	date = {1975-09-15},
	langid = {english},
}

@book{lanham_economics_2006,
	location = {Chicago},
	title = {The Economics of Attention. Style and Substance in the Age of Information},
	url = {http://www.press.uchicago.edu/ucp/books/book/chicago/E/bo3680280.html},
	abstract = {If economics is about the allocation of resources, then what is the most precious resource in our new information economy? Certainly not information, for we are drowning in it. No, what we are short of is the attention to make sense of that information. 

With all the verve and erudition that have established his earlier books as classics, Richard A. Lanham here traces our epochal move from an economy of things and objects to an economy of attention. According to Lanham, the central commodity in our new age of information is not stuff but style, for style is what competes for our attention amidst the din and deluge of new media. In such a world, intellectual property will become more central to the economy than real property, while the arts and letters will grow to be more crucial than engineering, the physical sciences, and indeed economics as conventionally practiced. For Lanham, the arts and letters are the disciplines that study how human attention is allocated and how cultural capital is created and traded. In an economy of attention, style and substance change places. The new attention economy, therefore, will anoint a new set of moguls in the business world—not the {CEOs} or fund managers of yesteryear, but new masters of attention with a grounding in the humanities and liberal arts. 

Lanham’s The Electronic Word was one of the earliest and most influential books on new electronic culture. The Economics of Attention builds on the best insights of that seminal book to map the new frontier that information technologies have created.},
	publisher = {Univ. of Chicago Press},
	author = {Lanham, Richard A.},
	date = {2006},
	langid = {english},
}

@online{scheinfeldt_dividends_2014,
	title = {The Dividends of Difference: Recognizing Digital Humanities' Diverse Family Tree/s},
	url = {http://www.foundhistory.org/2014/04/07/the-dividends-of-difference-recognizing-digital-humanities-diverse-family-trees/},
	shorttitle = {The Dividends of Difference},
	titleaddon = {Found History},
	author = {Scheinfeldt, Tom},
	urldate = {2014-04-09},
	date = {2014-04-07},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@book{weller_digital_2011,
	title = {The Digital Scholar: How Technology Is Transforming Scholarly Practice},
	url = {https://www.bloomsburycollections.com/book/the-digital-scholar-how-technology-is-transforming-scholarly-practice/},
	abstract = {While industries such as music, newspapers, film and publishing have seen radical changes in their business models and practices as a direct result of new technologies, higher education has so far resisted the wholesale changes we have seen elsewhere. However, a gradual and fundamental shift in the practice of academics is taking place. Every aspect of scholarly practice is seeing changes effected by the adoption and possibilities of new technologies. This book will explore these changes, their implications for higher education, the possibilities for new forms of scholarly practice and what lessons can be drawn from other sectors.},
	author = {Weller, Martin},
	date = {2011},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@book{jacobs_building_2014,
	location = {London},
	title = {Building the Digital Infrastructure: Strategies for Supporting Education and Research},
	isbn = {9781856048569  185604856X},
	url = {http://www.amazon.de/Building-Digital-Infrastructure-Strategies-Supporting/dp/185604856X},
	abstract = {A cutting-edge analysis of topics such as open access and identity management, interoperability and shared services business models, and scholarly communications and research data management from the groundbreaking Digital Infrastructure team at {JISC}. The team provide an analysis of where we are now, looks at future trends, challenges and issues of sustainability and explores the strategies and approaches that are evolving to deal with the new environment. An effective digital infrastructure allows for the appropriate creation, management and exploitation of information resources and services to enable effective and high quality research and education. The focus is on supporting innovative abnd effective research and learning throught the development and implementation of a digital infrastructure for higher education. The experience and knowledge base of {JISC}'s Digital Infrastructure team is placed in a wider context to enable practitioners, service planners and users alike to easily apply the lessons. Readership: Academics, researchers and students of {LIS} and related disciplines including publishing and practitioners involved in the digital infrastructure including staff, librarians, archivists and records managers.},
	publisher = {Facet},
	author = {Jacobs, Neil and Dovey, Matthew and Bruce, Rachel},
	date = {2014},
	langid = {english},
	keywords = {goal\_Collaboration, obj\_AnyObject},
}

@online{meeks_digital_2013,
	title = {The Digital Humanities Contribution to Topic Modeling},
	url = {http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/},
	titleaddon = {Journal of Digital Humanities},
	author = {Meeks, Elijah},
	urldate = {2013-04-15},
	date = {2013-04-09},
	langid = {english},
}

@online{robertson_differences_2014,
	title = {The Differences between Digital History and Digital Humanities},
	url = {http://drstephenrobertson.com/2014/05/23/the-differences-between-digital-history-and-digital-humanities/},
	abstract = {For the last nine months I've spent much of my time exploring digital history. Part of becoming director of {RRCHNM} involved familiarizing myself with areas of work about which I had only passing kn...},
	titleaddon = {Dr Stephen Robertson},
	author = {Robertson, Stephen},
	urldate = {2014-05-24},
	date = {2014},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@book{freeman_development_2004,
	title = {The Development of Social Network Analysis: A Study in the Sociology of Science},
	isbn = {1594577145},
	url = {http://moreno.ss.uci.edu/91.pdf},
	shorttitle = {The Development of Social Network Analysis},
	abstract = {Ideas about social structure and social networks are very old. People have always believed that biological and social links among individuals are important. But it wasn't until the early 1930s that systematic research that explored the patterning of social ties linking individuals emerged. And it emerged, not once, but several times in several different social science fields and in several places. This book reviews these developments and explores the social processes that wove all these "schools" of network analysis together into a single coherent approach.},
	pagetotal = {218},
	publisher = {Empirical Press},
	author = {Freeman, Linton C.},
	date = {2004-07-23},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_Databases},
}

@incollection{berman_data_2003,
	location = {Chichester, {UK}},
	title = {The Data Deluge: An e-Science Perspective},
	isbn = {0470853190, 0470867167},
	url = {http://doi.wiley.com/10.1002/0470867167.ch36},
	shorttitle = {The Data Deluge},
	abstract = {Summary

This chapter contains sections titled:

    Introduction

    The Imminent Scientific Data Deluge

    Scientific Metadata, Information and Knowledge

    Data Grids and Digital Libraries

    Open Archives and Scholarly Publishing

    Digital Preservation and Data Curation

    Concluding Remarks

    Acknowledgements

    References},
	pages = {809--824},
	booktitle = {Wiley Series in Communications Networking \& Distributed Systems},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Hey, Tony and Trefethen, Anne},
	editor = {Berman, Fran and Fox, Geoffrey and Hey, Tony},
	urldate = {2011-11-18},
	date = {2003},
	langid = {english},
}

@article{sainte-marie_concept_2011,
	title = {The Concept of Evolution in the Origin of Species: A Computer-Assisted Analysis},
	volume = {26},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/26/3/329},
	doi = {10.1093/llc/fqr019},
	shorttitle = {The concept of evolution in the Origin of Species},
	abstract = {At the time Darwin first published the Origin of Species, the word ‘evolution’ was used by most biologists of the time to refer not only to specific development, as is the case today, but also to embryological development. Darwin's own stance in that matter is however open to debate, his rare use of the word making it hard to determine whether it is strictly specific or dual, and thus whether the author's conception of evolution is representative or ahead of its time. While this situation certainly stimulates philological, historical, and philosophical debates, it however complicates any attempt to settle the matter on a strict lexical basis, thus making standard text-mining techniques ineffective. To address this specific issue, a computer-assisted method for ‘reading Darwin between the lines’ is here attempted and described: by using an iterative concordance clustering algorithm, this approach aims at ‘digging’ into Darwin's concept of evolution as found in the sixth edition of the Origin of Species, regardless of any proper designation. In light of the results thus obtained, the concept of evolution in the sixth edition of the Origin of Species appears closer to its modern and strictly specific interpretation, inferences made to words related to embryological development being rather rare.},
	pages = {329--334},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Sainte-Marie, Maxime B and Meunier, Jean-Guy and Payette, Nicolas and Chartier, Jean-François},
	urldate = {2012-04-25},
	date = {2011-09-01},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_ContentAnalysis, goal\_Analysis, goal\_Interpretation},
}

@article{whallon_jr_computer_1972,
	title = {The Computer in Archaeology: A Critical Survey},
	volume = {7},
	url = {http://link.springer.com/article/10.1007/BF02403759?no-access=true},
	pages = {29--45},
	number = {1},
	journaltitle = {Computers and the Humanities},
	author = {Whallon Jr, Robert},
	date = {1972-09},
	langid = {english},
	keywords = {meta\_Assessing, meta\_GiveOverview, obj\_Artefacts, obj\_Tools},
}

@article{serra_computer_2012,
	title = {The Computer as Music Critic},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2012/09/16/opinion/sunday/the-computer-as-music-critic.html},
	journaltitle = {The New York Times},
	author = {Serrà, Joan and Arcos, Josep Lluís},
	urldate = {2012-09-17},
	date = {2012-09-15},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Music},
}

@article{wittig_computer_1977,
	title = {The Computer and the Concept of Text},
	volume = {11},
	url = {http://www.jstor.org/discover/10.2307/30199899?uid=3737864&uid=2134&uid=2&uid=70&uid=4&sid=21105175796093},
	pages = {211--215},
	number = {4},
	journaltitle = {Computers and the Humanities},
	author = {Wittig, Susan},
	date = {1977},
	langid = {english},
	keywords = {meta\_Theorizing, obj\_DigitalHumanities},
}

@book{kenny_computation_1982,
	title = {The computation of style: an introduction to statistics for students of literature and humanities},
	isbn = {9780080242828},
	url = {http://www.sciencedirect.com/science/book/9780080242811},
	shorttitle = {The computation of style},
	abstract = {Each year more and more scholars are becoming aware of the importance of the statistical study of literary texts. The present book is the first elementary introduction in English for those wishing to use statistical techniques in the study of literature. Unlike other introductions to statistics, it specifically emphasizes those techniques most useful in literary contexts and gives examples of their application from literary and linguistic material. The text is aimed at those with the minimum of mathematical background and gives exercises for the student and relevant statistical tables.},
	pagetotal = {196},
	publisher = {Pergamon Press},
	author = {Kenny, Anthony},
	date = {1982},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@article{mendenhall_characteristic_1887,
	title = {The Characteristic Curves of Composition},
	volume = {ns-9},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/ns-9/214S/237.citation},
	doi = {10.1126/science.ns-9.214S.237},
	pages = {237--246},
	number = {214},
	journaltitle = {Science},
	author = {Mendenhall, T. C.},
	urldate = {2011-12-14},
	date = {1887-03-11},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}
@article{burgess_book_1994,
	title = {The Book in the Machine. First Report on a Computerassisted Analysis of Die Wahlverwandtschaften},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0483.1994.tb01550.x/full},
	abstract = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0483.1994.tb01550.x/abstract},
	pages = {418--431},
	journaltitle = {German Life and Letters 47 (1994)},
	author = {Burgess, Gordon J.A.},
	date = {1994},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@inproceedings{rosen-zvi_author-topic_2004,
	title = {The author-topic model for authors and documents},
	url = {http://mimno.infosci.cornell.edu/info6150/readings/398.pdf},
	abstract = {We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation ({LDA}; Blei, Ng, \& Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 {NIPS} conference papers and 160,000 {CiteSeer} abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: {LDA} (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output.},
	pages = {487--494},
	booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
	publisher = {{AUAI} Press},
	author = {Rosen-Zvi, M. and Griffiths, Tom and Steyvers, Mark and Smith, P.},
	date = {2004},
	langid = {english},
	keywords = {meta\_Theorizing},
}

@article{goldhaber_attention_1997,
	title = {The attention economy and the net},
	volume = {2},
	url = {http://firstmonday.org/article/view/519/440},
	abstract = {If the Web and the Net can be viewed as spaces in which we will increasingly live our lives, the economic laws we will live under have to be natural to this new space. These laws turn out to be quite different from what the old economics teaches, or what rubrics such as "the information age" suggest. What counts most is what is most scarce now, namely attention. The attention economy brings with it its own kind of wealth, its own class divisions - stars vs. fans - and its own forms of property, all of which make it incompatible with the industrial-money-market based economy it bids fair to replace. Success will come to those who best accommodate to this new reality.},
	number = {4},
	journaltitle = {First Monday},
	author = {Goldhaber, Michael H.},
	date = {1997},
	langid = {english},
}

@article{dierks-ventling_attachment_1975,
	title = {The attachment of glutamine synthetase to brain membranes},
	volume = {13},
	issn = {0006-2944},
	url = {http://www.sciencedirect.com/science/article/pii/0006294475900782},
	abstract = {Up to 75\% of the activity of particulate bound rat brain glutamine synthetase (microsomal and mitochondrial) was shown to be latent, and solubilization by a detergent (0.3\% deoxycholate) was required in order to assay the total enzyme activity of brain. However, the particulate component was not different from the soluble one. The detergent had no effect on the specific activity of liver glutamine synthetase, but it considerably decreased the stability towards heat of both brain and liver enzyme. A comparison of brain enzyme with liver enzyme with regard to magnesium requirement showed that the enzyme activity of brain was inversely proportional of the magnesium concentration, whereas the liver enzyme was not affected by changes in magnesium concentration. A developmental study showed that the latency of the brain enzyme is only established after birth. It was concluded that the attachment of brain glutamine synthetase to membranes may sufficiently alter some of the requirements of the enzyme to result in distinctive differences of the brain and liver enzymes, and that this difference may play a functional role.},
	pages = {213--223},
	number = {3},
	journaltitle = {Biochemical medicine},
	shortjournal = {Biochem Med},
	author = {Dierks-Ventling, C and Cone, A L and Bessman, S P},
	date = {1975-07},
	langid = {english},
}

@incollection{holmes_analysis_1988,
	location = {Paris},
	title = {The Analysis of Literary Style – A Review},
	url = {http://www.jstor.org/discover/10.2307/2981893?uid=3737864&uid=2&uid=4&sid=21104511620561},
	abstract = {This paper considers the problem of quantifying literary style and looks at several variables which may be used as stylistic "fingerprints" of a writer. Statistics plays an important role in the sampling of texts, the building and fitting of models to linguistic data and in the testing of hypotheses, particularly in problems concerned with authorship attribution. The increasing availability of computer concordances of literary texts offers great possibilities to the analyst.},
	pages = {67--76},
	booktitle = {Vocabulary Structure and Lexical Richness},
	publisher = {Champion / Slatkine},
	author = {Holmes, David I.},
	editor = {Thoiron, Philippe and Labbé, Dominique and Serant, Daniel},
	date = {1988},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview, t\_Stylometry},
}

@article{goodman_ten_2014,
	title = {Ten Simple Rules for the Care and Feeding of Scientific Data},
	volume = {10},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1003542},
	doi = {10.1371/journal.pcbi.1003542},
	abstract = {In the early 1600s, Galileo Galilei turned a telescope toward Jupiter. In his log book each night, he drew to-scale schematic diagrams of Jupiter and some oddly moving points of light near it. Galileo labeled each drawing with the date. Eventually he used his observations to conclude that the Earth orbits the Sun, just as the four Galilean moons orbit Jupiter. History shows Galileo to be much more than an astronomical hero, though. His clear and careful record keeping and publication style not only let Galileo understand the solar system, they continue to let anyone understand how Galileo did it. Galileo's notes directly integrated his data (drawings of Jupiter and its moons), key metadata (timing of each observation, weather, and telescope properties), and text (descriptions of methods, analysis, and conclusions). Critically, when Galileo included the information from those notes in Sidereus Nuncius, this integration of text, data, and metadata was preserved, as shown in Figure 1. Galileo's work advanced the “Scientific Revolution,” and his approach to observation and analysis contributed significantly to the shaping of today's modern “scientific method”},
	pages = {e1003542},
	number = {4},
	journaltitle = {{PLoS} Comput Biol},
	shortjournal = {{PLoS} Comput Biol},
	author = {Goodman, Alyssa and Pepe, Alberto and Blocker, Alexander W. and Borgman, Christine L. and Cranmer, Kyle and Crosas, Merce and Di Stefano, Rosanne and Gil, Yolanda and Groth, Paul and Hedstrom, Margaret and Hogg, David W. and Kashyap, Vinay and Mahabal, Ashish and Siemiginowska, Aneta and Slavkovic, Aleksandra},
	urldate = {2014-05-16},
	date = {2014-04-24},
	langid = {english},
	keywords = {*****, goal\_Dissemination, goal\_Storage, obj\_Data, t\_OpenAccess},
}

@article{chenhall_archaeological_1971,
	title = {The Archaeological Data Bank: A Progress Report},
	volume = {5},
	url = {http://www.jstor.org/discover/10.2307/30199402?uid=3737864&uid=2&uid=4&sid=21104511620561},
	pages = {159--169},
	number = {3},
	journaltitle = {Computers and the Humanities},
	author = {Chenhall, Robert G},
	date = {1971-01},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Artefacts},
}

@article{binongo_application_1999,
	title = {The application of principal component analysis to stylometry},
	volume = {14},
	url = {http://llc.oxfordjournals.org/content/14/4/445.abstract},
	doi = {10.1093/llc/14.4.445},
	abstract = {In recent years principal component analysis has become popular for investigations in computational stylistics, particularly for studies of authorship. The mathematical nature of the theory that underpins the method makes it rather inaccessible to linguists and literary scholars. Consequently, confidence in its correct application is diminished. By first restricting the procedure to the use of two marker words, a pictorial description of its operation is derived. Some characteristics of the method are then examined. Finally, in the context of a Shakespearean example the technique is extended to p words, and suggestions are advanced to alleviate possible shortcomings.},
	pages = {445 --466},
	number = {4},
	journaltitle = {Literary and Linguistic Computing},
	author = {Binongo, {JNG} and Smith, {MWA}},
	urldate = {2011-10-05},
	date = {1999-12-01},
	langid = {english},
	keywords = {*****, act\_StylisticAnalysis, meta\_GiveOverview},
}

@book{amy_e._earhart_and_andrew_jewell_american_2011,
	title = {The American Literature Scholar in the Digital Age},
	url = {http://hdl.handle.net/2027/spo.9362034.0001.001},
	abstract = {Observing the title and concerns of this collection, many may wonder why we have chosen to focus on the American literature scholar; certainly the concerns of digital humanities are relevant across literary specializations. In fact, as other digital humanities scholarship demonstrates, the humanities as a boundary is itself suspect: it is not uncommon to see collaborations between a literary scholar, a computer scientist, and a librarian in digital humanities work. The artificial distinctions that have replicated the discipline divisions have become less relevant to those working in digital humanities, who often group around subject matter, not training. Add to this the increasing breakdown of national boundaries in literary studies, and perhaps it seems antiquated or anathema to reproduce American as a term with which to saddle a supposedly cutting-edge collection of essays.},
	author = {Amy E. Earhart \{and\} Andrew Jewell, Editors},
	date = {2011},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, obj\_Literature},
}

@incollection{siemens_text_2004,
	location = {Oxford},
	edition = {Hardcover},
	title = {Text Encoding},
	isbn = {1405103213},
	url = {http://www.digitalhumanities.org/companion/view?docId=blackwell/9781405103213/9781405103213.xml&chunk.id=ss1-3-5&toc.depth=1&toc.id=ss1-3-5&brand=default},
	series = {Blackwell Companions to Literature and Culture},
	abstract = {Text encoding holds a special place in humanities computing. It is not only of considerable practical importance and commonly used, but it has proven to be an exciting and theoretically productive area of analysis and research. Text encoding in the humanities has also produced a considerable amount of interesting debate – which can be taken as an index of both its practical importance and its theoretical significance.},
	booktitle = {A Companion to Digital Humanities},
	publisher = {Blackwell Publishing Professional},
	author = {Renear, Allen H.},
	editor = {Siemens, Ray and Unsworth, John and Schreibman, Susan},
	urldate = {2010-05-17},
	date = {2004},
	langid = {english},
	keywords = {X-{CHECK}, act\_Publishing, meta\_GiveOverview, meta\_Theorizing, t\_Encoding},
}

@incollection{burrows_textual_2004,
	location = {Oxford},
	title = {Textual Analysis},
	url = {http://digitalhumanities.org/companion/view?docId=blackwell/9781405103213/9781405103213.xml&chunk.id=ss1-4-4&toc.id=0&brand=9781405103213_brand},
	abstract = {The object of this paper is to show that computer-assisted textual analysis can be of value in many different sorts of literary inquiry, helping to resolve some questions, to carry others forward, and to open entirely new ones. The emphasis will not be on the straightforward, albeit valuable, business of gathering specimens of chosen phenomena for close study – the business of concordances and tagged sets. It will fall rather on the form of computational stylistics in which all the most common words (whatever they may be) of a large set of texts are subjected to appropriate kinds of statistical analysis.},
	pages = {323--347},
	booktitle = {A Companion to Digital Humanities},
	publisher = {Blackwell Publishing},
	author = {Burrows, John},
	editor = {Schreibman, Susan and Siemens, Ray and Unsworth, John},
	date = {2004},
	langid = {english},
	keywords = {goal\_Analysis, meta\_GiveOverview},
}

@article{hearst_texttiling:_1997,
	title = {{TextTiling}: Segmenting text into multi-paragraph subtopic passages},
	url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.144.4470},
	shorttitle = {{TextTiling}},
	abstract = {{TextTiling} is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics. The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution. The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts. Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization.},
	pages = {33--64},
	journaltitle = {Computational Linguistics},
	author = {Hearst, Marti A},
	date = {1997},
	langid = {english},
	keywords = {act\_Annotating, goal\_Enrichment, obj\_Text},
}

@article{dimpel_textstatistische_2004,
	title = {Textstatistische Analysen an mittelhochdeutschen Texten},
	url = {http://www.computerphilologie.uni-muenchen.de/jg04/dimpel.html},
	abstract = {The following paper offers an introduction to a set of programs called {ErMaStat}, which is an instrument for textual analysis of Middle High German rhymed epics. With the help of {ErMaStat}, texts can be analysed with respect to statistical differences, for example in the case of uncertain authorship or if the relative chronology of works of one author is unclear. {ErMaStat} is the first set of programs in the area of German Medieval Studies which makes it possible to record a large number of different textual features automatically and without any manual input, and subsequently carries out a statistical analysis. The scope of the textual features that can be examined is wide: It ranges from simple quantitative features such as words per line, function words and grammatical phenomena to an automatic analysis of meter. The power of {ErMaStat} is demonstrated on a number of scholarly issues in German Medieval Literature. It can be shown that the results of the so-called ›schallanalytische Untersuchungen‹ practised by Elisabeth Karg-Gasterstädt are more plausible than hitherto assumed. Another result is that statistical evidence can be presented for the theory that the first thousand lines of Hartmann von Aue's Iwein were written immediately after Hartmann had finished Erec.

Im Rahmen meiner Dissertation über Computergestützte textstatistische Untersuchungen an mittelhochdeutschen Texten habe ich das Programmpaket {ErMaStat} entwickelt und dokumentiert, ein Instrument der Textanalyse für mittelhochdeutsche Versromane. Mit ihm können Texte in Hinblick auf statistische Unterschiede untersucht werden, etwa bei Fragen unklarer Autorschaft oder der Chronologie innerhalb des Werks eines Autors. Mit {ErMaStat} wurde erstmals in der germanistischen Mediävistik ein Programmpaket vorgestellt, das eine Vielzahl von Textmerkmalen automatisch erfasst und statistisch auswertet. Die Bandbreite der untersuchten Textmerkmale ist groß: Sie bewegt sich von einfachen quantitativen Merkmalen wie Wörtern pro Zeile über Funktionswörter und grammatikalische Phänomene bis hin zu einer automatischen metrischen Analyse. Im Folgenden will ich das Programmpaket vorstellen und die wichtigsten Ergebnisse diskutieren.},
	pages = {95--118},
	journaltitle = {Jahrbuch für Computerphilologie 6 (2004)},
	author = {Dimpel, Friedrich Michael},
	date = {2004},
	langid = {german},
	keywords = {X-{CHECK}},
}

@article{mittelbach_textgrid:_2009,
	title = {{TextGrid}: Virtuelle Arbeitsumgebung für die Geisteswissenschaften},
	url = {http://www.springer.com/computer/ai/journal/13218},
	shorttitle = {{TextGrid}},
	abstract = {Dies ist ein Bericht über das Projekt {TextGrid}, das von Februar 2006 bis Mai 2009 durch das Bundesministerium für Bildung und Forschung gefördert wurde. Das Ziel des Projekts ist die Entwicklung einer offenen Plattform, die vielfältige Bedürfnisse und Perspektiven im Gebiet der Textwissenschaften einbeziehen wird. Der Fokus liegt in erster Instanz auf den Textdaten und dem Bereich der Editionswissenschaften in funktionaler Verknüpfung mit den anderen Bereichen. Weitere fachspezifische Funktionalitäten und zusätzliche Datentypen können aber aufgrund der Flexibilität der zugrunde liegenden Infrastrukur mit wenig zusätzlichem Aufwand integriert werden.},
	pages = {36--39},
	number = {4},
	journaltitle = {{KI} Künstliche Intelligenz},
	author = {Mittelbach, Jens and Vitt, Thorsten and Kerzel, Martina},
	date = {2009},
	langid = {german},
	keywords = {act\_Collaborating, goal\_Collaboration, obj\_Infrastructures},
}

@incollection{kuster_textgrid:_2010,
	location = {Würzburg},
	title = {{TextGrid}: {eScholarship} und der Fortschritt der Wissenschaft durch vernetzte Angebote},
	url = {http://www.textgrid.de/fileadmin/TextGrid/veroeffentlichungen/kuester-ludwig-aschenbrenner_textgrid-escholarship.pdf},
	abstract = {Das {TextGrid}-Konsortium bringt acht uber ganz ¨
Deutschland verteilte Institutionen aus dem akademischen und
kommerziellen Sektor zusammen, um eine modulare Plattform fur¨
verteilte und kooperative wissenschaftliche Textdatenverarbeitung
zu erarbeiten. Unter Einsatz der Grid-Infrastruktur fuhrt es ¨
verschiedene Datenstrukturen aus unterschiedlichen Projekten
in einem virtuellen Korpus zusammen, das als ganzes oder in
ausgewahlten Teilen durchsucht und analysiert werden kann. ¨
Als dezidiert auf Kollaboration auch uber die Grenzen von ¨
{TextGrid} selbst ausgerichtete Plattform verandert sie auch den ¨
Arbeitsalltag des geisteswissenschaftlichen Forschers, weg vom
Bild des weitgehend isoliert arbeitenden Philologen hin zur
Teamarbeit an Daten und Werkzeugen gleichermaßen. Sie bildet
aber auch auf Informatikseite ein Beispiel fur ein komplexes, ¨
lose gekoppeltes Digitales Okosystem mit Spielern aus vielen ¨
Disziplinen und Landern.},
	pages = {193--205},
	booktitle = {Wissensspeicher in digitalen Räumen.},
	publisher = {Ergon Verlag},
	author = {Küster, Marc Wilhelm and Ludwig, Christoph and Aschenbrenner, Andreas and Al-Hajj, Yahya},
	editor = {Sieglerschmidt, Jörg and Ohly, H. Peter},
	date = {2010},
	langid = {german},
	keywords = {act\_Collaborating, goal\_Collaboration, obj\_Infrastructures, t\_Encoding},
}

@incollection{heiden_txm_2010,
	location = {Rome},
	title = {{TXM} : Une plateforme logicielle open-source pour la textométrie – conception et développement},
	url = {https://halshs.archives-ouvertes.fr/halshs-00549779},
	abstract = {The research project Federation and Research Developments in Textometry around the creation of an Open- Source Platform distributes its {XML}-{TEI} encoded corpus textometric analysis platform online. The design of this platform is based on a synthesis of features of existing textometric software. It relies on identifying the open-source software technology available and effectively processing digital resources encoded in {XML} and Unicode, and on a state of the art of open-source full-text search engines on structured and annotated corpora. The architecture is based on a Java toolkit component articulating a search engine ({IMS} {CWB}), a statistical computing environment (R) and a module for importing {XML}-{TEI} encoded corpora. The platform is distributed as an open-source toolkit for developers and in the form of two applications for end users of textometry: a local application to install on a workstation (Windows or Linux) and an online web application. Still early in its development, the platform implements at present only a few essential features, but its distribution in open-source already allows an open community development. This should facilitate its development and integration of new models and methods.},
	pages = {1021--1032},
	booktitle = {Statistical Analysis of Textual Data -Proceedings of 10th International Conference {JADT} 2010},
	publisher = {Edizioni Universitarie di Lettere Economia Diritto},
	author = {Heiden, Serge and Magué, Jean-Philippe and Pincemin, Bénédicte},
	editor = {Bolasco, Sergio and Chiari, Isabella and Giuliano, Luca},
	date = {2010},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Tools},
}

@collection{peursen_text_2010,
	location = {Leiden ; Boston},
	title = {Text comparison and digital creativity: the production of presence and meaning in digital text scholarship},
	isbn = {9789004188655},
	url = {http://www.brill.com/text-comparison-and-digital-creativity},
	series = {Scholarly communication},
	shorttitle = {Text comparison and digital creativity},
	abstract = {In fourteen thoughtful essays this book reports and reflects on the many changes that a digital workflow brings to the world of original texts and textual scholarship, and the effect on scholarly communication practices. The spread of digital technology across philology, linguistics and literary studies suggests that text scholarship is taking on a more laboratory-like image. The ability to sort, quantify, reproduce and report text through computation would seem to facilitate the exploration of text as another type of quantitative scientific data. However, developing this potential also highlights text analysis and text interpretation as two increasingly separated sub-tasks in the study of texts. The implied dual nature of interpretation as the traditional, valued mode of scholarly text comparison, combined with an increasingly widespread reliance on digital text analysis as scientific mode of inquiry raises the question as to whether the reflexive concepts that are central to interpretation – individualism, subjectivity – are affected by the anonymised, normative assumptions implied by formal categorisations of text as digital data.},
	pagetotal = {296},
	number = {v. 1},
	publisher = {Brill},
	editor = {Peursen, W. Th van and Thoutenhoofd, Ernst D. and Weel, Adriaan van der},
	date = {2010},
	keywords = {goal\_Enrichment, obj\_Manuscripts, t\_Encoding},
}

@book{joachims_text_1998,
	title = {Text Categorization with Support Vector Machines: Learning with Many Relevant Features},
	url = {http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf},
	shorttitle = {Text Categorization with Support Vector Machines},
	abstract = {This paper explores the use of Support Vector Machines ({SVMs}) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies, why {SVMs} are appropriate for this task. Empirical results support the theoretical findings. {SVMs} achieve substantial improvements over the currently best performing methods and they behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning.},
	author = {Joachims, Thorsten},
	date = {1998},
	keywords = {{AnalyzeStatistically}},
}

@thesis{schwiebert_tesla_2012,
	location = {Köln},
	title = {Tesla - Ein virtuelles Labor für experimentelle Computer- und Korpuslinguistik},
	url = {http://kups.ub.uni-koeln.de/4571/},
	abstract = {Linguistisch motivierte Komponentensysteme bieten die Möglichkeit, Prozessketten zur maschinellen Annotation natürlichsprachlicher Daten zu definieren und auszuführen. Bisherige Ansätze unterliegen dabei verschiedenen Einschränkungen: So werden i.d.R. datenorientierte Austauschformate verwendet, die eine Abbildung generierter Daten auf proprietäre Metaformate erfordern, wodurch die Flexibilität bei der Implementation neuer Komponenten reduziert wird. Zudem wird der Reproduzierbarkeit von Ergebnissen nur eine niedrige Priorität eingeräumt, was die Nachvollziehbarkeit und Adaption neuer Verfahren erschwert. In dieser Arbeit wird mit Tesla (Text Engineering Software Laboratory) ein alternatives Komponentensystem vorgestellt, das die skizzierten Kritikpunkte ebenso wie weitere Nachteile und Einschränkungen derartiger Systeme vermeidet. Anhand eines Verfahrens zur automatischen Extraktion syntaktischer Strukturen, das auf die distributionelle Analyse nach Harris zurückgeführt werden kann, werden zunächst die Anforderungen, denen ein computerlinguistisches Komponentensystem genügen muss, konkretisiert und diskutiert. Im Anschluss daran werden verschiedene Frameworks hinsichtlich dieser Anforderungen evaluiert, um schließlich Konzept, Design und Implementation von Tesla vorzustellen und dadurch zu verdeutlichen, dass das beschriebene Strukturierungsverfahren in Form verschiedener experimenteller Versuchsaufbauten analysiert, erweitert und auf neue Untersuchungsgegenstände angewendet wird.},
	institution = {Universität zu Köln},
	type = {phdthesis},
	author = {Schwiebert, Stephan},
	date = {2012},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, act\_StructuralAnalysis, obj\_Language},
}

@collection{burnard_tei_2010,
	location = {Charlottesville},
	edition = {Version 1.9.1, March 2011},
	title = {{TEI} P5: Guidelines for Electronic Text Encoding and Interchange},
	url = {http://www.tei-c.org/release/doc/tei-p5-doc/en/html/index.html},
	shorttitle = {{TEI} P5},
	abstract = {These Guidelines have been developed and are maintained by the Text Encoding Initiative Consortium ({TEI}); see iv.2. Historical Background. They are addressed to anyone who works with any kind of textual resource in digital form.},
	publisher = {{TEI} Consortium},
	editor = {Burnard, Lou and Bauman, Syd},
	urldate = {2011-08-26},
	date = {2010},
	langid = {english},
}

@online{burnard_tei_2006,
	title = {{TEI} Lite: Encoding for Interchange: an introduction to the {TEI} (Revised for {TEI} P5 release)},
	url = {http://www.tei-c.org/release/doc/tei-p5-exemplars/html/tei_lite.doc.html},
	abstract = {The Text Encoding Initiative ({TEI}) Guidelines are addressed to anyone who wants to interchange information stored in an electronic form. They emphasize the interchange of textual information, but other forms of information such as images and sound are also addressed. The Guidelines are equally applicable in the creation of new resources and in the interchange of existing ones.

The Guidelines provide a means of making explicit certain features of a text in such a way as to aid the processing of that text by computer software running on different machines. This process of making explicit we call markup or encoding. Any textual representation on a computer uses some form of markup; the {TEI} came into being partly because of the enormous variety of mutually incomprehensible encoding schemes currently besetting scholarship, and partly because of the expanding range of scholarly uses now being identified for texts in electronic form.},
	titleaddon = {Text Encoding Inititative},
	author = {Burnard, Lou and Sperberg-{McQueen}, C. M.},
	urldate = {2010-05-21},
	date = {2006},
	langid = {english},
	keywords = {t\_Encoding},
}

@inproceedings{anderson_taking_2012,
	location = {Cologne},
	title = {Taking the Long View: From e-Science Humanities to Humanities Digital Ecosystems Sheila Anderson and Tobias Blanke},
	url = {http://www.cceh.uni-koeln.de/events/CologneDialogue/Controversy5},
	abstract = {Fifth controversy: Big structures or lightweight webs. What is the most sensible technical template for research infrastructures for the Digital Humanities?},
	eventtitle = {The Cologne Dialogue on Digital Humanities 2012},
	booktitle = {The Cologne Dialogue on Digital Humanities 2012},
	publisher = {Cologne Center for {eHumanities} {CCeH}},
	author = {Anderson, Sheila and Blanke, Tobias},
	date = {2012-05},
	langid = {english},
	keywords = {X-{CHECK}, meta\_Advocating, meta\_Assessing, obj\_Infrastructures},
}

@article{meister_tagging_2005,
	title = {Tagging Time in Prolog. The Temporality Effect Project},
	url = {http://llc.oxfordjournals.org/content/20/supplement/107.abstract},
	abstract = {This article combines a brief introduction into a particular philosophical theory of ‘time’ with a demonstration of how this theory has been implemented in a Literary Studies oriented Humanities Computing project. The aim of the project was to create a model of text-based time cognition and design customized markup and text analysis tools that help to understand “how time works”: more precisely, how narratively organised and communicated information motivates readers to generate the mental image of a chronologically organized world. The approach presented is based on the unitary model of time originally proposed by {McTaggart}, who distinguished between two perspectives onto time, the so-called A- and B-series. The first step towards a functional Humanities Computing implementation of this theoretical approach was the development of {TempusMarker}—a software tool providing automatic and semi-automatic markup routines for the tagging of temporal expressions in natural language texts. In the second step we discuss the principals underlying {TempusParser}—an analytical tool that can re-construct temporal order in events by way of an algorithm-driven process of analysis and recombination of textual segments during which the ‘time stamp’ of each segment as indicated by the temporal tags is interpreted.},
	pages = {107--124},
	journaltitle = {Literary and Linguistic Computing 2005 Elektronische Version siehe},
	author = {Meister, Jan Christoph},
	date = {2005},
	langid = {english},
	note = {Meister, Jan Christoph: Tagging Time in Prolog. The Temporality Effect Project. In: (Ders.): Literary and Linguistic Computing 2005, S. 107-124. Elektronische Version siehe},
	keywords = {{AnalyzeQualitatively}, obj\_Literature},
}

@thesis{hermes_textprozessierung_2012,
	location = {Köln},
	title = {Textprozessierung - Design und Applikation},
	url = {http://kups.ub.uni-koeln.de/4561/},
	abstract = {Die wissenschaftliche Kommunikation und der Austausch von Forschungsergebnissen beruhte lange Zeit einzig auf der Veröffentlichung und der Rezeption von Fachbüchern und -artikeln. Erst in der jüngeren Vergangenheit wurden auch Lösungen entworfen, wie die dem Forschungsprozess zugrundeliegenden sowie die aus diesem resultierenden Daten ausgetauscht werden können. Eine zentrale Rolle spielt dabei die beständig fortschreitende Entwicklung innerhalb der Informationstechnologie. Im Rahmen dieser Arbeit wurde ein Software-System entwickelt, das es erlaubt, Experimente auszutauschen. Damit ist ein Wissenschaftler in der Lage, die Grundlage seiner empirischen Forschung direkt weiterzugeben. Dieses System ist das Text Engineering Software Laboratory, kurz Tesla. Es stellt eine Arbeitsumgebung für Wissenschaftler, die auf textuellen Daten arbeiten, bereit. Innerhalb dieser Arbeitsumgebung können in einem Client Experimente mithilfe eines graphischen Workflow-Editors sowie diverser Konfigurations-Editoren zusammengestellt werden. Diese werden auf einem Server ausgeführt und können dann wieder im Client auf unterschiedliche Arten visualisiert werden. Die Experimente werden dabei vollständig dokumentiert (Ausgangsdaten, angewendete Verfahren, Resultate). Diese Dokumentation kann exportiert und distribuiert werden, so dass die Experimente jederzeit von anderen Nutzern des Systems reproduziert werden können. Die Arbeit geht zunächst darauf ein, welche Bereiche der Wissenschaft in das Feld der Textprozessierung fallen. Daraus werden Anforderungen abgeleitet, welche von diesen Wissenschaften als Basis für Forschungen an ihrem Gegenstandsbereichen und deren Weitergabe gestellt werden. Auf dieser Grundlage wird das System Tesla vorgestellt, das den formulierten Ansprüchen gerecht wird. Dabei werden die wichtigsten Features behandelt, die Tesla dem Anwender bietet. Die Demonstration des Systems erfolgt am Beispiel einer Analyse des sogenannten Voynich-Manuskripts. Dieses Dokument wurde 1912 in Italien entdeckt wurde und stammt mutmaßlich aus dem 15. Jahrhundert. Das Manuskript enthält einen Text eines unbekannten Autors, dessen Inhalt bisher nicht entschlüsselt werden konnte. Bisher wurde auch noch kein Verschlüsselungsverfahren gefunden, das einen vergleichbaren Text erzeugt, was sich mit dieser Arbeit ändert.},
	institution = {Universität zu Köln},
	type = {phdthesis},
	author = {Hermes, Jürgen},
	date = {2012},
	langid = {german},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, obj\_Language},
}

@book{buch_text_2008,
	location = {Berlin},
	title = {Text Mining : zur automatischen Wissensextraktion aus unstrukturierten Dokumenten},
	isbn = {9783836495509},
	url = {http://www.amazon.de/Text-Mining-Wissensextraktion-unstrukturierten-Textdokumenten/dp/3836495503},
	shorttitle = {Text Mining},
	abstract = {Wir befinden uns mitten im sogenannten Informationszeitalter und das Internet wird mehr und mehr zum vorherrschenden Medium.Allerdings hat sich der Zugang zur eigentlichen Basis des Internets - nämlich dem Text - kaum verbessert. Mit Text bzw. Sprache wird Wissen transportiert und gespeichert. Es ist jedoch unmöglich die Gesamtheit aller Textquellen zu einem bestimmten Thema zu erfassen, geschweige denn zu verstehen. Das Problem ist die Perspektive aus der Texte betrachtet werden. Herkömmliche Suchmaschinen bieten einen unidirektionalen Zugang zur Masse der vorhandenen Textdokumente und liefern nach Relevanz sortierte Dokumentlisten. Der Nutzer muss sich selbst ein Bild von den Inhalten der Dokumente machen und Rückschlüsse über die im Text enthaltenen Informationen und Zusammenhänge ziehen. Text-Mining-Lösungen versprechen den Zugang zuden in Texten verborgenen Wissensstrukturen. Bastian Buch untersucht in diesem Werk die Grundlagen des Text Mining, zeigt aktuelle Entwicklungen auf und entwickelt exemplarisch eine webbasierte Text-Mining-Anwendung bei der aus einem großen Textkorpus extrahierte semantische Informationen geografisch abgebildet werden.},
	publisher = {{VDM} Verl. Müller},
	author = {Buch, Bastian},
	date = {2008},
	langid = {german},
	keywords = {act\_ContentAnalysis, goal\_Analysis, t\_InformationRetrieval},
}
@book{heyer_text_2006,
	location = {Herdecke [u.a.]},
	title = {Text Mining: Wissensrohstoff Text: Konzepte, Algorithmen, Ergebnisse},
	isbn = {3937137300 9783937137308},
	url = {http://www.amazon.de/Text-Mining-Wissensrohstoff-Algorithmen-Ergebnisse/dp/3937137300},
	shorttitle = {Text Mining},
	abstract = {Ein großer Teil des Weltwissens befindet sich in Form digitaler Texte im Internet oder in Intranets. Heutige Suchmaschinen nutzen diesen Wissensrohstoff nur rudimentär: Sie können semantische Zusammen-hänge nur bedingt erkennen. Alle warten auf das semantische Web, in dem die Ersteller von Text selbst die Semantik einfügen. Das wird aber noch lange dauern. Es gibt jedoch eine Technologie, die es bereits heute ermöglicht semantische Zusammenhänge in Rohtexten zu analysieren und aufzubereiten. Das Forschungsgebiet "Text Mining" ermöglicht es mit Hilfe statistischer und musterbasierter Verfahren, Wissen aus Texten zu extrahieren, zu verarbeiten und zu nutzen. Hier wird die Basis für die Suchmaschinen der Zukunft gelegt.},
	publisher = {W3L-Verl.},
	author = {Heyer, Gerhard and Quasthoff, Uwe and Wittig, Thomas},
	date = {2006},
	langid = {german},
}

@inproceedings{kuster_textgrid_2007,
	title = {Textgrid as a Digital Ecosystem},
	url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4233763},
	doi = {10.1109/DEST.2007.372029},
	abstract = {The {TextGrid} project brings together eight German institutions from both academia and the commercial sector to "create a community grid for the collaborative editing, annotation, analysis and publication of specialist texts". Leveraging the grid infrastructure, textual data and supporting images from various research projects and content providers such as archives and libraries can fuse into a virtual corpus that can be seamlessly searched and analyzed. {TextGrid} will be a key enabler for collaborative textual scholarship, aiming at overcoming current isolation in research and facilitating cooperative working methods and the sharing of resources, content and software agents alike. It will also enable quantitative and comparative studies across corpora on a scale that might otherwise have been impossible to achieve.},
	pages = {506--511},
	booktitle = {Digital {EcoSystems} and Technologies Conference, 2007. {DEST} '07. Inaugural {IEEE}-{IES}},
	author = {Küster, Marc Wilhelm and Ludwig, Christoph and Aschenbrenner, Andreas},
	date = {2007},
	langid = {english},
	keywords = {act\_Collaborating, goal\_Collaboration, obj\_Infrastructures},
}

@article{gietz_textgrid_2006,
	title = {{TextGrid} and {eHumanities}},
	url = {http://www.textgrid.de/fileadmin/publikationen/Gietz_et_al.-2006.pdf},
	abstract = {{TextGrid} is a new Grid project in the framework of the
German D-Grid initiative, with the aim to deploy Grid
technologies for humanities scholars working on histori-
cal (German) texts. Its two roots, humanities computing
and {eScience} (Grid computing used by research together
with modern communication technologies), are the basis
for {TextGrid} to provide pioneer work in {eHumanities}. After
summarizing Humanities Computing and modern network
technologies, community expectations in the fields of philo-
logical edition and other application areas are set forth,
from which functional requirements such as modularity, dis-
tribution, etc. are distilled. The first version of the {TextGrid}
architecture was designed in accordance with these require-
ments, and focuses on openness by standard conformance
and encapsulation. It provides storage Grid services via
a pure Web Services interface to dedicated Web Services
tools for different aspects of text processing, analysis and
retrieval. This platform aims to provide easily usable tools
for scholars, but also specifies interfaces for external pro-
gram developers to add functionality},
	journaltitle = {{eHumanities} - an emerging discipline. Workshop in the Conference on e-Science and Grid Computing},
	author = {Gietz, Peter and Aschenbrenner, Andreas and Büdenbender, Stefan and Jannidis, Fotis and Küster, Marc W. and Ludwig, Christoph and Pempe, Wolfgang and Vitt, Thorsten and Wegstein, Werner and Zielinski, Andrea},
	date = {2006},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@collection{deegan_text_2009,
	location = {Farnham, England ; Burlington, {VT}},
	title = {Text editing, print and the digital world},
	isbn = {9780754673071},
	url = {http://www.ashgate.com/isbn/9780754673071},
	series = {Digital research in the arts and humanities},
	abstract = {Traditional critical editing, defined by the paper and print limitations of the book, is now considered by many to be inadequate for the expression and interpretation of complex works of literature. At the same time, digital developments are permitting us to extend the range of text objects we can reproduce and investigate critically - not just books, but newspapers, draft manuscripts and inscriptions on stone. Some exponents of the benefits of new information technologies argue that in future all editions should be produced in digital or online form. By contrast, others point to the fact that print, after more than five hundred years of development, continues to set the agenda for how we think about text, even in its non-print forms. This important book brings together leading textual critics, scholarly editors, technical specialists and publishers to discuss whether and how existing paradigms for developing and using critical editions are changing to reflect the increased commitment to and assumed significance of digital tools and methodologies.},
	pagetotal = {205},
	publisher = {Ashgate},
	editor = {Deegan, Marilyn and Sutherland, Kathryn},
	date = {2009},
	langid = {english},
	keywords = {goal\_Enrichment, t\_Encoding},
}

@book{jockers_text_2014,
	title = {Text Analysis with R for Students of Literature},
	isbn = {978-3-319-03164-4},
	url = {http://www.springer.com/statistics/computational+statistics/book/978-3-319-03163-7},
	abstract = {Text Analysis with R for Students of Literature is written with students and scholars of literature in mind but will be applicable to other humanists and social scientists wishing to extend their methodological tool kit to include quantitative and computational approaches to the study of text. Computation provides access to information in text that we simply cannot gather using traditional qualitative methods of close reading and human synthesis. Text Analysis with R for Students of Literature provides a practical introduction to computational text analysis using the open source programming language R. R is extremely popular throughout the sciences and because of its accessibility, R is now used increasingly in other research areas. Readers begin working with text right away and each chapter works through a new technique or process such that readers gain a broad exposure to core R procedures and a basic understanding of the possibilities of computational text analysis at both the micro and macro scale. Each chapter builds on the previous as readers move from small scale “microanalysis” of single texts to large scale “macroanalysis” of text corpora, and each chapter concludes with a set of practice exercises that reinforce and expand upon the chapter lessons. The book’s focus is on making the technical palatable and making the technical useful and immediately gratifying.},
	pagetotal = {194},
	author = {Jockers, Matthew L.},
	urldate = {2014-09-11},
	date = {2014},
	langid = {english},
	keywords = {act\_NetworkAnalysis, act\_StylisticAnalysis, goal\_Analysis, obj\_Text},
}

@inproceedings{kermes_text_2003,
	title = {Text Analysis Meets Corpus Linguistics},
	url = {ftp://ftp.ims.uni-stuttgart.de/pub/Users/kermes/papers/03_CL_KermesEvert.doc},
	abstract = {In recent years, there has been rising interest to using evidence derived from automatic syntactic analysis in large-scale corpus studies. Ideally, of course, corpus linguists would prefer to have access to the wealth of structural and featural information provided by a full parser based on a complex grammar},
	pages = {402--411},
	booktitle = {Proceedings of Corpus Linguistics},
	publisher = {{UCREL}},
	author = {Kermes, Hannah and Evert, Stefan},
	editor = {Archer, Dawn and Rayson, Paul and Wilson, Andrew and {McEnery}, Tony},
	date = {2003},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, goal\_Enrichment, obj\_Tools},
}

@inproceedings{kermes_text_2008,
	location = {Geneva},
	title = {Text Analysis Meets Computational Lexicography},
	url = {http://www.aclweb.org/anthology/C04-1123},
	abstract = {More and more text corpora are available electronically. They contain information about linguistic and lexicographic properties of words, and word combinations. The amount of data is too large to extract the information manually. Thus, we need means for a (semi-)automatic processing, i.e., we need to analyse the text to be able to extract the relevant information.The question is what are the requirements for a text analysing tool, and do existing systems meet the needs of lexicographic acquisition. The hypothesis is that the better and more detailed the off-line annotation, the better and faster the on-line extraction. However, the more detailed the off-line annotation, the more complex the grammar, the more time consuming and difficult the grammar development, and the slower the parsing process.For the application as an analyzing tool in computational lexicography a symbolic chunker with a hand-written grammar seems to be a good choice. The available chunkers for German, however, do not consider all of the additional information needed for this task such as head lemma, morpho-syntactic information, and lexical or semantic properties, which are useful if not necessary for extraction processes. Thus, we decided to build a recursive chunker for unrestricted German text within the framework of the {IMS} Corpus Workbench ({CWB}).},
	pages = {854--860},
	booktitle = {Proceedings of the 20th International Conference on Computational Linguistics (Coling)},
	author = {Kermes, Hannah},
	date = {2008},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, goal\_Enrichment},
}

@article{hoover_testing_2004,
	title = {Testing Burrows's Delta},
	volume = {19},
	url = {http://llc.oxfordjournals.org/content/19/4/453.abstract},
	doi = {10.1093/llc/19.4.453},
	abstract = {Delta, a simple measure of the difference between two texts, has been proposed by John F. Burrows as a tool in authorship attribution problems, particularly in large ‘open’ problems in which conventional methods of attribution are not able to limit the claimants effectively. This paper tests Delta's effectiveness and accuracy, and shows that it works nearly as well on prose as it does on poetry. It also shows that much larger numbers of frequent words are even more accurate than the 150 that Burrows tested. Automated methods that allow for tests on large numbers of differently selected words show that removing personal pronouns and words for which a single text supplies most of the occurrences greatly increases the accuracy of Delta tests. Further tests suggest that large changes in Delta and Delta z-scores from the likeliest to the second likeliest author typically characterize correct attributions, that differences in point of view among the texts are more significant than differences in nationality, and that combining several texts for each author in the primary set reduces the effect of intra-author variability. Although Delta occasionally produces errors in attribution with characteristics that would normally lead to a great deal of confidence, the results presented here confirm its usefulness in the preliminary stages of authorship attribution problems.},
	pages = {453 --475},
	number = {4},
	journaltitle = {Literary and Linguistic Computing},
	author = {Hoover, David L.},
	urldate = {2011-12-14},
	date = {2004-11},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Assessing, obj\_Research, t\_Stylometry},
}

@article{jannidis_tei_2009,
	title = {{TEI} in a crystal ball},
	volume = {24},
	url = {http://llc.oxfordjournals.org/content/24/3/253.abstract},
	doi = {10.1093/llc/fqp015},
	abstract = {Text Encoding Initiative ({TEI}) is an organization, a research community, and a markup language. Looking back into the history of these three {TEIs}, this article tries to describe what has been achieved and what its future challenges will be. The historical analysis is based on a closer look at the development of the {TEI}-L and topics covered by the Guidelines. A final section outlines possible roles of the {TEI} as an infrastructure for digital libraries and disciplinary virtual environments.},
	pages = {253 --265},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	author = {Jannidis, Fotis},
	urldate = {2011-04-26},
	date = {2009},
	langid = {english},
	keywords = {obj\_Code, obj\_Standards, obj\_Text, t\_Encoding},
}

@article{kirsch_technology_2014,
	title = {Technology Is Taking Over English Departments},
	issn = {0028-6583},
	url = {http://www.newrepublic.com/article/117428/limits-digital-humanities-adam-kirsch},
	journaltitle = {The New Republic},
	author = {Kirsch, Adam},
	urldate = {2014-05-02},
	date = {2014-05-02},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@book{kelly_teaching_nodate,
	title = {Teaching history in the digital age},
	isbn = {9780472118786},
	url = {http://www.press.umich.edu/3526836/teaching_history_in_the_digital_age},
	series = {Digital humanities},
	abstract = {A practical guide on how one professor employs the transformative changes of digital media in the research, writing, and teaching of history},
	author = {Kelly, T. Mills},
	langid = {english},
	keywords = {meta\_Teaching/Learning, obj\_DigitalHumanities},
}

@book{bartscherer_switching_2011,
	title = {Switching Codes: Thinking Through Digital Technology in the Humanities and the Arts},
	isbn = {0226038319},
	url = {http://press.uchicago.edu/ucp/books/book/chicago/S/bo6027946.html},
	shorttitle = {Switching Codes},
	pagetotal = {333},
	publisher = {Univ of Chicago Pr},
	author = {Bartscherer, Thomas and Coover, Roderick},
	date = {2011-05-30},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@report{romary_sustainable_2014,
	title = {Sustainable data for sustainable infrastructures},
	url = {http://hal.inria.fr/hal-00992220},
	abstract = {{DARIAH}, the Digital Research Infrastructure for the Arts and Humanities, is committed to advancing the digital revolution that has captured the arts and humanities. As more legacy primary and secondary sources become digital, more digital content is being produced and more digital tools are being deployed, we see a next generation of digitally aware scholars in the humanities emerge. {DARIAH} aims to connect these resources, tools and scholars, ensuring that the state-of-the-art in research is sustained and integrated across European countries. To do so, it is important to understand the actual role that proper data modelling and standards could play to make digital content sustainable. Even if it does not seem obvious at first sight that the arts and humanities would be fit for taking up the technological prerequisites of standardisation, we want to show in this paper that we can and should integrate standardisation issues at the core of our {DARIAH} infrastructural work. This analysis may lead us to a wider understanding of the role of scholars within a digital infrastructure and consequently on how {DARIAH} could better integrate a variety of research communities in the arts and humanities.},
	author = {Romary, Laurent},
	urldate = {2014-09-16},
	date = {2014},
	langid = {english},
	keywords = {act\_Collaborating, act\_Sharing, goal\_Collaboration, obj\_Data, obj\_Infrastructures},
}

@article{muralidharan_supporting_2013,
	title = {Supporting exploratory text analysis in literature study},
	volume = {28},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/28/2/283},
	doi = {10.1093/llc/fqs044},
	abstract = {We present {WordSeer}, an exploratory analysis environment for literary text. Literature study is a cycle of reading, interpretation, exploration, and understanding. While there is now abundant technological support for reading and interpreting literary text in new ways through text-processing algorithms, the other parts of the cycle—exploration and understanding—have been relatively neglected. We are motivated by the literature on sensemaking, an area of computer science devoted to supporting open-ended analysis on large collections of data. Our software system integrates tools for algorithmic processing of text with interaction techniques that support the interpretive, exploratory, and note-taking aspects of scholarship. At present, the system supports grammatical search and contextual similarity determination, visualization of patterns of word context, and examination and organization of the source material for comparison and hypothesis building. This article illustrates its capabilities by analyzing language-use differences between male and female characters in Shakespeare’s plays. We find that when love is a major plot point, the language Shakespeare uses to refer to women becomes more physical, and the language referring to men becomes more sentimental. Future work will incorporate additional sensemaking tools to aid comparison, exploration, grouping, and pattern recognition.},
	pages = {283--295},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Muralidharan, Aditi and Hearst, Marti A.},
	urldate = {2013-06-12},
	date = {2013-06-01},
	langid = {english},
	keywords = {goal\_Analysis},
}

@article{kestemont_stylometry_2012,
	title = {Stylometry for Medieval Authorship Studies: An Application to Rhyme Words},
	volume = {1},
	issn = {2162-9552},
	url = {http://muse.jhu.edu/content/crossref/journals/digital_philology/v001/1.1.kestemont.html},
	doi = {10.1353/dph.2012.0002},
	shorttitle = {Stylometry for Medieval Authorship Studies},
	abstract = {In the digital humanities much research has been done concerning stylometry, the computational study of style. Literary authorship at-tribution, especially, has been a central topic. After a brief introduction, I will discuss the enormous potential of this paradigm for medieval philology, a field that studies so many texts of unknown or disputed origin. At the same time, it will be stressed that stylometry’s application to medieval texts is currently not without problems: many attribution techniques are still controversial and do not account for the specific nature of medieval text production. Throughout this paper, I will tentatively apply two well-established attribution techniques (principal components analysis and Burrows’s Delta) to a number of case studies in Middle Dutch studies. These analyses shall be restricted to rhyme words, since these words are less likely to have been altered by scribes.},
	pages = {42--72},
	number = {1},
	journaltitle = {Digital Philology: A Journal of Medieval Cultures},
	author = {Kestemont, Mike},
	urldate = {2012-10-12},
	date = {2012},
	langid = {english},
	keywords = {t\_Stylometry},
}

@article{holmes_stylometry_2003,
	title = {Stylometry and the Civil War: The Case of the Pickett Letters},
	volume = {16},
	url = {http://www.cs.toronto.edu/~gh/Courses/2528/Readings/Binongo.pdf},
	pages = {18--25},
	number = {2},
	journaltitle = {Chance},
	author = {Holmes, David I.},
	date = {2003},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@article{forsyth_stylochronometry_1999,
	title = {Stylochronometry with substrings, or: a poet young and old},
	volume = {14},
	url = {http://llc.oxfordjournals.org/content/14/4/467.abstract},
	doi = {10.1093/llc/14.4.467},
	shorttitle = {Stylochronometry with substrings, or},
	abstract = {Assigning a date to a text is an important task in stylometry. Most previous research, however, have worked on intractable problems, where a true chronology will never be known with certainty, such as the works of Plato, Shakespeare or Marlowe. It is argued here that stylochronometric methods should be extensively tested on unproblematic texts before being used in disputed cases. As part of such testing, the present study applies a novel technique, Monte-Carlo Feature-Finding, to the verse of W. B. Yeats, where the dating is relatively well documented. Yeats insisted that his language changed as he grew older, and most readers would concur; yet scholars have not reached agreement on the nature of this linguistic change. A quasi-random search algorithm was used to find markers substrings in 142 poems of Yeats. To test their distinctiveness, four trials were performed: (1) assignment of ten poems absent from the training sample to their correct period; (2) detecting differences in two poems written by Yeats in his twenties and revised when he was fifty; (3) constructing a regression formula; and (4) classifying two prose extracts written forty-six years apart. Assigning short poems (median length 114 words) to their correct chronological period is a non-trivial task. Nevertheless, counting of distinctive substrings gave the right assignment in nine out of ten unseen cases. Moreover, these substring frequencies were sensitive enough to detect authorial revision in two early poems revised by Yeats many years after he originally wrote them, and robust enough to classify a pair of short prose extracts correctly; as well as accounting for 71\% of the variance when used in a regression to predict the year in which thirteen poems, absent from the training sample, were composed. These results suggest that short substrings found by a Monte-Carlo process warrant further investigation as stylistic indicators.},
	pages = {467 --478},
	number = {4},
	journaltitle = {Literary and Linguistic Computing},
	author = {Forsyth, Rs},
	urldate = {2011-12-14},
	date = {1999-12},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@article{stamou_stylochronometry:_2008,
	title = {Stylochronometry: Stylistic Development, Sequence of Composition, and Relative Dating},
	volume = {23},
	url = {http://llc.oxfordjournals.org/content/23/2/181.abstract},
	doi = {10.1093/llc/fqm029},
	shorttitle = {Stylochronometry},
	abstract = {This article examines representative successful and unsuccessful applications of stylochronometric approaches of the last sixty years in a thematic fashion, aiming to present in a concise manner, although not exhaustive, modern approaches. Differences concerned with adopted methodologies, stylistic markers, and text size render any comparisons among the studies difficult. Nevertheless, common problems may be traced, whereas groups of different stylistic marker types of potential use for applications concerned with stylistic change in time are identifiable.},
	pages = {181 --199},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Stamou, Constantina},
	urldate = {2011-12-14},
	date = {2008-06},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview, t\_Stylometry},
}

@incollection{wynne_stylistics:_2005,
	location = {Oxford},
	title = {Stylistics: Corpus Approaches},
	url = {http://www.pala.ac.uk/uploads/2/5/1/0/25105678/corpora_stylistics.pdf},
	publisher = {Oxford University Press},
	author = {Wynne, Martin},
	date = {2005},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_StylisticAnalysis, meta\_GiveOverview},
}

@inproceedings{stajner_stylistic_2013,
	location = {Pilsen, Czech Republic},
	title = {Stylistic Changes for Temporal Text Classification},
	url = {http://www.uni-koeln.de/~mzampier/papers/tsd2013.pdf},
	abstract = {This paper investigates stylistic changes in a set of Portuguese histori-
cal texts ranging from the 17
th
to the early 20
th
century and presents a supervised
method to classify them per century. Four stylistic features – average sentence
length ({ASL}), average word length ({AWL}), lexical density ({LD}), and lexical rich-
ness ({LR}) – were automatically extracted for each sub-corpus. The initial analysis
of diachronic changes in these four features revealed that the texts written in the
17
th
and 18
th
centuries have similar {AWL}, {LD} and {LR}, which differ significantly
from those in the texts written in the 19
th
and 20
th
centuries. This information was
later used in automatic classification of texts per century, leading to an F-Measure
of 0.92.},
	booktitle = {Proceedings of the 16th International Conference on Text Speech and Dialogue ({TSD}2013), Lecture Notes in Artificial Intelligence ({LNAI})},
	publisher = {Springer},
	author = {Stajner, Sanja and Zampieri, Marcos},
	date = {2013},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@incollection{craig_stylistic_2004,
	location = {Oxford},
	title = {Stylistic Analysis and Authorship Studies},
	url = {http://www.academia.edu/3114753/Stylistic_analysis_and_authorship_studies},
	pages = {273--288},
	booktitle = {A Companion to Digital Humanities},
	publisher = {Blackwell},
	author = {Craig, Hugh},
	editor = {Schreibman, Susan and Siemens, Ray and Unsworth, John},
	date = {2004},
	langid = {english},
	keywords = {*****, {AnalyzeStatistically}},
}

@article{craig_style_2009,
	title = {Style, statistics, and new models of authorship},
	volume = {15},
	url = {http://purl.oclc.org/emls/15-1/craistyl.htm},
	abstract = {In this essay I argue that humanities computing can change ideas about the role authors play within texts and can offer a path to a new conception of authorship. My focus is on computational stylistics in particular. The results of this practice tell us on a statistical basis that texts reflect the styles of their authors to a remarkable, perhaps unsuspected, extent. The results do not support the idea that authors are insignificant as sources of meaning, but neither do they license a return to an older idea of sovereign, hegemonic authors, as I hope to show. The findings of computational stylistics can serve to test theories about authorship and in turn to suggest modifications to those theories.},
	number = {1},
	journaltitle = {Early Modern Literary Studies},
	author = {Craig, Hugh},
	date = {2009},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@article{chow_studies_1975,
	title = {Studies of oxygen binding energy to hemoglobin molecule},
	volume = {66},
	issn = {0006-291X},
	url = {http://www.sciencedirect.com/science/article/pii/0006291X75905185},
	abstract = {The fractional oxygen saturation of hemoglobin and of its isolated α-chains have been determined by the application of the technique of Mossbauer effect while the oxygen equilibrium pressure was measured directly. By comparing the experimental data so obtained with theoretically derived oxygen saturation curves for hemoglobin and isolated α-chains the values of the oxygen pairing energy and oxygen binding energy to hemoglobin molecule have been found to be 0.043 ± 0.004 and 0.60 ± 0.06 {eV} respectively.

☆

    Partially supported by the Shuster Fellowship Fund and the Institutional Grants for Science ({NSF}).},
	pages = {1424--1431},
	number = {4},
	journaltitle = {Biochemical and biophysical research communications},
	shortjournal = {Biochem. Biophys. Res. Commun.},
	author = {Chow, Y W and Pietranico, R and Mukerji, A},
	date = {1975-10-27},
	langid = {english},
}

@book{hanlein_studies_1999,
	location = {Frankfurt am Main [u.a.]},
	title = {Studies in authorship recognition: a corpus based approach},
	volume = {352},
	isbn = {3-631-34420-1},
	url = {http://www.peterlang.com/index.cfm?event=cmp.ccc.seitenstruktur.detailseiten&seitentyp=produkt&pk=19102},
	series = {Europäische Hochschulschriften},
	abstract = {This study in authorship recognition takes as its starting point the intuitive capacity of readers to recognise authors under certain circumstances. Making use of corpus- and computerlinguistic methods and statistical procedures, the study shows that intuitive stylistic findings can in many cases be corroborated by empirical data. Ideally, the intuitive and the empirical approaches to style should be understood as complementing each other rather than as mutually exclusive concepts. The notion of style that emerges from this study is that of a number of basic decisions made by the author of a text, from which the remaining discriminatory linguistic characteristics of that text, choices, are derived.},
	pagetotal = {426},
	number = {14},
	publisher = {Lang},
	author = {Hänlein, Heike},
	date = {1999},
	langid = {english},
	note = {Augsburg, Univ., Diss., 1998},
	keywords = {{AnalyzeStatistically}, act\_StylisticAnalysis, t\_Stylometry},
}

@article{rommel_statistische_1998,
	title = {Statistische Literaturanalyse},
	pages = {502--503},
	journaltitle = {Metzler Lexikon Literatur- und Kulturtheorie},
	author = {Rommel, Thomas},
	date = {1998},
	langid = {german},
	note = {Rommel, Thomas: Statistische Literaturanalyse. In: Ansgar Nünning (Hg.): Metzler Lexikon Literatur- und Kulturtheorie. Stuttgart, Weimar 1998, S. 502-503.},
	keywords = {obj\_Literature, t\_Stylometry},
}
@book{lebart_statistique_1994,
	location = {Paris},
	title = {Statistique Textuelle},
	url = {http://lexicometrica.univ-paris3.fr/livre/st94/},
	abstract = {La statistique textuelle, en plein développement, est à la croisée de plusieurs disciplines : la statistique classique, l'analyse du discours, l'informatique, le traitement des enquêtes.

En effet, chercheurs et praticiens ont aujourd'hui à faire face à un double développement, d'une part celui des textes provenant des enquêtes, des entretiens, des archives, des bases documentaires, d'autre part, celui des outils informatiques de saisie et de gestion des textes.

La statistique textuelle se veut précisément un outil destiné à parfaire l'analyse, la description, la comparaison, en un mot, le traitement des textes.

Ce livre, illustré d'exemples nombreux, présente les concepts de base et les fondements des méthodes de la statistique textuelle. Il combine une approche pédagogique des outils et un exposé sur l'état de l'art de cette discipline.},
	publisher = {Dunod},
	author = {Lebart, Ludovic and Salem, André},
	date = {1994},
	langid = {french},
}

@book{gries_statistics_2009,
	location = {Berlin},
	title = {Statistics for linguistics with R : a practical introduction / by Stefan Th. Gries},
	isbn = {978-3-11-020564-0},
	url = {http://www.degruyter.com/view/product/203826},
	shorttitle = {Statistics for linguistics with R},
	abstract = {his book is the revised and extended second edition of Statistics for Linguistics with R. The volume is an introduction to statistics for linguists using the open source software R. It is aimed at students and instructors/professors with little or no statistical background and is written in a non-technical and reader-friendly/accessible style.

It first introduces in detail the overall logic underlying quantitative studies: exploration, hypothesis formulation and operationalization, and the notion and meaning of significance tests. It then introduces some basics of the software R relevant to statistical data analysis. A chapter on descriptive statistics explains how summary statistics for frequencies, averages, and correlations are generated with R and how they are graphically represented best. A chapter on analytical statistics explains how statistical tests are performed in R on the basis of many different linguistic case studies: For nearly every single example, it is explained what the structure of the test looks like, how hypotheses are formulated, explored, and tested for statistical significance, how the results are graphically represented, and how one would summarize them in a paper/article. A chapter on selected multifactorial methods introduces how more complex research designs can be studied: methods for the study of multifactorial frequency data, correlations, tests for means, and binary response data are discussed and exemplified step-by-step. Also, the exploratory approach of hierarchical cluster analysis is illustrated in detail.

The book comes with many exercises, boxes with short think breaks and warnings, recommendations for further study, and answer keys as well as a statistics for linguists newsgroup on the companion website.

Just like the first edition, it is aimed at students, faculty, and researchers with little or no statistical background in statistics or the open source programming language R. It avoids mathematical jargon and discusses the logic and structure of quantitative studies and introduces descriptive statistics as well as a range of monofactorial statistical tests for frequencies, distributions, means, dispersions, and correlations. The comprehensive revision includes new small sections on programming topics that facilitate statistical analysis, the addition of a variety of statistical functions readers can apply to their own data, a revision of overview sections on statistical tests and regression modeling, a complete rewrite of the chapter on multifactorial approaches, which now contains sections on linear regression, binary and ordinal logistic regression, multinomial and Poisson regression, and repeated-measures {ANOVA}, and a new visual tool to identify the right statistical test for a given problem and data set. The amount of code available from the companion website has doubled in size, providing much supplementary material on statistical tests and advanced plotting.},
	publisher = {Mouton de Gruyter},
	author = {Gries, Stefan Thomas},
	date = {2009},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Language},
}

@article{hoover_statistical_2001,
	title = {Statistical Stylistic and Authorship Attribution: an Empirical Investigation},
	volume = {16},
	url = {http://llc.oxfordjournals.org/content/16/4/421.full.pdf},
	abstract = {This paper investigates the effectiveness and accuracy of multivariate analysis, specifically cluster analysis, of the frequencies of very frequent words in distinguishing texts by different authors and grouping texts by a single author. An examination of groups of texts by known authors shows that cluster analyses typically achieve an accuracy rate of less than 90 per cent for contemporary novels, modern British and American novels, and contemporary literary critical texts, both on relatively large groups of texts and on smaller subsets of those texts. Although limiting the analysis to third-person narration, and disambiguating homographic function words improves the results, inaccuracies remain. Furthermore, small groups of problematic texts extracted from the larger groups in simulated authorship studies also fail to cluster correctly. These failures suggest general rather than local problems with the technique, and cast doubt on the effectiveness of cluster analysis for authorship attribution and stylistic study.},
	pages = {421--444},
	journaltitle = {Literary and Linguistic Computing},
	author = {Hoover, David L.},
	date = {2001},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@article{potter_statistical_1991,
	title = {Statistical Analysis of Literature. A Retrospective on Computers and the Humanities, 1966-1990},
	url = {http://link.springer.com/article/10.1007%2FBF00141190},
	abstract = {This retrospective on statistical analysis of literature in the first twenty-four years of Computers and the Humanities divides the essays under review into four groups: the philosophical, the statistical analyses of language, the statistical analyses of literary texts, and the statistical analyses of themes. It begins with the question: must valid statistical analysis of any literary text be based on a complete linguistic description of the language of the text? It summarizes and evaluates over forty essays, giving details on works discussed, sample sizes used, statistical methods applied, and quotations from the researchers. The essay ends with a polemical summary of what has been done and what the future holds. It emphasizes the importance of extended pre-computational stages of learning about language and discourse analysis; reading previous research, building on and challenging theory; and the use of carefully crafted, small databases to test specific questions.},
	pages = {401--429},
	number = {25},
	journaltitle = {Computers and the Humanities},
	author = {Potter, Rosanne G.},
	date = {1991},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_StylisticAnalysis, meta\_GiveOverview, obj\_Literature, t\_Stylometry},
}

@book{drucker_speclab_2009,
	location = {Chicago},
	title = {{SpecLab} digital aesthetics and projects in speculative computing},
	isbn = {9780226165097  0226165094  9780226165073  0226165078  9780226165080  0226165086},
	url = {http://public.eblib.com/EBLPublic/PublicView.do?ptiID=448540},
	abstract = {Nearly a decade ago, Johanna Drucker cofounded the University of Virginia's {SpecLab}, a digital humanities laboratory dedicated to risky projects with serious aims. In {SpecLab} she explores the implications of these radical efforts to use critical practices and aesthetic principles against the authority of technology based on analytic models of knowledge.},
	publisher = {University of Chicago Press},
	author = {Drucker, Johanna},
	urldate = {2013-03-22},
	date = {2009},
	langid = {english},
	keywords = {meta\_Assessing, meta\_GiveOverview, obj\_Project},
}

@article{ramsay_special_2003,
	title = {Special Section: Reconceiving Text Analysis},
	volume = {18},
	url = {http://llc.oxfordjournals.org/content/18/2/167.abstract},
	doi = {10.1093/llc/18.2.167},
	shorttitle = {Special Section},
	abstract = {The inability of computing humanists to break into the mainstream of literary critical scholarship may be attributed to the prevalence of scientific methodologies and metaphors in humanities computing research—methodologies and metaphors that are wholly foreign not only the language of literary criticism, but to its entire purpose. Breaking out of this unfortunate misalignment entails reaching for more appropriate paradigms. The ‘algorithmic criticism’ here proposed rejects the empiricist vision of the computer as a means by which critical interpretations may be verified, and instead seeks to locate computational processes within the rich tradition of interpretive endeavours (usually aligned more with art than criticism), which seek not to constrain meaning, but to guarantee its multiplicity. Computational processes, which are perhaps more conformable to this latter purpose, may be usefully viewed as ways of providing the necessary conditions for interpretive insight. Algorithmic criticism seeks, therefore, in the narrowing forces of constraint embodied and instantiated in the strictures of algorithmic processing, an analogue to the liberating potentialities of art and the ludic values of humanistic inquiry. It proposes that we reconceive computer‐assisted text analysis as an activity best employed not in the service of a heightened critical objectivity, but as one that embraces the possibilities of that deepened subjectivity upon which critical insight depends.},
	pages = {167 --174},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Ramsay, Stephen},
	urldate = {2012-02-08},
	date = {2003-06},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, meta\_Theorizing},
}

@article{olsen_something_2011,
	title = {Something Borrowed: Sequence Alignment and the Identification of Similar Passages in Large Text Collections},
	volume = {2},
	issn = {1918-3666},
	url = {http://www.digitalstudies.org/ojs/index.php/digital_studies/article/view/190},
	shorttitle = {Something Borrowed},
	abstract = {The following article describes a simple technique to identify lexically-similar passages in large collections of text using sequence alignment algorithms. Primarily used in the field of bioinformatics to identify similar segments of {DNA} in genome research, sequence alignment has also been employed in many other domains, from plagiarism detection to image processing. While we have applied this approach to a wide variety of diverse text collections, we will focus our discussion here on the identification of similar passages in the famous 18th-century Encyclopédie of Denis Diderot and Jean d'Alembert. Reference works, such as encyclopedias and dictionaries, are generally expected to "reuse" or "borrow" passages from many sources and Diderot and d'Alembert's Encyclopédie was no exception. Drawn from an immense variety of source material, both French and non-French, many, if not most, of the borrowings that occur in the Encyclopédie are not sufficiently identified (according to our standards of modern citation), or are only partially acknowledged in passing. The systematic identification of recycled passages can thus offer us a clear indication of the sources the philosophes were exploiting as well as the extent to which the intertextual relations that accompanied its composition and subsequent reception can be explored. In the end, we hope this approach to "Encyclopedic intertextuality" using sequence alignment can broaden the discussion concerning the relationship of Enlightenment thought to previous intellectual traditions as well as its reuse in the centuries that followed},
	number = {1},
	journaltitle = {Digital Studies / Le champ numérique},
	shortjournal = {{DS}/{CN}},
	author = {Olsen, Mark and Horton, Russell and Roe, Glenn},
	date = {2011-05-17},
	langid = {english},
	keywords = {act\_RelationalAnalysis},
}

@book{manovich_software_2013,
	location = {New York},
	title = {Software takes command},
	isbn = {1623567459  9781623567453  162356817X  9781623568177},
	url = {http://manovich.net/index.php/projects/software-takes-command},
	abstract = {Software has replaced a diverse array of physical, mechanical, and electronic technologies used before 21st century to create, store, distribute and interact with cultural artifacts. It has become our interface to the world, to others, to our memory and our imagination - a universal language through which the world speaks, and a universal engine on which the world runs. What electricity and combustion engine were to the early 20th century, software is to the early 21st century. Offering the the first theoretical and historical account of software for media authoring and its effects on the practice and the very concept of 'media,' the author of The Language of New Media (2001) develops his own theory for this rapidly-growing, always-changing field. What was the thinking and motivations of people who in the 1960 and 1970s created concepts and practical techniques that underlie contemporary media software such as Photoshop, Illustrator, Maya, Final Cut and After Effects? How do their interfaces and tools shape the visual aesthetics of contemporary media and design? What happens to the idea of a 'medium' after previously media-specific tools have been simulated and extended in software? Is it still meaningful to talk about different mediums at all? Lev Manovich answers these questions and supports his theoretical arguments by detailed analysis of key media applications such as Photoshop and After Effects, popular web services such as Google Earth, and the projects in motion graphics, interactive environments, graphic design and architecture. Software Takes Command is a must for all practicing designers and media artists and scholars concerned with contemporary media.},
	publisher = {Bloomsbury},
	author = {Manovich, Lev},
	date = {2013},
	langid = {english},
}

@article{bingenheimer_social_2011,
	title = {Social network visualization from {TEI} data},
	volume = {26},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/26/3/271},
	doi = {10.1093/llc/fqr020},
	abstract = {The focus of this article is a system for visualizing social network data derived from a {TEI}-encoded corpus of texts. It describes the collection of biographies of historical Chinese Buddhist monks, which constitutes this corpus and the {TEI} markup, in particular the innovative concept of a ‘nexus-point’ that was originally applied to them with the goal of producing {GIS}-like visualizations [see Bingenheimer, M., Hung, J.-J., and Wiles, S. (2009). Markup meets {GIS} - Visualizing the ‘Biographies of Eminent Buddhist Monks’. In Banissi, E. et al. (eds), Proceedings of Information Visualization {IV} 2009. {IEEE} Computer Society: 550–4.]. Over the course of this work, it became clear that a data set of nexus-points could be derived from this markup which would support a representation of the social network which can be inferred from the corpus. The nature of this social network is explored and some interesting preliminary applications are suggested. The software architecture which supports the visualization, based on the Prefuse toolkit, is introduced. Finally, the scope for the future development of the corpus and the system are discussed, and some avenues for potentially fruitful analysis are suggested. Throughout the article, it is argued that the methods and techniques employed here are applicable well beyond the present context. In describing this project of social network visualization, it is demonstrated that a well-marked-up {TEI} corpus can, with very little additional technical overhead and using the same markup, serve as the basis for multiple representations of the same data.},
	pages = {271--278},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Bingenheimer, Marcus and Hung, Jen-Jou and Wiles, Simon},
	urldate = {2012-07-08},
	date = {2011-09-01},
	langid = {english},
	keywords = {act\_RelationalAnalysis},
}

@incollection{ajouri_social_2013,
	location = {Münster},
	title = {Social Network Analysis ({SNA}) als Methode einer textempirischen Literaturwissenschaft},
	isbn = {9783897854581 3897854589},
	url = {http://www.academia.edu/4973000/Social_Network_Analysis_SNA_als_Methode_einer_textempirischen_Literaturwissenschaft._In_Philip_Ajouri_Katja_Mellmann_u._Christoph_Rauen_Hg._Empirie_in_der_Literaturwissenschaft_M%C3%BCnster_2013_S._201-247},
	pages = {201--247},
	booktitle = {Empirie in der Literaturwissenschaft},
	publisher = {Mentis},
	author = {Trilcke, Peer},
	editor = {Ajouri, Philip and Mellmann, Katja and Rauen, Christoph},
	date = {2013},
	langid = {german},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, act\_RelationalAnalysis, goal\_Analysis, t\_NetworkAnalysis},
}

@book{wasserman_social_1994,
	title = {Social Network Analysis: Methods and Applications},
	isbn = {9780521387071},
	url = {http://www.cambridge.org/us/academic/subjects/sociology/sociology-general-interest/social-network-analysis-methods-and-applications},
	shorttitle = {Social Network Analysis},
	abstract = {Social network analysis is used widely in the social and behavioral sciences, as well as in economics, marketing, and industrial engineering. The social network perspective focuses on relationships among social entities and is an important addition to standard social and behavioral research, which is primarily concerned with attributes of the social units. Social Network Analysis: Methods and Applications reviews and discusses methods for the analysis of social networks with a focus on applications of these methods to many substantive examples. It is a reference book that can be used by those who want a comprehensive review of network methods, or by researchers who have gathered network data and want to find the most appropriate method by which to analyze it. It is also intended for use as a textbook as it is the first book to provide comprehensive coverage of the methodology and applications of the field.},
	pagetotal = {852},
	publisher = {Cambridge University Press},
	author = {Wasserman, Stanley},
	date = {1994-11-25},
	langid = {english},
	keywords = {*****, act\_RelationalAnalysis, goal\_Analysis, t\_NetworkAnalysis},
}

@book{scott_social_2012,
	edition = {3},
	title = {Social Network Analysis},
	isbn = {1446209040},
	url = {http://www.amazon.de/Social-Network-Analysis-A-Handbook/dp/0761963391},
	abstract = {The revised and updated edition of this bestselling text provides an accessible introduction to the theory and practice of network analysis in the social sciences. It gives a clear and authoritative guide to the general framework of network analysis, explaining the basic concepts, technical measures and reviewing the available computer programs. The book outlines both the theoretical basis of network analysis and the key techniques for using it as a research tool. Building upon definitions of points, lines and paths, John Scott demonstrates their use in clarifying such measures as density, fragmentation and centralization. He identifies the various cliques, components and circles into which networks are formed, and outlines an approach to the study of socially structured positions. He also discusses the use of multidimensional methods for investigating social networks. Social Network Analysis is an invaluable resource for researchers across the social sciences and for students of social theory and research methods.},
	pagetotal = {201},
	publisher = {Sage Publications Ltd},
	author = {Scott, John P.},
	date = {2012-11-19},
	langid = {english},
}

@thesis{riepl_sind_1993,
	location = {St. Ottilien},
	title = {Sind David und Saul berechenbar?: von der sprachlichen Analyse zur literarischen Struktur von 1 Sam 21 und 22},
	url = {http://ggcki.mobi/1fum9z_sind-david-und-saul-berechenbar-von-der-sprachlichen-analyse-zur-literar.pdf},
	abstract = {Thesis ({PhD} Level)},
	pagetotal = {393},
	institution = {{EOS}-Verl.},
	type = {phdthesis},
	author = {Riepl, Christian},
	date = {1993},
	langid = {german},
	keywords = {{AnalyzeStatistically}, obj\_Literature},
}

@article{olsen_signs_1993,
	title = {Signs, Symbols and Discourses: A New Direction for Computer-Aided Literature Studies},
	url = {http://link.springer.com/article/10.1007/BF01829380?no-access=true},
	abstract = {Computer-aided literature studies have failed to have a significant impact on the field as a whole. This failure is traced to a concentration on how a text achieves its literary effect by the examination of subtle semantic or grammatical structures in single texts or the works of individual authors. Computer systems have proven to be very poorly suited to such refined analysis of complex language. Adopting such traditional objects of study has tended to discourage researchers from using the tool to ask questions to which it is better adapted, the examination of large amounts of simple linguistic features. Theoreticians such as Barthes, Foucault and Halliday show the importance of determining the linguistic and semantic characteristics of the language used by the author and her/his audience. Current technology, and databases like the {TLG} or {ARTFL}, facilitate such wide-spectrum analyses. Computer-aided methods are thus capable of opening up new areas of study, which can potentially transform the way in which literature is studied.},
	pages = {1--11},
	number = {27},
	journaltitle = {Computers and the Humanities},
	author = {Olsen, M.},
	date = {1993},
	langid = {english},
	keywords = {meta\_Assessing, meta\_GiveOverview, obj\_DigitalHumanities, t\_Stylometry},
}

@incollection{luyckx_shallow_2005,
	title = {Shallow Text Analysis and Machine Learning for Authorship Attribution},
	url = {http://www.clips.ua.ac.be/~kim/Papers/LD05.pdf},
	abstract = {Current advances in shallow parsing and machine learning allow us to use
results from these
fields in a methodology for Authorship Attribution. We report on experimen
ts with a corpus
that consists of newspaper articles about national current affairs by
different journalists from
the Belgian newspaper
De Standaard
. Because the documents are in a similar genre, register,
and range of topics, token-based (e.g., sentence length) and lexical
features (e.g., vocabulary
richness) can be kept roughly constant over the different authors.
This allows us to focus on
the use of syntax-based features as possible predictors for an author
’s style, as well as on those
token-based features that are predictive to author style more than to topic
or register. These
style characteristics are not under the author’s conscious control and
therefore good clues for
Authorship Attribution. Machine Learning methods ({TiMBL} and the {WEKA} sof
tware package)
are used to select informative combinations of syntactic, token-based a
nd lexical features and to
predict authorship of unseen documents. The combination of these fea
tures can be considered
an implicit profile that characterizes the style of an author.},
	pages = {149--160},
	author = {Luyckx, Kim and Daelemans, Walter},
	date = {2005},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Theorizing, t\_MachineLearning, t\_Stylometry},
}

@book{ilsemann_shakespeare_1998,
	location = {Frankfurt am Main ; New York},
	title = {Shakespeare disassembled: eine quantitative Analyse der Dramen Shakespeares},
	isbn = {3631327560},
	url = {http://www.amazon.de/Shakespeare-Disassembled-Quantitative-Shakespeares-Literarische/dp/3631327560},
	series = {Literarische Studien},
	shorttitle = {Shakespeare disassembled},
	pagetotal = {290},
	number = {Bd. 5},
	publisher = {P. Lang},
	author = {Ilsemann, Hartmut},
	date = {1998},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Text},
}

@collection{craig_shakespeare_2009,
	edition = {1},
	title = {Shakespeare, Computers, and the Mystery of Authorship},
	isbn = {0521516234},
	url = {http://ebooks.cambridge.org/ebook.jsf?bid=CBO9780511605437},
	abstract = {n this book Craig, Kinney and their collaborators confront the main unsolved mysteries in Shakespeare's canon through computer analysis of Shakespeare's and other writers' styles. In some cases their analysis confirms the current scholarly consensus, bringing long-standing questions to something like a final resolution. In other areas the book provides more surprising conclusions: that Shakespeare wrote the 1602 additions to The Spanish Tragedy, for example, and that Marlowe along with Shakespeare was a collaborator on Henry {VI}, Parts 1 and 2. The methods used are more wholeheartedly statistical, and computationally more intensive, than any that have yet been applied to Shakespeare studies. The book also reveals how word patterns help create a characteristic personal style. In tackling traditional problems with the aid of the processing power of the computer, harnessed through computer science, and drawing upon large amounts of data, the book is an exemplar of the new domain of digital humanities},
	pagetotal = {254},
	publisher = {Cambridge University Press},
	editor = {Craig, Hugh and Kinney, Arthur F.},
	date = {2009-08-27},
	langid = {english},
	keywords = {obj\_Literature, t\_Stylometry},
}

@article{rosso_shakespeare_2009,
	title = {Shakespeare and other English Renaissance authors as characterized by Information Theory complexity quantifiers},
	volume = {388},
	issn = {0378-4371},
	url = {http://www.sciencedirect.com/science/article/pii/S0378437108009461},
	doi = {10.1016/j.physa.2008.11.018},
	abstract = {We introduce novel Information Theory quantifiers in a computational linguistic study that involves a large corpus of English Renaissance literature. The 185 texts studied (136 plays and 49 poems in total), with first editions that range from 1580 to 1640, form a representative set of its period. Our data set includes 30 texts unquestionably attributed to Shakespeare; in addition we also included A Lover’s Complaint, a poem which generally appears in Shakespeare collected editions but whose authorship is currently in dispute. Our statistical complexity quantifiers combine the power of Jensen–Shannon’s divergence with the entropy variations as computed from a probability distribution function of the observed word use frequencies. Our results show, among other things, that for a given entropy poems display higher complexity than plays, that Shakespeare’s work falls into two distinct clusters in entropy, and that his work is remarkable for its homogeneity and for its closeness to overall means.},
	pages = {916--926},
	number = {6},
	journaltitle = {Physica A: Statistical Mechanics and its Applications},
	shortjournal = {Physica A: Statistical Mechanics and its Applications},
	author = {Rosso, Osvaldo A. and Craig, Hugh and Moscato, Pablo},
	urldate = {2013-10-01},
	date = {2009-03-15},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@online{lexalytics_sentiment_nodate,
	title = {Sentiment Extraction: Measuring the Emotional Tone of Text},
	url = {http://lexalytics.lexalytics1.netdna-cdn.com/system/files_force/whitepaper_sentiment_0612_0.pdf?download=1},
	author = {{Lexalytics}},
	langid = {english},
	keywords = {X-{CHECK}},
}

@inbook{chen_sentiment_2012,
	location = {New York, {NY}},
	title = {Sentiment Analysis},
	volume = {30},
	isbn = {978-1-4614-1556-5, 978-1-4614-1557-2},
	url = {http://www.springerlink.com/index/10.1007/978-1-4614-1557-2_10},
	abstract = {The Internet is frequently used as a medium for exchange of information and opinions, as well as propaganda dissemination. In this study, the use of sentiment analysis methodologies is proposed for classification of Web forum opinions in multiple languages. The utility of stylistic and syntactic features is evaluated for sentiment classification of English and Arabic content. Specific feature extraction components are integrated to account for the linguistic characteristics of Arabic. The entropy weighted genetic algorithm ({EWGA}) is also developed, which is a hybridized genetic algorithm that incorporates the information gain heuristic for feature selection. {EWGA} is designed to improve performance and get a better assessment of the key features. The proposed features and techniques are evaluated on {US} and Middle Eastern Web forum postings. The experimental results using {EWGA} with {SVM} indicate high performance levels, with accuracy over 95\% on the benchmark dataset and over 93\% for both the {US} and Middle Eastern forums. Stylistic features significantly enhanced performance across all test beds while {EWGA} also outperformed other feature selection methods, indicating the utility of these features and techniques for document-level classification of sentiments.},
	pages = {171--201},
	booktitle = {Dark Web},
	publisher = {Springer New York},
	author = {Chen, Hsinchun},
	bookauthor = {Chen, Hsinchun},
	urldate = {2012-07-30},
	date = {2012},
	langid = {english},
	keywords = {t\_SentimentAnalysis},
}

@book{bertin_semiology_2010,
	location = {Redlands, Calif},
	edition = {1st ed},
	title = {Semiology of graphics: diagrams, networks, maps},
	isbn = {9781589482616},
	url = {http://esripress.esri.com/display/index.cfm?fuseaction=display&websiteID=190},
	shorttitle = {Semiology of graphics},
	abstract = {Originally published in French in 1967, "Semiology of Graphics" holds a significant place in the theory of information design. Founded on Jacques Bertin's practical experience as a cartographer, Part One of this work is an unprecedented attempt to synthesize principles of graphic communication with the logic of standard rules applied to writing and topography. Part Two brings Bertin's theory to life, presenting a close study of graphic techniques including shape, orientation, color, texture, volume, and size in an array of more than 1,000 maps and diagrams.},
	pagetotal = {438},
	publisher = {{ESRI} Press : Distributed by Ingram Publisher Services},
	author = {Bertin, Jacques},
	editora = {Berg, William J.},
	editoratype = {collaborator},
	date = {2010},
	langid = {english},
}

@book{zipf_selected_1932,
	location = {Cambridge  Mass.},
	title = {Selected studies of the principle of relative frequency in language},
	url = {http://de.scribd.com/doc/153850777/Zipf-1932-Selected-Studies-of-the-Principle-of-Relative-Frequency-in-Language},
	publisher = {Harvard University Press},
	author = {Zipf, George},
	date = {1932},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Theorizing, obj\_Language},
}

@article{purdy_valuing_2010,
	title = {Valuing Digital Scholarship: Exploring the Changing Realities of Intellectual Work},
	volume = {2010},
	issn = {0740-6959},
	url = {http://www.mlajournals.org/doi/abs/10.1632/prof.2010.2010.1.177},
	doi = {10.1632/prof.2010.2010.1.177},
	shorttitle = {Valuing Digital Scholarship},
	pages = {177--195},
	journaltitle = {Profession},
	author = {Purdy, James P. and Walker, Joyce R.},
	urldate = {2011-11-08},
	date = {2010-11},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Research},
}

@inproceedings{ahlberg_visual_1994,
	location = {New York},
	title = {Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays},
	url = {http://dl.acm.org/citation.cfm?id=191775},
	abstract = {This paper offers new principles for visual information
seeking ({VIS}). A key concept is to support browsing, which
is distinguished from familiar query composition and
information retrieval because of its emphasis on rapid
filtering to reduce result sets, progressive refinement of
search parameters, continuous reformulation of goals, and
visual scanning to identify results. {VIS} principles
developed include: dynamic query filters (query parameters
are rapidly adjusted with sliders, buttons, maps, etc.),
starfield displays (two-dimensional scatterplots to structure
tesult sets and zooming to reduce clutter), and tight coupling
(interrelating query components to preserve display
invariants and support progressive refinement combined
with an emphasis on using search output to foster search
input). A {FilmFinder} prototype using a movie database
demonstrates these principles in a {VIS} environment.},
	pages = {313--317},
	booktitle = {Proceeding {CHI} '94 Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Ahlberg, Christopher and Shneiderman, Ben},
	date = {1994},
	langid = {english},
}

@book{fry_visualizing_2008,
	location = {Sebastopol, {CA}},
	title = {Visualizing Data: Exploring and Explaining Data with the Processing Environment},
	isbn = {978-0-596-51455-6},
	url = {http://books.google.de/books?hl=de&lr=&id=6jsVAiULQBgC&oi=fnd&pg=PR7&dq=Visualizing+data+fry&ots=2yn__7jiTQ&sig=RtGaSWoU-LZvvP3Rp8WBsGPXD8s#v=onepage&q=Visualizing%20data%20fry&f=false},
	abstract = {How you can take advantage of data that you might otherwise never use? With the help of a downloadable programming environment, this book helps you represent data accurately on the Web and elsewhere, complete with user interaction, animation, and more. You'll learn basic visualization principles, how to choose the right kind of display for your purposes, and how to provide interactive features to design entire interfaces around large, complex data sets.},
	pagetotal = {382},
	publisher = {O'Reilly Media, Inc},
	author = {Fry, Ben},
	date = {2008},
	langid = {english},
}
@article{gradmann_vom_2004,
	title = {Vom Verfertigen der Gedanken im digitalen Diskurs: Versuch einer wechselseitigen Bestimmung hermeneutisch und empirizistischer Positionen},
	volume = {29},
	issn = {0172-6404},
	url = {http://www.ssoar.info/ssoar/handle/document/3086},
	abstract = {"Über das elektronische Publizieren und die Frage darüber, was «open access» eigentlich bedeutet, herrscht in den unterschiedlichen Wissenschaftskulturen ein je eigenes Verständnis, was sich in einer je spezifischen Umsetzung niederschlägt. Grundlegende Differenzen in den hermeneutischen und empiristischen Wissenschaftskulturen bezüglich der zugrunde liegenden Informationsmodelle erzeugen ebenso grundlegend verschiedene Bedeutungsfelder von den Schlüsselbegriffen wie 'open' und 'access'. Der Beitrag sucht die Perspektiven für die Geistes-, Sozial- und Kulturwissenschaften und insbesondere deren Innovationspotenzial in diesem Bereich zu bestimmen." (Autorenreferat)

"Electronic publishing and open access to the resulting publications have a very specific status and equally specific consequences in the different scientific cultures. Fundamental differences with respect to the respective underlying information models in the hermeneutic and the empiricistic scientific cultures engender significantly different semantics of key words such as open and 'access'. The contribution aims at defining a perspective for the humanities in this context and is concerned with their innovation potential."},
	pages = {56--63},
	number = {1},
	journaltitle = {Historical Social Research},
	author = {Gradmann, Stefan},
	date = {2004},
	langid = {german},
	keywords = {meta\_Advocating, obj\_DigitalHumanities},
}

@article{aschenbrenner_von_2007,
	title = {Von e-Science zu e-Humanities - Digital vernetzte Wissenschaft als neuer Arbeits- und Kreativbereich für Kunst und Kultur},
	volume = {31},
	url = {https://www.b2i.de/fileadmin/dokumente/BFP_Bestand_2007/Jg_31-Nr_1/Jg_31-Nr_1_Aufsaetze/Jg_31-2007-Nr_1-S_11-21.pdf},
	abstract = {e-Science beschreibt eine neue Form wissenschaftlichen Arbeitens in globaler Vernetzung von Computerressour-
cen, Wissen, Werkzeugen und Menschen. Die ursprünglich aus den Naturwissenschaften stammenden Konzepte
und Technologien sind auch auf andere Disziplinen übertragbar. Insbesondere die Geisteswissenschaften und die
Kunst können davon profitieren und können umgekehrt auch zu den Entwicklungen beitragen. e-Science steckt ge-
rade erst in den Anfängen; Technologien, Strukturen und wissenschaftliche Arbeitskultur entwickeln sich wechselsei-
tig und in großen Schritten weiter.
Dieser Artikel beschreibt einige Anwendungsmöglichkeiten von e-Science anhand aktueller Initiativen aus den Geistes-
wissenschaften und der Kunst. Der Schwerpunkt „Textwissenschaften“ zeigt anhand des durch das {BMBF} geförderten
Projektes {TextGrid}, wie die Arbeiten und das Wissen diverser Anwendungsgruppen in einer grid-fähigen Arbeitsum-
gebung vernetzt werden können},
	pages = {11--21},
	journaltitle = {{BibliothekBibliothek}. Forschung und Praxis},
	author = {Aschenbrenner, Andreas and Blanke, Tobias and Dunn, Stuart and Kerzel, Martina and Rapp, Andrea and Zielinski, Andrea},
	date = {2007},
	langid = {german},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, obj\_Infrastructures},
}

@article{jannidis_was_1999,
	title = {Was ist Computerphilologie?},
	url = {http://computerphilologie.digital-humanities.de/jahrbuch/jb1/jannidis-1.html},
	abstract = {Im Zuge seiner weltweiten Verbreitung konnte sich der {PC} gegen anfängliche Bedenken und Widerstände auch in der Literaturwissenschaft als Werkzeug der täglichen Arbeit etablieren. Anfangs waren es vor allem die Vorteile der Textverarbeitung und deren Entlastung vom mechanischen Aspekt des Schreibens und Wiederschreibens, die den Rechnern den Weg auf die Schreibtische ebneten. Ist aber die Maschine einmal vorhanden, man sich mit geringem Aufwand Zugang zum Internet verschaffen. E-Mail und das World Wide Web eröffnen einfachere Kommunikationswege, dazu kommen die Vorteile des Intranets, also eines universitätseigenen Netzes mit Zugriff auf elektronische Bibliographien und die Bibliothekskataloge einschließlich der Bestellmöglichkeiten vor Ort. Nicht wenige Literaturwissenschaftler haben sich inzwischen auch mit den neueren elektronischen Texten angefreundet, deren einfachen Benutzeroberflächen althergebrachte philologische Tätigkeiten sehr beschleunigen, zum Beispiel die Klärung von Wortbedeutungen mittels der Suche nach Parallelstellen beim selben Autor oder in derselben Epoche.},
	pages = {39--60},
	number = {1},
	journaltitle = {Jahrbuch für Computerphilologie},
	author = {Jannidis, Fotis},
	date = {1999},
	langid = {german},
	keywords = {*****, goal\_Analysis, goal\_Enrichment, meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{hardmeier_was_2000,
	title = {Was ist Computerphilologie? Theorie, Methoden, Anwendungsfelder - eine Skizze},
	pages = {9--31},
	journaltitle = {Hardmeier 2000},
	author = {Hardmeier, Christof},
	date = {2000},
	langid = {german},
	keywords = {goal\_Analysis, goal\_Enrichment, meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{degkwitz_weg_2014,
	title = {Weg mit den Wissenskonserven},
	issn = {1865-2263},
	url = {http://www.tagesspiegel.de/wissen/wie-open-access-die-forschung-veraendert-weg-mit-den-wissenskonserven/10058452.html},
	abstract = {Aufsätze und Monografien als Auslaufmodelle: Digitales Publizieren könnte die Arbeit von Forschern grundlegend verändern. Noch verhindern das die großen Verlage. Sie fürchten um ihre Monopole.},
	journaltitle = {Der Tagesspiegel Online},
	author = {Degkwitz, Andreas},
	urldate = {2014-06-20},
	date = {2014-06-18},
	langid = {german},
	keywords = {act\_Publishing, goal\_Dissemination, obj\_ResearchResults, t\_OpenAccess},
}

@article{seefeldt_what_2009,
	title = {What is Digital History? A Look at Some Exemplar Projects},
	url = {http://digitalcommons.unl.edu/historyfacpub/98},
	abstract = {Both of us came into the history profession in the early 1990s and went through graduate school just before the remarkable emergence of the World Wide Web. Of course, we can see now that a communication revolution was taking place during those years and that it was changing the way we do historical scholarship and teaching. After the development of browsers like Mosaic Netscape and Netscape Navigator in 1994, the web grew at an astonishing rate into a global information network. Even at the early stages of the web’s growth, history was all over the web. Amazingly, people rushed to put their own histories on the web and to create sites dedicated to their favorite subjects. Big organizations, such as the National Park Service and the Library of Congress, put up web sites on major historical places and topics. Eventually, new tools, such as {JSTOR} and {ProQuest}, opened up full-text facsimiles of journal articles and major newspapers. Research libraries took the lead in developing their catalogs and collections for online access. Teaching everything from the U.S. history survey to specialized research seminars became more dynamic and student-centered. The primary sources of the past were open for students in ways unimaginable only a decade earlier. But just as research techniques and tools were being transformed by the new media, would scholarship also change? If so, how, and in what ways?

A whole new field opened up around the concept of digital history as historians tried to experiment with the new medium. They began using new tools that computational systems and networked information made available. Geographic Information Systems ({GIS}) have become prominent because of the wide interest in more spatial approaches to the past, but a whole range of technologies proved useful: Flash animation, {XML} coding, digital video, blogs, and wikis. Because the medium is still so new in comparison to traditional modes of communication, and the technology is still rapidly changing, we historians have only just begun to explore what history looks like in the digital medium. Increasingly, university departments are seeking scholars to translate history into this fast-paced, widely accessible environment and to work in digital history; however, they have found that without well-defined examples of digital scholarship, established best practices, and, especially, clear standards of review for tenure, few scholars have fully engaged with the digital medium. So, what is digital history and how should we understand its characteristics?},
	issue = {Paper 98},
	journaltitle = {Faculty Publications, Department of History},
	author = {Seefeldt, Douglas and Thomas, William G.},
	date = {2009},
	langid = {english},
	keywords = {X-{CHECK}},
}

@article{kirschenbaum_what_2010,
	title = {What Is Digital Humanities and What's It Doing in English Departments?},
	url = {http://mkirschenbaum.files.wordpress.com/2011/01/kirschenbaum_ade150.pdf},
	abstract = {People who say that the last battles of the computer revolution in En
glish departments have been
fought and won don’t know what they’re talking about. If our current use of computers in En
glish
studies is marked by any common theme at all, it is experimentation at the most basic level. As a pro
-
fession, we are just learning how to live with computers, just beginning to integrate these machines
effectively into writing- and reading-
intensive courses, just starting to consider the implications of the
multilayered literacy associated with computers},
	pages = {(preprint)},
	number = {150},
	journaltitle = {{ADE} Bulletin},
	author = {Kirschenbaum, Matthew G.},
	date = {2010},
	langid = {english},
	keywords = {*****, meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{unsworth_what_2002,
	title = {What is Humanities Computing and What is Not?},
	url = {http://computerphilologie.digital-humanities.de/jg02/unsworth.html},
	pages = {71--83},
	number = {4},
	journaltitle = {Jahrbuch für Computerphilologie 4 (2002)},
	author = {Unsworth, John},
	date = {2002},
	langid = {english},
}

@report{wissenschaftsrat_ubergreifende_2011,
	location = {Berlin},
	title = {Übergreifende Empfehlungen zu Informationsinfrastrukturen},
	url = {http://www.wissenschaftsrat.de/download/archiv/10466-11.pdf},
	institution = {Wissenschaftsrat},
	author = {{Wissenschaftsrat}},
	date = {2011-01-28},
	langid = {german},
	keywords = {meta\_DefinePolicy},
}

@collection{berry_understanding_2012,
	title = {Understanding Digital Humanities},
	isbn = {9780230292642},
	url = {http://www.palgrave.com/page/detail/?sf1=id_product&st1=493310},
	abstract = {The application of new computational techniques and visualisation technologies in the Arts and Humanities are resulting in fresh 
approaches and methodologies for the study of new and traditional corpora. This 'computational turn' takes the methods and techniques from computer science to create innovative means of close and distant reading. This book discusses the implications and applications of 'Digital Humanities' and the questions raised when using algorithmic techniques. Key researchers in the field provide a comprehensive introduction to important debates surrounding issues such as the contrast between narrative versus database, pattern-matching versus hermeneutics, and the statistical paradigm versus the data mining paradigm. Also discussed are the new forms of collaboration within the Arts and Humanities that are raised through modular research teams and new organisational structures, as well as techniques for collaborating in an interdisciplinary way.},
	publisher = {Palgrave Macmillan},
	editor = {Berry, David M.},
	date = {2012-02},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@incollection{unsworth_university_2008,
	title = {University 2.0},
	url = {https://net.educause.edu/ir/library/pdf/PUB7202w.pdf},
	abstract = {The major challenge facing universities in the next decade is to reinvent themselves as information 
organizations. Universities are, at their core, organizations that cultivate knowledge, seeking 
both to create new knowl The major challenge facing universities in the next decade is to reinvent 
themselves as information organizations. Universities are, at their core, organizations that cultivate 
knowledge, seeking both to create new knowledge and to preserve and convey existing 
knowledge, but they are remarkably inefficient and therefore ineffective in the way that ...},
	pages = {227--237},
	booktitle = {The Tower and the Cloud. Higher Education in the World of Cloud Computing},
	publisher = {Educause},
	author = {Unsworth, John},
	editor = {Katz, Richard N.},
	date = {2008},
	langid = {english},
}

@article{harms_usability_2011,
	title = {Usability of Generic Software in e-Research Infrastructures},
	volume = {1},
	url = {https://letterpress.uchicago.edu/index.php/jdhcs/article/view/89},
	abstract = {In e-Research Infrastructures ({eRIs}), software is used in diverse application contexts. To support this software is often implemented generically. The usability of software is strongly context dependent. Therefore, the use of generic software in different application contexts results in varying degress of usability depending on the concrete usage scenario. This paper focuses on the challenges of implementing usability-oriented generic software. First, we provide an introduction to generic software in the context of {eRIs}. Next, we offer an overview of usability and appropriate considerations in the software development process. Based on this, we demonstrate discrepancies between good usability and the application of generic software in distinct contexts. Finally, we provide a first architectural concept to address the identified challenges.},
	number = {3},
	journaltitle = {Journal of the Chicago Colloquium on Digital Humanities and Computer Science},
	shortjournal = {{JDHCS}},
	author = {Harms, Patrick and Grabowski, Jens},
	date = {2011-07-14},
	langid = {english},
	keywords = {obj\_Infrastructures, obj\_Tools, t\_Usability},
}

@book{lock_using_2003,
	title = {Using Computers in Archaeology: Towards Virtual Pasts},
	isbn = {0415167701},
	url = {http://www.amazon.de/Using-Computers-Archaeology-Towards-Virtual/dp/0415167701},
	series = {towards virtual pasts},
	abstract = {Today, archaeologists are spending more and more time examining the past with the aid of computers. How does this increased dependence on technology affect the theory and practice of archaeology? Using Computers in Archaeology is a comprehensive review of computer applications in archaeology from the archaeologist's perspective. The book deals with all aspects of the discipline, from survey and excavation, to museums and education. Discussion covers the theoretical aspects of computer applications, with particular reference to {GIS} and the analysis of data, but technical jargon is kept to a minimum. With numerous illustrations, case-studies and examples, Using Computers in Archaeology is a timely introduction to this increasingly important area of archaeology, catering both for the student and the experienced archaeologist.},
	pagetotal = {300},
	publisher = {Psychology Press},
	author = {Lock, Gary R},
	date = {2003},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_Artefacts, obj\_Tools},
}

@article{pennebaker_using_2011,
	title = {Using literature to understand authors: The case for computerized text analysis},
	volume = {1},
	issn = {22104372, 22104380},
	url = {http://openurl.ingenta.com/content/xref?genre=article&issn=2210-4372&volume=1&issue=1&spage=34},
	doi = {10.1075/ssol.1.1.04pen},
	shorttitle = {Using literature to understand authors},
	abstract = {Through computerized text analysis, the psychology of literature is on the threshold of becoming a dominant force in psychology and the social sciences. The ways people use words in their writing and in everyday life reflect people's social and psychological states. Whereas most text analysis research has focused on the content of people's writings, the current paper demonstrates that almost-invisible function words can be psychologically relevant as well. Through the analysis of pronouns, prepositions, and other function words used in literature, several studies demonstrate how authors' emotional states, aging processes, theories of mind, and the nature of their romantic and collaborative relationships are revealed through their words. The function word approach provides a glimpse of the rapidly expanding methods available to psychologists interested in tracking the social and psychological worlds of authors. With the upcoming release of data sets such as Google Books, the analysis of literature will likely serve as a foundational method used in the fields of psychology, linguistics, history, and other areas of the behavioral and social sciences.},
	pages = {34--48},
	number = {1},
	journaltitle = {Scientific Study of Literature},
	author = {Pennebaker, James W. and Ireland, Molly E.},
	urldate = {2012-04-25},
	date = {2011-01-01},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, obj\_Literature},
}

@article{khmelev_using_2001,
	title = {Using Markov Chains for Identification of Writers},
	volume = {16},
	url = {http://llc.oxfordjournals.org/content/16/3/299.full.pdf},
	abstract = {In this paper we present a technique for authorship attribution based on a simple Markov chain of letters (i.e., just letter bigrams are used). Many proposed methods of authorship attribution are illustrated on small examples. We show that this technique provides excellent results when applied to over 380 texts from the Project Gutenberg archives, as well as to two previously published data sets.},
	pages = {299--307},
	number = {4},
	journaltitle = {Literary \& Linguistic Computing},
	author = {Khmelev, D. and Tweedie, Fiona},
	date = {2001},
	langid = {english},
	note = {http://www.philol.msu.ru/{\textasciitilde}lex/khmelev/published/llc/khmelev.html},
	keywords = {{AnalyzeStatistically}, goal\_Analysis, obj\_Text, t\_MachineLearning, t\_MarkovChains, t\_Stylometry},
}

@report{reiche_verfahren_2014,
	location = {Göttingen},
	title = {Verfahren der Digital Humanities in den Geistes- und Kulturwissenschaften},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2014-4.pdf},
	abstract = {Verfahren der Digital Humanities ({DH}) gehen über einen Transfer analoger in digitale Verfahren hinaus und eröffnen neue Forschungsperspektiven, so dass neue digitale Arbeits-, Forschungs- und Wissensformen entstehen. Dieser Report hat deshalb zum Ziel, einen ersten und noch keineswegs umfassenden Überblick über in einzelnen Disziplinen verwendete {DH}-Verfahren zu erstellen sowie einige sich aus der Anwendung von {DH}-Verfahren ergebende Perspektiven und Möglichkeiten aufzuzeigen. Exemplarisch betrachtet werden in vorliegendem Report die Philologien, die Geschichtswissenschaft, die Kunstgeschichte, die Archäologie, die Musikwissenschaften, die Theologie, die Philosophie und die Jüdischen Studien. Resümierend wird insbesondere auf solche {DH}-Verfahren abgezielt, die Potentiale über eine bestimmte Fachdisziplin hinausgehend aufweisen. Aus diesem Grund gibt der vorliegende Report auch Impulse für in diesem Bericht nicht berücksichtigte Fachdisziplinen und eine disziplinenübergreifende Perspektive für die weitere Entwicklung der {DARIAH}-{DE}-Infrastruktur.},
	number = {4},
	institution = {{DARIAH}-{DE}},
	author = {Reiche, Ruth and Becker, Rainer and Bender, Michael and Munson, Matt and Schmunk, Stefan and Schöch, Christof},
	date = {2014},
	langid = {german},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{dacos_vers_2012,
	title = {Vers des médias numériques en sciences humaines et sociales},
	url = {http://halshs.archives-ouvertes.fr/halshs-00700238},
	abstract = {Sciences de l'Homme et de la Société},
	journaltitle = {Tracés. Revue de sciences humaines},
	author = {Dacos, Marin},
	urldate = {2012-08-18},
	date = {2012-06-15},
	langid = {french},
	keywords = {meta\_Advocating, obj\_DigitalHumanities},
}

@video{cohen_video:_2011,
	title = {Video: The Ivory Tower and the Open Web},
	url = {http://www.dancohen.org/2011/01/19/video-the-ivory-tower-and-the-open-web/},
	shorttitle = {Video},
	abstract = {The web is now over twenty years old, and there is no doubt that the academy has taken advantage of its tremendous potential for disseminating resources and scholarship. But a full accounting of the academic approach to the web shows that compared to the innovative vernacular forms that have flourished over the past two decades, we have been relatively meek in our use of the medium, often preferring to impose traditional ivory tower genres on the web rather than import the open web’s most successful models. For instance, we would rather digitize the journal we know than explore how blogs and social media might supplement or change our scholarly research and communication. What might happen if we reversed that flow and more wholeheartedly embraced the genres of the open web?},
	author = {Cohen, Dan},
	urldate = {2011-12-18},
	date = {2011-01-19},
	langid = {english},
	keywords = {goal\_Analysis, meta\_GiveOverview, obj\_OnlineContent},
}

@article{neuroth_virtuelle_2009,
	title = {Virtuelle Forschungsumgebungen für e-Humanities. Maßnahmen zur optimalen Unterstützung von Forschungsprozessen in den Geisteswissenschaften},
	volume = {33},
	issn = {0341-4183},
	url = {https://www.b2i.de/fileadmin/dokumente/BFP_Bestand_2009/Jg_33-Nr_2/Jg_33-Nr_2_Aufsaetze/Jg_33-2009-Nr_2-S_161-169.pdf},
	doi = {10.1515/bfup.2009.017},
	abstract = {e-Science bzw. e-Research führt wissenschaftliche Methoden zusammen, die bislang getrennt verfolgt wurden: Experiment, Theorie und Simulation werden in einer „datenzentrierten Wissenschaft“ vereinigt. Während dies für die Naturwissenschaften bereits zutrifft, sind die Geisteswissenschaften in dieser „4. Generation der Wissenschaften“ noch nicht vollständig angekommen. Doch gerade diese Fachdisziplinen eignen sich hervorragend, um in Form von Virtuellen Forschungsumgebungen durch den Zugang zu relevanten Ressourcen wie Inhalten, Forschungsdaten und Diensten, Tools kooperativ und kollaborativ auch über Disziplingrenzen hinweg zu forschen. Der Artikel gibt einen Überblick und benennt konkrete Maßnahmen, wie die Geisteswissenschaften in enger Zusammenarbeit mit Informationsspezialisten die Vorteile innovativer digitaler Technologien für den Aufbau und die Weiterentwicklung von Virtuellen Forschungsumgebungen nutzen können.},
	pages = {161--169},
	number = {2},
	journaltitle = {{BIBLIOTHEK} Forschung und Praxis},
	shortjournal = {{BIBLIOTHEK} Forschung und Praxis},
	author = {Neuroth, Heike and Jannidis, Fotis and Rapp, Andrea and Lohmeier, Felix},
	urldate = {2011-04-03},
	date = {2009-09},
	langid = {german},
	keywords = {act\_Collaborating, goal\_Collaboration, obj\_Infrastructures, obj\_VREs},
}

@online{bluestein_big_2012,
	title = {Big Data On Campus Is Like A Keg Stand For Your Brain},
	url = {http://www.fastcompany.com/node/1843980/},
	abstract = {Take a crash course in digital humanities with {McGill} professor Stefan Sinclair.},
	titleaddon = {Fastcompany},
	author = {Bluestein, Adam},
	urldate = {2012-07-30},
	date = {2012-07-31},
	langid = {english},
	keywords = {bigdata{\textasciitilde}, meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{sunyer_big_2013,
	title = {Big data meets the Bard},
	issn = {0307-1766},
	url = {http://www.ft.com/cms/s/2/fb67c556-d36e-11e2-b3ff-00144feab7de.html#axzz2WZKbigUO},
	abstract = {Here’s some advice for bibliophiles with teetering piles of books and not enough hours in the day: don’t read them. Instead, feed the books into a computer program and make graphs, maps and charts: it is the best way to get to grips with the},
	journaltitle = {Financial Times},
	author = {Sunyer, John},
	urldate = {2013-06-27},
	date = {2013-06-15},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata{\textasciitilde}},
}

@article{wladawsky-berger_big_2014,
	title = {Big Data May Spark Scientific Revolution in Humanities},
	url = {http://blogs.wsj.com/cio/2014/01/17/big-data-may-spark-scientific-revolution-in-humanities/},
	journaltitle = {The Wall Street Journal},
	author = {Wladawsky-Berger, Irving},
	urldate = {2014-01-22},
	date = {2014-01-17},
	langid = {english},
	keywords = {bigdata{\textasciitilde}},
}

@article{steadman_big_2013,
	title = {Big data and the death of the theorist},
	url = {http://www.wired.co.uk/news/archive/2013-01/25/big-data-end-of-theory},
	abstract = {Plenty of people have foreseen the death of the scientific theory at the hands of big data analysis, but when computers become good enough to understand literature, art and human history, will it spell the end for the humanities academic?},
	journaltitle = {Wired Magazine ({UK})},
	author = {Steadman, Ian},
	date = {2013-01-25},
	langid = {english},
	keywords = {bigdata{\textasciitilde}, goal\_Analysis, goal\_Interpretation},
}

@article{bobenhausen_automatisches_2009,
	title = {Automatisches metrisches Markup deutschsprachiger Gedichte},
	volume = {7},
	url = {http://computerphilologie.tu-darmstadt.de/jg07/bobgehl.html},
	abstract = {Metrical markup done manually demands an enormous effort of time. Wouldn’t it be smart to let computers do the work automatically – and would that be possible? The answer is »Yes«. The following text describes how automatic metrical markup for stressed and unstressed syllables in German verse text can be achieved on the bases of theoretical postulations and their methodical realization.},
	journaltitle = {Jahrbuch für Computerphilologie},
	author = {Bobenhausen, Klemens and Gehl, Günter},
	date = {2009},
	langid = {german},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@article{nicholas_abstract_2010,
	title = {Abstract Modelling of Digital Identifiers},
	volume = {62},
	issn = {1361-3200},
	url = {http://www.ariadne.ac.uk/issue62/nicholas-et-al/},
	abstract = {Discussion of digital identifiers, and persistent identifiers in particular, has often been confused by differences in underlying assumptions and approaches. To bring more clarity to such discussions, the {PILIN} Project has devised an abstract model of identifiers and identifier services, which is presented here in summary. Given such an abstract model, it is possible to compare different identifier schemes, despite variations in terminology; and policies and strategies can be formulated for persistence without committing to particular systems. The abstract model is formal and layered; in this article, we give an overview of the distinctions made in the model. This presentation is not exhaustive, but it presents some of the key concepts represented, and some of the insights that result.

The main goal of the Persistent Identifier Linking Infrastructure ({PILIN}) project [1] has been to scope the infrastructure necessary for a national persistent identifier service. There are a variety of approaches and technologies already on offer for persistent digital identification of objects. But true identity persistence cannot be bound to particular technologies, domain policies, or information models: any formulation of a persistent identifier strategy needs to outlast current technologies, if the identifiers are to remain persistent in the long term.

For that reason, {PILIN} has modelled the digital identifier space in the abstract. It has arrived at an ontology [2] and a service model [3] for digital identifiers, and for how they are used and managed, building on previous work in the identifier field [4] (including the thinking behind {URI} [5], {DOI} [6], {XRI} [7] and {ARK} [8]), as well as semiotic theory [9]. The ontology, as an abstract model, addresses the question ‘what is (and isn’t) an identifier?’ and ‘what does an identifier management system do?’. This more abstract view also brings clarity to the ongoing conversation of whether {URIs} can be (and should be) universal persistent identifiers.},
	journaltitle = {Ariadne},
	author = {Nicholas, Nick and Ward, Nigel and Blinco, Kerry},
	urldate = {2010-02-23},
	date = {2010-01-30},
	langid = {english},
	keywords = {act\_Identifying, bigdata, bigdata{\textasciitilde}, obj\_Research},
}
@book{lima_visual_2011,
	location = {Princeton, N.J.},
	title = {Visual Complexity: Mapping Patterns of Information},
	isbn = {1568989369},
	url = {http://www.amazon.de/exec/obidos/ASIN/1568989369/visualcompl0f-20/},
	shorttitle = {Visual Complexity},
	abstract = {Several researchers, scientists and designers across the globe are trying to make sense of a variety of complex networks employing an innovative mix of colors, symbols, graphics, algorithms, and interactivity to clarify, and often beautify, the clutter. By doing this they are in many ways creating the syntax of a new language. This book can be seen as the first dictionary of this new lexicon.

In Visual Complexity: Mapping Patterns of Information, Manuel Lima collects and presents almost three hundred of the most compelling examples of information design — everything from representing networks of followers on Twitter and the eighty-five recorded covers of Joy Division’s “Love Will Tear Us Apart” to depicting interconnections between members of the Al Queda network and interactions among proteins in a human cell. Lima also looks at the long tradition of mapping complex networks, offering the first book to integrate a thorough history of network vizualization with an examination of the real-life situations from which these graphics are generated.},
	pagetotal = {272},
	publisher = {Princeton Univ. Press},
	author = {Lima, Manuel},
	date = {2011},
	langid = {english},
	keywords = {act\_Visualizing, meta\_GiveOverview},
}

@article{labbe_tool_2006,
	title = {A Tool for Literary Studies: Intertextual Distance and Tree Classification},
	volume = {21},
	url = {http://llc.oxfordjournals.org/content/21/3/311.abstract},
	doi = {10.1093/llc/fqi063},
	shorttitle = {A Tool for Literary Studies},
	abstract = {How to measure proximities and oppositions in large text corpora? Intertextual distance provides a simple and interesting solution. Its properties make it a good tool for text classification, and especially for tree-analysis which is fully presented and discussed here. In order to measure the quality of this classification, two indices are proposed. The method presented provides an accurate tool for literary studies—as is demonstrated by applying it to two areas of French literature, Racine's tragedies and an authorship attribution experiment.},
	pages = {311 --326},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	author = {Labbé, Cyril and Labbé, Dominique},
	urldate = {2011-10-05},
	date = {2006},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, t\_Stylometry},
}

@report{heuser_quantitative_2012,
	location = {Standford {CA}},
	title = {A Quantitative Literary History of 2,958 Nineteenth-Century British Novels: The Semantic Cohort Method},
	url = {http://litlab.stanford.edu/LiteraryLabPamphlet4.pdf},
	abstract = {The nineteenth century in Britain saw tumultuous changes that reshaped the fabric of society and altered the course of modernization. It also saw the rise of the novel to the height of its cultural power as the most important literary form of the period. This paper reports on a long-term experiment in tracing such macroscopic changes in the novel during this crucial period. Specifically, we present findings on two interrelated transformations in novelistic language that reveal a systemic concretization in language and fundamental change in the social spaces of the novel. We show how these shifts have consequences for setting, characterization, and narration as well as implications for the responsiveness of the novel to the dramatic changes in British society.

This paper has a second strand as well. This project was simultaneously an experiment in developing quantitative and computational methods for tracing changes in literary language. We wanted to see how far quantifiable features such as word usage could be pushed toward the investigation of literary history. Could we leverage quantitative methods in ways that respect the nuance and complexity we value in the humanities? To this end, we present a second set of results, the techniques and methodological lessons gained in the course of designing and running this project.},
	institution = {Literary Lab, Stanford University},
	author = {Heuser, Ryan and Le-Khac, Long},
	date = {2012-05},
	langid = {english},
	keywords = {bigdata},
}

@inproceedings{zhang_novel_2009,
	location = {Stroudsburg, {PA}, {USA}},
	title = {A novel approach to automatic gazetteer generation using Wikipedia},
	isbn = {978-1-932432-55-8},
	url = {http://dl.acm.org/citation.cfm?id=1699765.1699766},
	series = {People's Web '09},
	abstract = {Gazetteers or entity dictionaries are important knowledge resources for solving a wide range of {NLP} problems, such as entity extraction. We introduce a novel method to automatically generate gazetteers from seed lists using an external knowledge resource, the Wikipedia. Unlike previous methods, our method exploits the rich content and various structural elements of Wikipedia, and does not rely on language- or domain-specific knowledge. Furthermore, applying the extended gazetteers to an entity extraction task in a scientific domain, we empirically observed a significant improvement in system accuracy when compared with those using seed gazetteers.},
	pages = {1--9},
	booktitle = {Proceedings of the 2009 Workshop on The People's Web Meets {NLP}: Collaboratively Constructed Semantic Resources},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Ziqi and Iria, José},
	urldate = {2013-05-06},
	date = {2009},
	langid = {english},
	keywords = {bigdata, t\_NamedEntityRecognition},
}

@article{baroni_new_2006,
	title = {A New Approach to the Study of Translationese: Machine-learning the Difference between Original and Translated Text},
	volume = {21},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/21/3/259},
	doi = {10.1093/llc/fqi039},
	shorttitle = {A New Approach to the Study of Translationese},
	abstract = {In this article we describe an approach to the identification of ‘translationese’ based on monolingual comparable corpora and machine learning techniques for text categorization. The article reports on experiments in which support vector machines ({SVMs}) are employed to recognize translated text in a corpus of Italian articles from the geopolitical domain. An ensemble of {SVMs} reaches 86.7\% accuracy with 89.3\% precision and 83.3\% recall on this task. A preliminary analysis of the features used by the {SVMs} suggests that the distribution of function words and morphosyntactic categories in general, and personal pronouns and adverbs in particular, are among the cues used by the {SVMs} to perform the discrimination task. A follow-up experiment shows that the performance attained by {SVMs} is well above the average performance of ten human subjects, including five professional translators, on the same task. Our results offer solid evidence supporting the translationese hypothesis, and our method seems to have promising applications in translation studies and in quantitative style analysis in general. Implications for the machine learning/text categorization community are equally important, both because this is a novel application and especially because we provide explicit evidence that a relatively knowledge-poor machine learning algorithm can outperform human beings in a text classification task.},
	pages = {259--274},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Baroni, Marco and Bernardini, Silvia},
	urldate = {2013-03-19},
	date = {2006-09-01},
	langid = {english},
	keywords = {bigdata, t\_MachineLearning},
}

@inproceedings{li_framework_2009,
	location = {Stroudsburg, {PA}, {USA}},
	title = {A framework of feature selection methods for text categorization},
	isbn = {978-1-932432-46-6},
	url = {http://dl.acm.org/citation.cfm?id=1690219.1690243},
	series = {{ACL} '09},
	abstract = {In text categorization, feature selection ({FS}) is a strategy that aims at making text classifiers more efficient and accurate. However, when dealing with a new task, it is still difficult to quickly select a suitable one from various {FS} methods provided by many previous studies. In this paper, we propose a theoretic framework of {FS} methods based on two basic measurements: frequency measurement and ratio measurement. Then six popular {FS} methods are in detail discussed under this framework. Moreover, with the guidance of our theoretical analysis, we propose a novel method called weighed frequency and odds ({WFO}) that combines the two measurements with trained weights. The experimental results on data sets from both topic-based and sentiment classification tasks show that this new method is robust across different tasks and numbers of selected features.},
	pages = {692--700},
	booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}: Volume 2 - Volume 2},
	publisher = {Association for Computational Linguistics},
	author = {Li, Shoushan and Xia, Rui and Zong, Chengqing and Huang, Chu-Ren},
	urldate = {2012-12-03},
	date = {2009},
	langid = {english},
	keywords = {X-{CHECK}, bigdata},
}

@article{vis_critical_2013,
	title = {A critical reflection on Big Data: Considering {APIs}, researchers and tools as data makers},
	volume = {18},
	rights = {Authors submitting a paper to First Monday automatically agree to confer a limited license to First Monday if and when the manuscript is accepted for publication. This license allows First Monday to publish a manuscript in a given issue. Authors have a choice of: 1. Dedicating the article to the public domain. This allows anyone to make any use of the article at any time, including commercial use. A good way to do this is to use the Creative Commons Public Domain Dedication Web form; see  http://creativecommons.org/license/publicdomain-2?lang=en . 2. Retaining some rights while allowing some use. For example, authors may decide to disallow commercial use without permission. Authors may also decide whether to allow users to make modifications (e.g. translations, adaptations) without permission. A good way to make these choices is to use a Creative Commons license. * Go to  http://creativecommons.org/license/ . * Choose and select a license. * What to do next — you can then e–mail the license html code to yourself. Do this, and then forward that e–mail to First Monday’s editors. Put your name in the subject line of the e–mail with your name and article title in the e–mail. Background information about Creative Commons licenses can be found at  http://creativecommons.org/about/licenses/ . 3. Retaining full rights, including translation and reproduction rights. Authors may use the statement: © Author 2008 All Rights Reserved. Authors may choose to use their own wording to reserve copyright. If you choose to retain full copyright, please add your copyright statement to the end of the article. Authors submitting a paper to First Monday do so in the understanding that Internet publishing is both an opportunity and challenge. In this environment, authors and publishers do not always have the means to protect against unauthorized copying or editing of copyright–protected works.},
	issn = {13960466},
	url = {http://firstmonday.org/ojs/index.php/fm/article/view/4878},
	doi = {10.5210/fm.v18i10.4878},
	shorttitle = {A critical reflection on Big Data},
	abstract = {This paper looks at how data is ‘made’, by whom and how. Rather than assuming data already exists ‘out there’, waiting to simply be recovered and turned into findings, the paper examines how data is co–produced through dynamic research intersections. A particular focus is the intersections between the application programming interface ({API}), the researcher collecting the data as well as the tools used to process it. In light of this, this paper offers three new ways to define and think about Big Data and proposes a series of practical suggestions for making data.},
	number = {10},
	journaltitle = {First Monday},
	author = {Vis, Farida},
	urldate = {2014-01-28},
	date = {2013-10-02},
	langid = {english},
	keywords = {bigdata},
}

@inproceedings{burgoyne_comparative_2007,
	location = {Vienna, Austria},
	title = {A Comparative Survey of Image Binarisation Algorithms for Optical Recognition on Degraded Musical Sources},
	volume = {509-12},
	url = {http://www.aruspix.net/publications/burgoyne07comparative.pdf},
	abstract = {Binarisation of greyscale images is a critical step in optic
al music recognition ({OMR}) preprocessing. Binarising mu-
sic documents is particularly challenging because of the
nature of music notation, even more so when the sources
are degraded, e.g., with ink bleed-through from the other
side of the page. This paper presents a comparative eval-
uation of 25 binarisation algorithms tested on a set of 100
music pages. A real-world {OMR} infrastructure for early
music (Aruspix) was used to perform an objective, goal-
directed evaluation of the algorithms’ performance. Our
results differ significantly from the ones obtained in stud-
ies on non-music documents, which highlights the impor-
tance of developing tools specific to our community.},
	booktitle = {Proceedings of the 8th International Conference on Music Information Retrieval ({ISMIR} 2007)},
	author = {Burgoyne, John Ashley and Pugin, Laurent and Eustace, Greg and Fujinaga, Ichiro},
	date = {2007},
	langid = {english},
	keywords = {act\_DataRecognition, bigdata, obj\_SheetMusic},
}

@inproceedings{yang_comparative_1997,
	title = {A Comparative Study on Feature Selection in Text Categorization},
	abstract = {This paper is a comparative study of feature selection methods in statistical learning of text categorization. The focus is on aggressive dimensionality reduction. Five methods were evaluated, including term selection based on document frequency ({DF}), information gain ({IG}), mutual information ({MI}), a  Ø  2  -test ({CHI}), and term strength ({TS}). We found {IG} and {CHI} most effective in our experiments. Using {IG} thresholding with a knearest neighbor classifier on the Reuters corpus, removal of up to 98\% removal of unique terms actually yielded an improved classification accuracy (measured by average precision) . {DF} thresholding performed similarly. Indeed we found strong correlations between the {DF}, {IG} and {CHI} values of a term. This suggests that {DF} thresholding, the simplest method with the lowest cost in computation, can be reliably used instead of {IG} or {CHI} when the computation of these measures are too expensive. {TS} compares favorably with the other methods with up to 50\% vocabulary redu...},
	pages = {412--420},
	publisher = {Morgan Kaufmann Publishers},
	author = {Yang, Yiming and Pedersen, Jan O.},
	date = {1997},
	keywords = {{AnalyzeStatistically}, bigdata, t\_MachineLearning},
}

@book{cleveland_visualizing_1993,
	location = {Murray Hill, N.J. : [Summit, N.J},
	title = {Visualizing data},
	isbn = {0963488406},
	url = {http://dl.acm.org/citation.cfm?id=529269},
	abstract = {Enormous quantities of data go unused or underused today, simply because people can't visualize the quantities and relationships in it. Using a downloadable programming environment developed by the author, Visualizing Data demonstrates methods for representing data accurately on the Web and elsewhere, complete with user interaction, animation, and more. How do the 3.1 billion A, C, G and T letters of the human genome compare to those of a chimp or a mouse? What do the paths that millions of visitors take through a web site look like? With Visualizing Data, you learn how to answer complex questions like these with thoroughly interactive displays. We're not talking about cookie-cutter charts and graphs. This book teaches you how to design entire interfaces around large, complex data sets with the help of a powerful new design and prototyping tool called "Processing". Used by many researchers and companies to convey specific data in a clear and understandable manner, the Processing beta is available free. With this tool and Visualizing Data as a guide, you'll learn basic visualization principles, how to choose the right kind of display for your purposes, and how to provide interactive features that will bring users to your site over and over. This book teaches you: The seven stages of visualizing data -- acquire, parse, filter, mine, represent, refine, and {interactHow} all data problems begin with a question and end with a narrative construct that provides a clear answer without extraneous {detailsSeveral} example projects with the code to make them {workPositive} and negative points of each representation discussed. The focus is on customization so that each one best suits what you want to convey about your data set The book does not provide ready-made "visualizations" that can be plugged into any data set. Instead, with chapters divided by types of data rather than types of display, you'll learn how each visualization conveys the unique properties of the data it represents -- why the data was collected, what's interesting about it, and what stories it can tell. Visualizing Data teaches you how to answer questions, not simply display information.},
	pagetotal = {360},
	publisher = {At\&T Bell Laboratories ; Published by Hobart Press},
	author = {Cleveland, William S.},
	date = {1993},
	langid = {english},
}

@article{evans_computational_2014,
	title = {A Computational Approach to Qualitative Analysis in Large Textual Datasets},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pone.0087908},
	doi = {10.1371/journal.pone.0087908},
	abstract = {In this paper I introduce computational techniques to extend qualitative analysis into the study of large textual datasets. I demonstrate these techniques by using probabilistic topic modeling to analyze a broad sample of 14,952 documents published in major American newspapers from 1980 through 2012. I show how computational data mining techniques can identify and evaluate the significance of qualitatively distinct subjects of discussion across a wide range of public discourse. I also show how examining large textual datasets with computational methods can overcome methodological limitations of conventional qualitative methods, such as how to measure the impact of particular cases on broader discourse, how to validate substantive inferences from small samples of textual data, and how to determine if identified cases are part of a consistent temporal pattern.},
	number = {2},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Evans, Michael S.},
	urldate = {2014-02-04},
	date = {2014-02-03},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@inproceedings{chaney_visualizing_2012,
	title = {Visualizing Topic Models},
	url = {http://www.cs.columbia.edu/~blei/papers/ChaneyBlei2012.pdf},
	abstract = {Managing large collections of documents is an important
problem for many areas of science, industry, and
culture. Probabilistic topic modeling offers a promising
solution. Topic modeling is an unsupervised machine
learning method that learns the underlying themes in
a large collection of otherwise unorganized documents.
This discovered structure summarizes and organizes the
documents. However, topic models are high-level statistical
tools—a user must scrutinize numerical distributions
to understand and explore their results. In this
paper, we present a method for visualizing topic models.
Our method creates a navigator of the documents,
allowing users to explore the hidden structure that a
topic model discovers. These browsing interfaces reveal
meaningful patterns in a collection, helping end-users
explore and understand its contents in new ways. We
provide open source software of our method.},
	eventtitle = {{AAAI} Publications, Sixth International {AAAI} Conference on Weblogs and Social Media},
	pages = {4},
	booktitle = {{AAAI} Publications, Sixth International {AAAI} Conference on Weblogs and Social Media},
	author = {Chaney, Allison and Blei, David M.},
	date = {2012},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_Visualizing},
}

@article{argamon_vive_2009,
	title = {Vive la Différence! Text Mining Gender Difference in French Literature},
	volume = {3},
	url = {http://www.digitalhumanities.org/dhq/vol/3/2/000042.html},
	abstract = {In this study, a corpus of 300 male-authored and 300 female-authored French literary and historical texts is classified for author gender using the Support Vector Machine ({SVM}) implementation {SVMLight}, achieving up to 90\% classification accuracy. The sets of words that were most useful in distinguishing male and female writing are extracted from the support vectors. The results reinforce previous findings from statistical analyses of the same corpus, and exhibit remarkable cross-linguistic parallels with the results garnered from {SVM} models trained in gender classification on selections from the British National Corpus. It is found that female authors use personal pronouns and negative polarity items at a much higher rate than their male counterparts, and male authors demonstrate a strong preference for determiners and numerical quantifiers. Among the words that characterize male or female writing consistently over the time period spanned by the corpus, a number of cohesive semantic groups are identified. Male authors, for example, use religious terminology rooted in the church, while female authors use secular language to discuss spirituality. Such differences would take an enormous human effort to discover by a close reading of such a large corpus, but once identified through text mining, they frame intriguing questions which scholars may address using traditional critical analysis methods.},
	number = {2},
	journaltitle = {Digital Humanities Quarterly},
	author = {Argamon, Shlomo and Goulain, Jean-Baptiste and Horton, Russell and Olsen, Mark},
	urldate = {2012-01-08},
	date = {2009-06},
	langid = {english},
	keywords = {t\_Stylometry},
}

@inproceedings{jurt_vom_2012,
	location = {Wien},
	title = {Vom Aufsatz und Buch zum Journal. Wandel der Publikationskulturen in unterschiedlichen Wissenschaftsdisziplinen. Das Beispiel der Geisteswissenschaften},
	url = {http://www.oefg.at/legacy/text/veranstaltungen/qualitaetssicherung/beitrag_jurt.pdf},
	eventtitle = {Qualitätssicherung in der Forschung},
	publisher = {Ö{FG}},
	author = {Jurt, Joseph},
	date = {2012-05},
	langid = {german},
}

@incollection{schomburg_von_2011,
	location = {Köln},
	edition = {2., ergänzte Fassung},
	title = {Von der Schulbank in die Wissenschaft. Einsatz der Virtuellen Arbeits- und Forschungsumgebung von Edumeres.net in der internationalen Bildungsmedienforschung},
	url = {http://www.hbz-nrw.de/dokumentencenter/veroeffentlichungen/Tagung_Digitale_Wissenschaft.pdf},
	abstract = {Bildungsmedienforschung ist ein Disziplinen und Ländergrenzen übergreifendes Forschungsfeld, dessen Arbeit sich an der Schnittstelle von Wissenschaft, Bildungspolitik und Praxis bewegt. Ein so heterogen ausgerichtetes Forschungsgebiet erfordert für kollaboratives Forschen und Arbeiten eine Infrastruktur, die durch Konferenzen und traditionelle Formen der Kommunikation nur teilweise abgedeckt werden kann.

Aus dieser Situation heraus entwickelt und erprobt das Georg-Eckert-Institut für internationale Schulbuchforschung in Braunschweig mit Unterstützung durch die Deutsche Forschungsgemeinschaft eine virtuelle Arbeits- und Forschungsumgebung. Diese wird ab dem Sommer 2010 als elementarer und wichtigster Baustein im Informations- und Kommunikationsportal zur Bildungsmedienforschung „Edumeres.net" der wissenschaftlichen Community zugänglich sein.

Das Konzept dieser Forschungsumgebung sieht dabei vor, den Wissenschaftlern zwei Module an die Hand zu geben: ein Community-Modul zur Vernetzung der Teilnehmer und deren Kommunikation untereinander sowie ein Projekt-Modul, in dem einzelne Nutzer-Gruppen gemeinsam, von der Themenfindung, über die Arbeit an Dokumenten, hin zur elektronischen Veröffentlichung und Langzeitarchivierung tätig sind.

Jeder Projektgruppe steht dabei ein Mitglied des Redaktionsteams von Edumeres.net beratend zur Seite. Diese begleitende Projektarbeit dient insbesondere der Etablierung der virtuellen Arbeits- und Forschungsumgebung in den beteiligten Disziplinen. Die aus der Zusammenarbeit von Redaktion und Nutzern gewonnenen Erkenntnisse fließen direkt in die fortlaufende Weiterentwicklung ein.

An Hand von Fallbeispielen aus dem Fachkontext der Bildungsmedienforschung soll dieser Prozess kollaborativer Wissensgenerierung konkret dar- und der bisherigen Praxis gegenübergestellt werden.},
	pages = {67--71},
	booktitle = {Digitale Wissenschaft. Stand und Entwicklung digital vernetzter Forschung in Deutschland [Tagung, 20./21. September 2010, Köln]},
	publisher = {hbz},
	author = {Brink, Sylvia and Frey, Christian and Fuchs, Andreas L. and Henry, Roderich and Reiß, Kathleen and Strötgen, Robert},
	editor = {Schomburg, Silke and Puschmann, Cornelius and Lobin, Henning},
	date = {2011},
	langid = {german},
	note = {www.scivee.tv/assets/files/25099/03\_afuchs\_köln\_edumeres.pdf
http://www.scivee.tv/node/25099},
	keywords = {X-{CHECK}, meta\_Assessing, obj\_VREs},
}

@article{kraus_when_2011,
	title = {When Data Disappears},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2011/08/07/opinion/sunday/when-data-disappears.html?_r=3&ref=opinion},
	journaltitle = {The New York Times},
	author = {Kraus, Kari},
	urldate = {2011-08-07},
	date = {2011-08-06},
	langid = {english},
	keywords = {act\_Archiving},
}

@online{underwood_where_2012,
	title = {Where to start with text mining.},
	url = {http://tedunderwood.wordpress.com/2012/08/14/where-to-start-with-text-mining/},
	abstract = {This post is an outline of discussion topics I’m proposing for a workshop at {NASSR}2012 (a conference of Romanticists). I’m putting it on the blog since some of the links might be useful for a broader audience.

In the morning I’ll give a few examples of concrete literary results produced by text mining. I’ll start the afternoon workshop by opening two questions for discussion: first, what are the obstacles confronting a literary scholar who might want to experiment with quantitative methods? Second, how do those methods actually work, and what are their limits?

I’ll also invite participants to play around with a collection of 818 works between 1780 and 1859, using an R program I’ve provided for the occasion. Links for these materials are at the end of this post},
	titleaddon = {The Stone and the Shell},
	author = {Underwood, Ted},
	urldate = {2012-08-21},
	date = {2012-08},
	langid = {english},
	keywords = {bigdata},
}

@inproceedings{zimmerman_whither_2006,
	location = {Montréal, Québec, Canada},
	title = {Whither or whether {HCI}: requirements analysis for multi-sited, multi-user cyberinfrastructures},
	isbn = {1-59593-298-4},
	url = {http://portal.acm.org/citation.cfm?id=1125451.1125743},
	doi = {10.1145/1125451.1125743},
	shorttitle = {Whither or whether {HCI}},
	abstract = {Cyberinfrastructures bring together distributed resources to support scientific discoveries. Cyberinfrastructures currently under development are intended to enable the cooperative work of diverse users over long periods of time. We analyze the challenges that cyberinfrastructures present to existing methods of user requirements analysis.},
	pages = {1601--1606},
	publisher = {{ACM}},
	author = {Zimmerman, Ann and Nardi, Bonnie A.},
	urldate = {2009-05-18},
	date = {2006},
	langid = {english},
	keywords = {goal\_Collaboration, meta\_Theorizing, obj\_DigitalHumanities, obj\_Infrastructures},
}

@article{fanta_was_2005,
	title = {Was ist Computerphilologie?},
	url = {http://www.ceeol.com/aspx/issuedetails.aspx?issueid=d63d8499-7a79-4561-847a-1962232cb170&articleId=975fa458-754e-4276-81ac-039f3e78d9b7},
	abstract = {Computerphilologie ist ein noch unabgestecktes Terrain der Germanistik. Der Beitrag geht auf die Zuständigkeiten, Instrumente und Frageweisen ein. Er verdeutlicht, dass es nicht bloß um den praktischen Einsatz einer Technologie geht, sondern um grundsätzliche Implikationen im Prozess eines Medienwechsels vom Druck zur Digitalität. Literarität wird auf sämtlichen Ebenen neu konfiguriert. Der Schwerpunkt der Betrachtung liegt am digitalen Edieren, da die Digitalisierung von historischen Quellen und ihr Transfer in elektronischen Hypertext nicht nur das Kerngebiet der Computerphilologie darstellt, sondern auch den Bereich, in dem die Wirkung der Technologie auf die Geschichte zurückgreift und sie einer Metamorphose unterzieht.},
	pages = {339--346},
	journaltitle = {Germanistik im Kontakt. Tagung Österreichischer und Kroatischer Germanist/inn/en, Opatija, 29.9},
	author = {Fanta, Walter},
	date = {2005},
	langid = {german},
	note = {Fanta, Walter: Was ist Computerphilologie? In: Svjetlan Lacko Viduli�c (Hg.): Germanistik im Kontakt. Tagung Österreichischer und Kroatischer Germanist/inn/en, Opatija, 29.9. - 1.10.2005. Zagreb: Univ. Zagreb 2006, S. 339-346.},
	keywords = {goal\_Analysis, goal\_Enrichment, meta\_GiveOverview, obj\_DigitalHumanities},
}

@online{schneider_why_2013,
	title = {Why Digital Humanities?},
	url = {http://www.politicseastasia.com/research/digital-nationalism/digital-humanities/},
	abstract = {What does the "digital turn" mean for scholarship and teaching in the arts and humanities? Find out in this discussion of the digital humanities.},
	titleaddon = {{PoliticsEastAsia}.com},
	author = {Schneider, Florian},
	urldate = {2013-11-05},
	date = {2013},
	langid = {english},
}

@article{riepl_wie_1999,
	title = {Wie wird Literatur berechenbar? Ein Modell zur rechnergestützten Analyse althebräischer Texte},
	volume = {1},
	url = {http://computerphilologie.uni-muenchen.de/jahrbuch/jb1/riepl.html},
	abstract = {Der Einsatz des Rechners in geisteswissenschaftlichen Disziplinen erweist sich im Bereich klassischer und altorientalischer Philologie auf dem Gebiet der rechnergestützten Analyse alter Sprachen, ferner davon ausgehend der rechnergestützten Analyse literarischer Werke als äußerst forschungsintensiv, sowohl für Philologen als auch für Informatiker. Sind einmal Textdaten im Rechner verfügbar und liegt ein anhand eines repräsentativen Textkorpus gewonnenes Regelwerk vor, wecken Problemstellungen, die sich aus den anfallenden großen Datenmengen und den darauf anzuwendenden komplexen Regeln ergeben, das Forschungsinteresse der Informatiker mit den Schwerpunkten ›Logikprogrammierung‹, ›Deduktive Datenbanken‹ und ›Informationssysteme‹. Wie fruchtbar die Zusammenarbeit von Philologen und Informatikern sein kann, zeigt das Projekt »Biblia Hebraica transcripta«.

1986 haben Wolfgang Richter und seine Mitarbeiter am Lehrstuhl für ugaritische und hebräische Sprach- und Literaturwissenschaft des Instituts für Assyriologie und Hethitologie der Ludwig-Maximilians-Universität München begonnen, den Text der Handschrift B 19A der Öffentlichen Bibliothek von St. Petersburg nach der Edition der Biblia Hebraica Stuttgartensia objektsprachlich zu transkribieren und in den Rechner einzutippen. Die Eingabe des gesamten alttestamentlichen Textkorpus ist seit 1990 abgeschlossen. Eine erste Auflage der Texte ist veröffentlicht. Der Publikation in Buchform sind als elektronische Ausgaben mittlerweile überarbeitete Auflagen gefolgt.

Bereits während der Texteingabe sind in Zusammenarbeit mit dem Lehrstuhl Prof. Rudolf Bayer Ph.D., Forschungs- und Lehreinheit Informatik {III} der Fakultät für Informatik der Technischen Universität München Programme zur Analyse der althebräischen Sprache entwickelt worden: »Salomo« zur Analyse der Morphologie und »Amos« zur Analyse der Morphosyntax. Beide Expertensysteme sind erfolgreich auf das gesamte Alte Testament angewendet worden; die Arbeit mit Salomo ist 1993, die Arbeit mit Amos 1995 abgeschlossen worden. Nachdem auch die Analyse der Eigennamen des Alten Testaments erfolgt ist, konzentriert sich die Forschung auf folgende Schwerpunkte: Korrektur der morphologischen und morphosyntaktischen Daten, Entwurf eines Systems zur semantischen und satzsyntaktischen Analyse.

Alle Analyseergebnisse werden von der Datenbank »Biblia Hebraica transcripta {DatenBank}« verwaltet, deren Konzept und Schema seit 1993 von Christian Riepl erstellt werden. Zur Verwaltung der Ergebnisse der morphosyntaktischen und syntaktischen Analyse ist von Hans Argenton am Wilhelm-Schickard-Institut für Informatik, Arbeitsbereich Datenbanken, Prof. Dr. Ulrich Güntzer, das Informationssystem »Venona« entwickelt worden. Das multimediale Datenbanksystem {MultiBHT}, 1995 vorgestellt von Günther Specht wird derzeit im Rahmen eines vom {DFN} geförderten Projekts um einen {WWW}-Zugang erweitert. Es vermag sämtliche Daten der Editionen, Transkriptionen, Versionen sowie die Ergebnisse der sprachlichen Analyse zu integrieren und bildet erstmals die Grundlage für ein vollständiges Lexikon samt Konkordanz und Grammatik der althebräischen Sprache. Dem Literaturwissenschaftler wird die Möglichkeit geboten, auf die grammatisch, semantisch und lexikalisch erschlossenen Daten eines Textes zuzugreifen, um diese unter literaturwissenschaftlichen Gesichtspunkten auszuwerten und die Ergebnisse der literaturwissenschaftlichen Analyse in die Wissensbasis einzubeziehen. Die rechnergestützte Verfahrensweise ist eingebunden in eine literatur- und sprachwissenschaftlich begründete Methodik, besitzt Modellcharakter und ist übertragbar auf andere Sprachen und Literaturen.},
	pages = {107--134},
	journaltitle = {Jahrbuch für Computerphilologie},
	author = {Riepl, Christian},
	date = {1999},
	langid = {german},
	note = {Riepl, Christian: Wie wird Literatur berechenbar? Ein Modell zur rechnergestützten Analyse althebräischer Texte. In: Jahrbuch für Computerphilologie 1, S. 107-134.},
	keywords = {{AnalyzeStatistically}, obj\_Literature},
}

@online{schmidt_words_2013,
	title = {Words Alone: Dismantling Topic Models in the Humanities},
	url = {http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/},
	shorttitle = {Words Alone},
	titleaddon = {Journal of Digital Humanities},
	author = {Schmidt, Benjamin M.},
	urldate = {2013-04-15},
	date = {2013-04-05},
	langid = {english},
	keywords = {bigdata},
}

@article{eisenberg_words_2012,
	title = {Words by the Millions, Sorted by Software},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2012/03/25/business/words-by-the-millions-sorted-by-software.html},
	abstract = {{IT} just keeps growing — the vast electronic archive of books, journals and scholarly literature stored on the Web. But scientists are aiming to keep up with this trove of collective knowledge by devising computer-based tools to winnow and quantify it.},
	journaltitle = {The New York Times},
	author = {Eisenberg, Anne},
	urldate = {2012-07-11},
	date = {2012-03-24},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@article{agar_what_2006,
	title = {What Difference Did Computers Make?},
	volume = {36},
	url = {http://sss.sagepub.com/content/36/6/869.abstract},
	doi = {10.1177/0306312706073450},
	abstract = {This paper asks the question: what difference did access to computers make to the
                first generation of scientists to use them? While we do know something
                about the use of computers in particular scientific specialities, a comparative
                perspective across disciplines is revealing. So this paper casts the net wider, not
                only revisiting microphysics and X-ray crystallography, but also examining natural
                history and the implicit social science of government administration. It focuses on
                the period when computers were first introduced, since the novelty of the techniques
                caused scientists to reflect on the changes. This has the advantage, too, of
                bringing to light the important relationship between routinization of scientific
                work prior to computerization and computerization itself.},
	pages = {869 --907},
	number = {6},
	journaltitle = {Social Studies of Science},
	author = {Agar, Jon},
	urldate = {2011-11-13},
	date = {2006-12-01},
	langid = {english},
	keywords = {meta\_Assessing, meta\_GiveOverview, obj\_DigitalHumanities},
}

@book{burnard_what_2014,
	location = {Marseille},
	title = {What is the Text Encoding Initiative? How to add intelligent markup to digital resources},
	isbn = {9782821834606},
	url = {http://books.openedition.org/oep/426},
	series = {Encyclopédie numérique},
	abstract = {The Text Encoding Initiative ({TEI}) Guidelines have long been regarded as the de facto standard for the preparation of digital textual resources in the scholarly research community. For the beginner, they offer a daunting range of possibilities, reflecting the huge range of potential applications for text encoding, from traditional scholarly editions, to language corpora, historical lexicons, digital archives and beyond.

Drawing on many examples of {TEI}-encoded text from a variety of research domains, this simple and straightforward book is intended to help the beginner make their own choices from the full range of {TEI} options.

It explains the {XML} technology used by the {TEI} in language accessible to the non-technical reader and provides a guided tour of the many parts of the {TEI} universe, and how it may be customized to suit an individual project’s needs.},
	publisher = {{OpenEdition} Press},
	author = {Burnard, Lou},
	date = {2014},
	langid = {english},
	keywords = {*****, act\_Annotating, act\_Editing, goal\_Enrichment, obj\_Text},
}
@online{oreilly_what_2005,
	title = {What Is Web 2.0?},
	url = {http://oreilly.com/web2/archive/what-is-web-20.html},
	titleaddon = {O'Reilly},
	author = {O'Reilly, Tim},
	date = {2005},
	note = {Der Text ist auch in anderen Sprachen verfügbar: {AR}, {ZH}, {FR}, {DE}, {IT}, {JA}, {KO}, {ES}},
	keywords = {*****, goal\_Collaboration, obj\_Web},
}

@report{european_commission_wissenschaftliche_2012,
	location = {Brüssel},
	title = {Wissenschaftliche Daten: freier Zugang zu Forschungsergebnissen wird Innovationskapazität der {EU} stärken [Press Release]},
	url = {http://europa.eu/rapid/press-release_IP-12-790_de.htm},
	institution = {European Commission},
	author = {{European Commission}},
	date = {2012-07},
	langid = {german},
	note = {Der Text ist auch in anderen Sprachen verfügbar: {EN} {FR} {DA} {ES} {NL} {IT} {SV} {PT} {FI} {EL} {CS} {ET} {HU} {LT} {LV} {MT} {PL} {SK} {SL} {BG} {RO}},
	keywords = {goal\_Dissemination, obj\_Data},
}

@article{robinson_what_2009,
	title = {What Scholarly Editors Need to Help us Make Sense Together in the Digital Age},
	volume = {1},
	url = {https://letterpress.uchicago.edu/index.php/jdhcs/article/view/64},
	abstract = {This paper will argue that the single greatest effect of the digital revolution on scholarship is not that it is giving us near-instant access to resources through multiple digital libraries, or that it offers multiple new publication possibilities, or that it supplies many new tools – databases, analytic programs, and more. Rather, the single greatest effect of the digital revolution is that it is is empowering a new model of collaboration, and hence new modes of readership and study, among scholars, and between scholars and readers. This is particularly true of scholarly editing.},
	number = {1},
	journaltitle = {Journal of the Chicago Colloquium on Digital Humanities and Computer Science},
	shortjournal = {{JDHCS}},
	author = {Robinson, Peter M.},
	date = {2009-07-17},
	langid = {english},
	keywords = {goal\_Collaboration, obj\_Infrastructures, obj\_Metadata},
}

@report{grindley_whats_2006,
	title = {What's in the Art Historian's Toolkit? A Methods Network Working Paper},
	url = {http://www.methodsnetwork.ac.uk/resources/wkp01.html},
	abstract = {It is clear that there is a widespread and majority reluctance across most areas of the arts and humanities to engage with any technology that might be considered more advanced or applied than the ‘usual’ desktop productivity tools; a view reflected by a recent report from the proceedings of the Summit on Digital Tools for the Humanities held at the University of Virginia in 2005. Whilst it is important therefore not to brand the art history scholarly community as being especially benighted in its dealings with technology, it should also be noted that they deal with a particularly rich, diverse and highly visual body of material as the focus of their research and analysis. Presently, the technological response to the complex questions that arise from the study of that material is - for the most part - fragmentary, inadequate and poorly coordinated.

To address this problem, art historians may benefit from looking outside of their discipline to learn from colleagues pursuing technology-led solutions in other fields. The Methods Network has a remit to support and fund this kind of interdisciplinary forum and has done so with a series of events and activities that has brought together researchers from diverse fields with the aim of defining shared challenges and collaborative solutions. The tools and methods referred to in this paper are a distillation of some of the techniques that have been presented or referred to in the course of that programme and the intention is to consider how they might be applicable, relevant and useful to art historians carrying out research.},
	institution = {{AHRC} {ICT} Methods Network},
	author = {Grindley, N.},
	date = {2006},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_Images, obj\_Methods},
}

@online{gold_whose_2012,
	title = {Whose Revolution? Towards a More Equitable Digital Humanities},
	url = {http://blog.mkgold.net/2012/01/10/whose-revolution-toward-a-more-equitable-digital-humanities/},
	titleaddon = {The Lapland Chronicles},
	author = {Gold, Matt},
	date = {2012-01-10},
	langid = {english},
	keywords = {meta\_Advocating, meta\_Assessing, obj\_DigitalHumanities},
}

@article{holmes_who_2003,
	title = {Who Was the Author? An Introduction to Stylometry},
	volume = {16},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09332480.2003.10554842#.VGDG3PmUckw},
	pages = {5--8},
	number = {2},
	journaltitle = {Chance},
	author = {Holmes, David I. and Kardos, Judit},
	date = {2003},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, meta\_GiveOverview, t\_Stylometry},
}

@article{burrows_who_2005,
	title = {Who Wrote Shamela? Verifying the Authorship of a Parodic Text},
	url = {http://llc.oxfordjournals.org/content/20/4/437.short},
	abstract = {Imitative texts of high quality are of some importance to students of attribution, especially those who use computational methods. The authorship of such texts is always likely to be difficult to demonstrate. In some cases, the identity of the author is a question of interest to literary scholars. Even when that is not so, students of attribution face a challenge. If we cannot distinguish between original and imitation in such cases, we must always concede that an imitator may have been at work. Shamela (1741) has always been regarded as a brilliant parody. When it is subjected to our standard common-words tests of authorship, it yields mixed results. A new procedure, in which special word-lists are established according to a predetermined set of rules, proves more effective. It needs, however, to be tried in other cases.},
	pages = {437--450},
	number = {20},
	journaltitle = {Literary and Linguistic Computing},
	author = {Burrows, John F.},
	date = {2005},
	langid = {english},
	keywords = {bigdata, t\_Stylometry},
}

@article{binongo_who_2003,
	title = {Who Wrote the 15th Book of Oz? An Application of Multivariate Analysis to Authorship Attribution},
	volume = {16},
	url = {http://www.ssc.wisc.edu/~zzeng/soc357/OZ.pdf},
	shorttitle = {Who Wrote the 15th Book of Oz?},
	pages = {9--17},
	number = {2},
	journaltitle = {Chance},
	author = {Binongo, José Nilo G},
	date = {2003},
	langid = {english},
	keywords = {bigdata, t\_Stylometry},
}

@article{altwegg_wie_2009,
	location = {Frankfurt},
	title = {Wie die Ehebrecherin Emma ins Netz geht},
	url = {http://www.genios.de/presse-archiv/artikel/FAZ/20090509/wie-die-ehebrecherin-emma-ins-netz-/FD1200905092245437.html},
	pages = {Z2},
	journaltitle = {Frankfurter Allgemeine Zeitung ({FAZ})},
	author = {Altwegg, Jürg},
	date = {2009-05-09},
	langid = {german},
	note = {Der Text ist nicht kostenlos. Man muss 4.17 € bezahlen, um den kompletten Artikel freizuschalten.},
	keywords = {act\_Transcription, goal\_Collaboration, goal\_Enrichment, t\_Encoding},
}

@report{dfg-ausschuss_fur_wissenschaftliche_bibliotheken_und_informationssysteme_wissenschaftliche_2006,
	location = {Bonn},
	title = {Wissenschaftliche Literaturversorgungs- und Informationssysteme. Schwerpunkte der Förderung bis 2015. Ein Positionspapier},
	url = {http://dfg.de/download/pdf/foerderung/programme/lis/positionspapier.pdf},
	institution = {{DFG}},
	author = {{DFG-Ausschuss für Wissenschaftliche Bibliotheken und Informationssysteme}},
	date = {2006-06},
	langid = {german},
	keywords = {act\_Conceptualizing, meta\_Advocating, meta\_DefinePolicy, obj\_Infrastructures, obj\_VREs},
}

@article{kammer_wordcruncher._1989,
	title = {{WordCruncher}. Problems of Multilingual Usage},
	volume = {4},
	url = {http://llc.oxfordjournals.org/content/4/2/135.short},
	abstract = {Word Cruncher is a full-text-retrieval system especially developed for literary textual corpora and optimized for this purpose There are two main presentation forms of occurrences. onscreen presentation of short contexts and short indexes. The program has been well known for a number of years and has been described several times, so the goal of this article cannot be to describe all the features and details The users manual gives all the necessary information on programs, features and options.

In this article some problems concerning the usage of {WordCruncher} with multilingual texts will be discussed Choosing texts not written in English will result in some special problems of intercultural transfer of software being indicated These problems will increase when trying to produce a lemmatized index. Keeping this special point of view in mind we will discuss some of the program options and restrictions and show how it is possible to solve the remaining problems by using a template.},
	pages = {135--140},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Kammer, Manfred},
	date = {1989},
	langid = {english},
	note = {Kammer, Manfred: {WordCruncher}. Problems of Multilingual Usage. In: Literary and Linguistic Computing 4 (1989), S. 135-140.},
	keywords = {{AnalyzeStatistically}, meta\_Theorizing, obj\_Tools},
}

@report{research_information_network_research_2010,
	location = {London},
	title = {Research Support Services in {UK} Universities. A Research Information Network report},
	url = {https://www.soas.ac.uk/careers/earlycareerresearchers/file69090.pdf},
	abstract = {The search for improvements in research performance 
is a powerful influence on all universities. Success 
in research is a major component in the various 
indicators of overall university performance. Hence 
universities are increasingly interested in how they 
can improve their competitive position in attracting, 
supporting and promoting the work of high-quality 
researchers.},
	pages = {22},
	institution = {Research Information Network},
	author = {{Research Information Network}},
	date = {2010},
	langid = {english},
	keywords = {obj\_Research},
}

@report{moulin_research_2011,
	title = {Research Infrastructures in the Digital Humanities},
	url = {http://www.esf.org/hosting-experts/scientific-review-groups/humanities-hum/strategic-activities/research-infrastructures-in-the-humanities.html},
	abstract = {The {ESF} Standing Committee for the Humanities ({HUM} (formerly {SCH})) identified the topic of Research Infrastructures ({RIs}) as an area of strategic priority for the humanities in 2009. An {HUM} (formerly {SCH}) expert group on {RIs} was set up the same year and mandated to propose a proactive strategy for {RIs} in the humanities at the European level on behalf of {HUM} (formerly {SCH}). In the past few years the group, together with the support of the {ESF} office, has been active both at the policy and research levels.},
	institution = {European Science Foundation ({ESF})},
	author = {Moulin, Claudine},
	date = {2011-09-01},
	langid = {english},
	keywords = {meta\_Advocating, meta\_Assessing, meta\_GiveOverview, obj\_Infrastructures},
}

@article{eisen_research_2012,
	title = {Research Bought, Then Paid For},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2012/01/11/opinion/research-bought-then-paid-for.html?_r=2&ref=opinion},
	abstract = {Through the National Institutes of Health, American taxpayers have long supported research directed at understanding and treating human disease. Since 2009, the results of that research have been available free of charge on the National Library of Medicine’s Web site, allowing the public (patients and physicians, students and teachers) to read about the discoveries their tax dollars paid for.},
	journaltitle = {The New York Times},
	author = {Eisen, Michael B.},
	urldate = {2012-01-11},
	date = {2012-01-10},
	langid = {english},
}

@article{miall_representing_1995,
	title = {Representing and Interpreting Literature by Computer},
	volume = {25},
	rights = {Copyright © 1995 Modern Humanities Research Association},
	issn = {0306-2473},
	url = {http://www.jstor.org/stable/3508827},
	doi = {10.2307/3508827},
	abstract = {It is clear that the advent of computers has so far had almost no impact on the mainstream activities of producing, reading, or studying literary texts. This may be about to change. The prophecy that computing will transform the nature of literary studies is certainly one that we have heard before, but the widespread use of powerful personal computers in the last few years and the increasing role played by the internet, now makes such a forecast seem to carry more weight. Advocates of these technologies have recently begun to put a new and powerful argument: computer technology for modelling, representing, or creating texts is emerging that will allow us to bring these processes a major step nearer to the activities of actual readers; this in turn will revolutionize understanding of the nature of textuality itself. If this is true, the forthcoming shift in the domain of the literary will be on a tectonic scale, analogous to that brought about in the visual arts by the invention of photography and film.},
	pages = {199--212},
	journaltitle = {The Yearbook of English Studies},
	author = {Miall, David D.},
	urldate = {2011-04-26},
	date = {1995-01-01},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, act\_Visualizing, goal\_Interpretation, meta\_GiveOverview, obj\_Literature, t\_Encoding},
}

@article{stiegler_relational_2012,
	title = {Relational Ecology and the Digital Pharmakon},
	volume = {13},
	url = {http://www.culturemachine.net/index.php/cm/article/view/464/501},
	abstract = {Alphabetical vocalic writing, which appeared between the 8th and 7th Century B.C., allowed the constitution of a singular attentional process which is the very basis of ancient Greek civilisation. They called it the logos. At the same time, an equally alphabetical, but consonant-based form of writing allowed the construction of the kingdom of Judea. When the two civilisations will meet through Paul of Tarsus, the West will be formed – and ceaselessly reformed, deformed and transformed as the process of psychic and collective individuation based on writing as the technique of the formation of attention. This includes what are known as the Scriptures, which will come into their own with the printing press, inaugurating the attentional revolution which was the Reformation.
In this way the elements of what Katherine Hayles has called ‘deep attention’ came together – an attentional form allowing its own replacement by another form that she calls ‘hyper-attention’ produced by the digital technologies of attention capture (Hayles, 2007).
If we want to analyse and understand the stakes of this transformation (insofar as this is possible), we must analyse what, as process of ‘grammatisation’, leads us from the appearance of the writing of grammata up to the digital apparatuses and the new attentional forms that they constitute. For these inaugurate a new process of psychic and collective individuation that emerges at the heart of what must be understood as a network society of planetary proportions.},
	pages = {11},
	issue = {Paying Attention},
	author = {Stiegler, Bernard},
	date = {2012},
	langid = {english},
	keywords = {meta\_Collaborating, meta\_Teaching/Learning, meta\_Theorizing, obj\_AnyObject, obj\_DigitalHumanities},
}

@online{canet_reflexiones_2014,
	title = {Reflexiones sobre las humanidades digitales},
	rights = {http://creativecommons.org/licenses/by/3.0/es/},
	url = {http://www.janusdigital.es/anexos/contribucion.htm;jsessionid=4F686E22E5A31F197F803425C7932E9A?id=4},
	abstract = {Revista Digital {JANUS} - Estudios sobre el Siglo de Oro.
A partir de mi experiencia personal a lo largo de 30 años en edición digital de revistas y libros electrónicos; creación, preservación y gestión de bibliotecas y archivos digitales; investigación basada en ordenadores y aplicaciones informáticas para estudios literarios, lingüísticos, culturales e históricos; análisis de textos, corpus, marcación de corpus, bases de datos, etc., llego a la conclusión de que las Humanidades digitales han avanzado mediante el autoaprendizaje y también han sido posibles grandes proyectos a través del trabajo en equipo y la interdisciplinariedad, junto con el aprendizaje continuo. Lo que me lleva a reflexionar si debería haber estudios específicos en Humanidades digitales y, si así fuera, cuáles deberían ser. Hago un breve repaso a diferentes másteres existentes en la actualidad en {EE}.{UU}., Inglaterra y España, resaltando el alto contenido en {TIC}, y finalmente doy mi propuesta sobre las posibles alternativas futuras de las Humanidades digitales.},
	type = {text},
	author = {Canet, José Luis},
	urldate = {2014-05-04},
	date = {2014},
	langid = {spanish},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{rehbein_reconstructing_2009,
	title = {Reconstructing the textual evolution of a medieval manuscript},
	volume = {24},
	url = {http://llc.oxfordjournals.org/content/24/3/319.abstract},
	doi = {10.1093/llc/fqp020},
	abstract = {This article presents the results of the work on kundige bok, one of Göttingen's town records, containing late medieval town law. Due to the fact that this law was frequently subject to change, the text itself was revised over and over again, giving evidence for its frequent use and its dynamic nature. What has come to us, is, thus, a multi-layered text in which all layers represent a different (e.g. chronological) stage of the town law. Consequently they have to be regarded, processed and represented equally. A dynamic text like this requires a dynamic representation. The article shows how an electronic scholarly edition of a multi-layered text can be created and used, first, to reconstruct the genesis of the text; second, to make this evolution understandable, processable and visible; and third, with the text as a witness to display the development of urban law and urban life in the Late Middle Ages.This article: outlines the challenge of editing a multi-layered medieval manuscript;discusses why this leads to a new understanding of a critical edition of such a text; and introduces the techniques used to create the electronic edition of kundige bok, in particular highlighting the linkage between the two dimensions of ‘text’and ‘time’ based on the {TEI} P5 scheme.},
	pages = {319 --327},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	author = {Rehbein, Malte},
	urldate = {2011-04-26},
	date = {2009},
	langid = {english},
	keywords = {act\_RelationalAnalysis, obj\_Manuscripts, obj\_Variants, t\_Encoding},
}

@book{bode_reading_2012,
	location = {New York},
	title = {Reading by numbers: recalibrating the literary field},
	isbn = {9780857284549},
	url = {http://www.anthempress.com/reading-by-numbers},
	shorttitle = {Reading by numbers},
	abstract = {‘Reading by Numbers: Recalibrating the Literary Field’ proposes and demonstrates a new digital approach to literary history. Drawing on bibliographical information on the Australian novel in the {AustLit} database, the book addresses debates and issues in literary studies through a method that combines book history’s pragmatic approach to literary data with the digital humanities’ idea of computer modelling as an experimental and iterative practice. As well as showcasing this method, the case studies in ‘Reading by Numbers’ provide a revised history of the Australian novel, focusing on the nineteenth century and the decades since the end of the Second World War, and engaging with a range of themes including literary and cultural value, authorship, gender, genre and the transnational circulation of fiction. The book’s findings challenge established arguments in Australian literary studies, book history, feminism and gender studies, while presenting innovative ways of understanding literature, publishing, authorship and reading, and the relationships between them. More broadly, by demonstrating critical ways in which the growing number of digital archives in the humanities can be mined, modelled and visualised, ‘Reading by Numbers’ offers new directions and scope for digital humanities research.},
	publisher = {Anthem Press},
	author = {Bode, Katherine},
	date = {2012},
	langid = {english},
	keywords = {{AnalyzeStatistically}, X-{CHECK}, bigdata, goal\_Analysis},
}

@article{pfanner_quietly_2011,
	title = {Quietly, Google Puts History Online},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2011/11/21/technology/quietly-google-puts-history-online.html},
	abstract = {When the Israel Museum in Jerusalem, home to the Dead Sea Scrolls, reopened last year after an extensive renovation, it attracted a million visitors in the first 12 months. When the museum opened an enhanced Web site with newly digitized versions of the scrolls in September, it drew a million virtual visitors in three and a half days.

The scrolls, scanned with ultrahigh-resolution imaging technology, have been viewed on the Web from 210 countries — including some, like Afghanistan, Iran, Iraq and Syria, that provide few real-world visitors to the Israel Museum.

“This is taking the material to an amazing range of audiences,” said James S. Snyder, the museum’s director. “There’s no way we would have had the technical capability to do this on our own.”},
	journaltitle = {The New York Times},
	author = {Pfanner, Eric},
	urldate = {2011-11-23},
	date = {2011-11-20},
	langid = {english},
	keywords = {act\_Publishing, goal\_Capture, meta\_GiveOverview, obj\_Artefacts, obj\_Manuscripts},
}

@article{parry_quantitative_2013,
	title = {Quantitative History Makes a Comeback. Historians Argue for a Scientific Study of the Past},
	url = {http://chronicle.com/article/Quantifying-the-Past/137419/},
	abstract = {Numbers illuminate the shape of history.

That's a core belief of the Stanford archaeologist and historian Ian Morris, and his book Why the West Rules—for Now puts it into practice by quantifying the development of different societies.

But he's hardly the only one. A growing crop of scholars share Morris's science-steeped, big-picture approach to the past. And while his book relies on simple mathematics, others go further. They aspire to capture history in equations. And they want to use math to predict how the future might play out, given different assumptions about factors like population growth or climate change.},
	journaltitle = {The Chronicle of Higher Education},
	author = {Parry, Marc},
	date = {2013-02-25},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@incollection{hoover_quantitative_2008,
	location = {Oxford},
	edition = {Online},
	title = {Quantitative Analysis and Literary Studies},
	isbn = {1405103213},
	url = {http://www.digitalhumanities.org/companion/view?docId=blackwell/9781405148641/9781405148641.xml&chunk.id=ss1-6-9&toc.depth=1&toc.id=ss1-6-9&brand=9781405148641_brand},
	series = {Blackwell Companions to Literature and Culture},
	abstract = {Modern quantitative studies of literature begin about 1850, with periods of intense activity in the 1930s and the 1980s. Fortunately, several excellent overviews discuss earlier work in the context of computers and literary studies (Burrows 1992a), stylometry (Holmes 1998), and authorship attribution (Holmes 1994; Love 2002). We can thus concentrate here on recent advances, driven primarily by the huge growth in the availability of electronic texts, increasingly sophisticated statistical techniques, and the advent of much more powerful computers that have produced much more accurate and persuasive analyses.},
	pages = {517--533},
	booktitle = {A Companion to Digital Literary Studies},
	publisher = {Blackwell},
	author = {Hoover, David L.},
	date = {2008},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview},
}

@article{harvie_publisher_2013,
	title = {Publisher, be damned! From price gouging to the open road},
	volume = {31},
	issn = {0810-9028},
	url = {http://dx.doi.org/10.1080/08109028.2014.891710},
	doi = {10.1080/08109028.2014.891710},
	abstract = {All four authors are members of the Leicester school of critical management and have previously written together on academic publishing. David Harvie lectures in finance and is interested in ethical issues related to this and other matters. He is a member of The Free Association writing collective. Geoff Lightfoot lectures in entrepreneurship and has particular interests in the ideology of markets and critical accounting. Simon Lilley works on information aspects of organisation and is currently head of the School of Management at Leicester University. Kenneth Weir is interested in accounting practices, especially critical and social accounting.},
	pages = {229--239},
	number = {3},
	journaltitle = {Prometheus},
	author = {Harvie, David and Lightfoot, Geoff and Lilley, Simon and Weir, Kenneth},
	urldate = {2014-06-19},
	date = {2013},
	langid = {english},
	keywords = {act\_Publishing, goal\_Dissemination, obj\_ResearchResults},
}

@article{balazs_published_2009,
	title = {Published Yet Never Done: The Tension Between Projection and Completion in Digital Humanities Research},
	volume = {3},
	url = {http://digitalhumanities.org/dhq/vol/3/2/000040/000040.html},
	abstract = {The case of the Orlando Project offers a useful interrogation of concepts like completion and finality, as they emerge in the arena of electronic publication. The idea of "doneness" circulates discursively within a complex and evolving scholarly ecology where new modes of digital publication are changing our conceptions of textuality, at the same time that models of publication, funding, and archiving are rapidly changing. Within this ecology, it is instrumental and indeed valuable to consider particular tasks and stages done, even as the capacities of digital media push against a sense of finality. However, careful interrogation of aims and ends is required to think through the relation of a digital project to completion, whether modular, provisional, or of the project as a whole.},
	number = {2},
	journaltitle = {Digital Humanities Quarterly},
	author = {Balazs, Sharon and Brown, Susan and Clements, Patricia and Grundy, Isobel and Ruecker, Stan and Antoniuk, Jeffery},
	date = {2009},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, obj\_Project},
}

@article{fujimoto_proposed_1975,
	title = {Proposed mechanisms of stimulation and inhibition of guanylate cyclase with reference to the actions of chlorpromazine, phosphoipases and Triton X-100},
	volume = {67},
	issn = {0006-291X},
	url = {http://www.sciencedirect.com/science/article/pii/0006291X75901734},
	abstract = {In the analysis of bibliometric networks, researchers often use mapping and clustering techniques in a combined fashion. Typically, however, mapping and clustering techniques that are used together rely on very different ideas and assumptions. We propose a unified approach to mapping and clustering of bibliometric networks. We show that the {VOS} mapping technique and a weighted and parameterized variant of modularity-based clustering can both be derived from the same underlying principle. We illustrate our proposed approach by producing a combined mapping and clustering of the most frequently cited publications that appeared in the field of information science in the period 1999-2008.},
	pages = {1332--1336},
	number = {4},
	journaltitle = {Biochemical and biophysical research communications},
	shortjournal = {Biochem. Biophys. Res. Commun.},
	author = {Fujimoto, M and Okabayashi, T},
	date = {1975-12-15},
	langid = {english},
}
@incollection{steyvers_probabilistic_2006,
	title = {Probabilistic Topic Models},
	url = {http://books.google.de/books?hl=de&lr=&id=JbzCzPvzpmQC&oi=fnd&pg=PA427&dq=Probabilistic+Topic+Models&ots=aMG3M0R_IK&sig=DpWCn5nSwZdnO-NYvZts27g9wu0#v=onepage&q=Probabilistic%20Topic%20Models&f=false},
	abstract = {The Handbook of Latent Semantic Analysis is the authoritative reference for the theory behind Latent Semantic Analysis ({LSA}), a burgeoning mathematical method used to analyze how words make meaning, with the desired outcome to program machines to understand human commands via natural language rather than strict programming protocols. The first book of its kind to deliver such a comprehensive analysis, this volume explores every area of the method and combines theoretical implications as well as practical matters of {LSA}.Readers are introduced to a powerful new way of understanding language phenomena, as well as innovative ways to perform tasks that depend on language or other complex systems. The Handbook clarifies misunderstandings and pre-formed objections to {LSA}, and provides examples of exciting new educational technologies made possible by {LSA} and similar techniques. It raises issues in philosophy, artificial intelligence, and linguistics, while describing how {LSA} has underwritten a range of educational technologies and information systems. Alternate approaches to language understanding are addressed and compared to {LSA}. This work is essential reading for anyone—newcomers to this area and experts alike—interested in how human language works or interested in computational analysis and uses of text. Educational technologists, cognitive scientists, philosophers, and information technologists in particular will consider this volume especially useful.},
	booktitle = {Latent Semantic Analysis: A Road to Meaning},
	publisher = {Laurence Erlbaum},
	author = {Steyvers, Mark and Griffiths, Tom and Kintsch, W.},
	editor = {Landauer, T. and {McNamara}, D. and Dennis, S.},
	date = {2006},
	langid = {english},
	keywords = {X-{CHECK}},
}

@incollection{verhagen_predictive_2012,
	location = {Amsterdam},
	title = {Predictive Modelling: a Case Study of Agricultural Terraces at Monte Pallano (Abruzzo, Italy)},
	isbn = {9789085550662},
	url = {http://dare.uva.nl/document/450268},
	abstract = {As a chartered excavation, the project seeks to characterize and investigate the nature, pattern and dynamics of human habitation and land use in the longue durée within the context of a Mediterranean river valley system. The project sustains both a research program and a month-long didactic field school for undergraduates and sees the symbiotic relation between the two as fundamental to its mission. It also seeks to engage in knowledge transfer to the benefit of the local communities, as well as acting as a professional research partner for the Soprintendenza in its work in the region.},
	pages = {307--317},
	booktitle = {Revive the past : proceedings of the 39th Annual Conference of Computer Applications and Quantitative Methods in Archaeology ({CAA}), Beijing, China, 12-16 April 2011},
	publisher = {Amsterdam Univ. Press},
	author = {Countryman, James R. and Carrier, Sam C. and Kane, Susan E.},
	editor = {Verhagen, Philip},
	date = {2012},
	langid = {english},
	keywords = {act\_Modeling, obj\_ArchaeologicalSites, obj\_Artefacts},
}

@article{bonisch_prasentation_2011,
	title = {Präsentation zu “Zukunft der Wissenschaftskommunikation. Benötigen Wissenschaftler zukünftig noch Verlage und Buchhändler?”},
	url = {http://digiwis.de/blog/2011/05/05/praesentation-zu-zukunft-der-wissenschaftskommunikation-benoetigen-wissenschaftler-zukuenftig-noch-verlage-und-buchhaendler/},
	abstract = {Ich sehe für die zukünftige Wissenschaftskommunikation eine größere Unabhängigkeit der Wissenschaftler von den Verlagen bedingt durch neue elektronische Publikationsmöglichkeiten im Web. Zudem verändert sich auch das Zielpublikum der Wissenschaftler: sind heute noch fast 90 \% der Leser ihrer Texte aus der Wissenschaftscommunity, so wird es durch Blogs, Microblogging etc. in Zukunft die allgemeine Öffentlichkeit an Bedeutung gewinnen. Mehr als heute wird es für den Wissenschaftler von morgen wichtig sein, ein eigenes Reputationsmanagement aufzubauen, auch hier nicht nur für den Fachbereich, sondern auch für die Öffentlichkeit.},
	journaltitle = {Wissenschaft und neue Medien},
	author = {Bönisch, Wenke},
	urldate = {2012-01-05},
	date = {2011-05-05},
	langid = {german},
}

@online{hastac_scholars_program_pixels_2012,
	title = {Pixels and Print: Redefining Academic Publishing \& Scholarly Communication},
	url = {http://hastac.org/forums/pixels-and-print},
	abstract = {The workshop was quickly filled to capacity and the wait list has been full for months. It is one of the first large-scale and inter-institutional workshops on this topic, and it comes at a time when many universities are trying to develop best practices for considering digital work while hiring new professors, evaluating digital dissertation projects, considering how to make these works accessible in library systems and how they can be published and developed. With the acknowledgment that academics are committed to producing digital scholarship, the question has shifted from "Does digital scholarship even count as academic publishing?" to “How do we evaluate, publish and support digital work?” The latest issue of {MLA}'s Profession announces that there is "a growing consensus that humanities disciplines must find ways not simply of evaluating but also of valuing digital scholarship as part of hiring, promotion, and tenure decisions."},
	author = {{HASTAC Scholars program}},
	date = {2012-01-06},
	langid = {english},
}

@article{allen_philologic4:_2013,
	title = {{PhiloLogic}4: An Abstract {TEI} Query System},
	rights = {{TEI} Consortium 2013 (Creative Commons Attribution-{NoDerivs} 3.0 Unported License)},
	issn = {2162-5603},
	url = {http://jtei.revues.org/817},
	shorttitle = {{PhiloLogic}4},
	abstract = {A common problem for {TEI} software development is that projects develop their own custom software stack to address the semantic intricacies present in a deeply-encoded {TEI} corpus. This article describes the design of version 4 of the {PhiloLogic} corpus query engine, which is designed to handle heterogeneous {TEI} encoding through its redesigned abstract data model. We show that such an architecture has substantial benefits for software reuse, allowing for powerful {TEI} applications to be adapted to new corpora with a minimum of custom programming, and we discuss the more general and theoretical implications of abstraction as a {TEI} processing technique.},
	issue = {Issue 5},
	journaltitle = {Journal of the Text Encoding Initiative},
	author = {Allen, Timothy and Gladstone, Clovis and Whaling, Richard},
	editora = {Blanke, Tobias and Romary, Laurent},
	editoratype = {collaborator},
	urldate = {2013-07-01},
	date = {2013-06-21},
	langid = {english},
	keywords = {act\_Query/Retrieve},
}

@article{meister_parsing_2002,
	title = {Parsing for the Theme. A Computer Based Approach},
	url = {http://books.google.de/books?id=DDwZZm1L_FEC&pg=PA407&lpg=PA407&dq=Parsing+for+the+Theme.+A+Computer+Based+Approach&source=bl&ots=7ZxoKSsqit&sig=rM49ANLPlGufkxOcsExYvZG3SMY&hl=de&sa=X&ei=QatfVMiVLY2BPevWgfAB&ved=0CCYQ6AEwAA#v=onepage&q=Parsing%20for%20the%20Theme.%20A%20Computer%20Based%20Approach&f=false},
	abstract = {Themes play a central role in our everyday communication: we have to know what a text is about in order to understand it. Intended meaning cannot be understood without some knowledge of the underlying theme. This book helps to define the concept of  themes  in texts and how they are structured in language use.Much of the literature on Thematics is scattered over different disciplines (literature, psychology, linguistics, cognitive science), which this detailed collection pulls together in one coherent overview. The result is a new landmark for the study and understanding of themes in their everyday manifestation.},
	pages = {407--431},
	journaltitle = {Thematics. Interdisciplinary Studies},
	author = {Meister, Jan Christoph},
	date = {2002},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, obj\_Literature},
}

@article{brunet_ou_2004,
	title = {Où l’on mesure la distance entre les distances},
	url = {http://www.revue-texto.net/Inedits/Brunet/Brunet_Distance.html},
	journaltitle = {Texto!},
	author = {Brunet, Étienne},
	date = {2004-04},
	langid = {french},
	keywords = {meta\_Assessing, t\_Stylometry},
}

@book{pang_opinion_2008,
	title = {Opinion Mining and Sentiment Analysis},
	url = {http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf},
	abstract = {An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object. This survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include materialon summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.},
	author = {Pang, Bo and Lee, Lillian},
	date = {2008},
	keywords = {act\_ContentAnalysis, bigdata, goal\_Analysis, t\_SentimentAnalysis},
}

@article{koch_benefits_1991,
	title = {On the Benefits of Interrelating Computer Science and the Humanities. The case of metaphor},
	url = {http://link.springer.com/article/10.1007/BF00120965},
	abstract = {If there is to be a new, substantive area of teaching and research that combines competence in specific areas of the humanities with computer science understandings and skills, such teaching and research needs to be led by persons who themselves are competent in both the humanities and in computer science, rather than by a team of persons who represent a division of labors along the lines of “idea” persons and “technical” persons. The new kind of teaching and research that might result is pointed to by describing a connectionist, neural network approach to the study of metaphor.},
	pages = {289--295},
	journaltitle = {Computers and the humanities 25 (1991) N.5},
	author = {Koch, Christian},
	date = {1991},
	langid = {english},
	keywords = {X-{CHECK}},
}

@article{serra_measuring_2012,
	title = {Measuring the Evolution of Contemporary Western Popular Music},
	volume = {2},
	rights = {© 2012 Macmillan Publishers Limited. All rights reserved},
	issn = {2045-2322},
	url = {http://www.nature.com/srep/2012/120726/srep00521/full/srep00521.html},
	doi = {10.1038/srep00521},
	abstract = {Popular music is a key cultural expression that has captured listeners' attention for ages. Many of the structural regularities underlying musical discourse are yet to be discovered and, accordingly, their historical evolution remains formally unknown. Here we unveil a number of patterns and metrics characterizing the generic usage of primary musical facets such as pitch, timbre, and loudness in contemporary western popular music. Many of these patterns and metrics have been consistently stable for a period of more than fifty years. However, we prove important changes or trends related to the restriction of pitch transitions, the homogenization of the timbral palette, and the growing loudness levels. This suggests that our perception of the new would be rooted on these changing characteristics. Hence, an old tune could perfectly sound novel and fashionable, provided that it consisted of common harmonic progressions, changed the instrumentation, and increased the average loudness.},
	pages = {1--6},
	number = {251},
	journaltitle = {Scientific Reports},
	author = {Serrà, Joan and Corral, Álvaro and Boguñá, Marián and Haro, Martín and Arcos, Josep Ll},
	urldate = {2012-09-17},
	date = {2012-07-26},
	langid = {english},
	keywords = {*****, {AnalyzeStatistically}, obj\_Music},
}

@book{kehren_moglichkeiten_1994,
	location = {Erkelenz},
	title = {Möglichkeiten und Grenzen der computativen Auswertung von Daten des Atlas der deutschen Volkskunde ({ADV})},
	isbn = {3-924928-16-X},
	url = {http://www.amazon.de/M%C3%B6glichkeiten-computativen-Auswertung-deutschen-Volkskunde/dp/3924928169},
	series = {Bonner kleine Reihe zur Alltagskultur},
	abstract = {Thesis ({PhD} level)},
	pagetotal = {277},
	number = {2},
	publisher = {Kehren},
	author = {Kehren, Georg},
	date = {1994},
	langid = {german},
	keywords = {{AnalyzeStatistically}, obj\_Maps},
}

@article{veit_musikwissenschaft_2005,
	title = {Musikwissenschaft und Computerphilologie - eine schwierige Liaison?},
	url = {http://computerphilologie.uni-muenchen.de/jg05/veit.html},
	abstract = {From the perspective of an editor who intends to make use of the new digital technologies some of the digital libraries and numerous musical databases founded within the last years (or, in a few cases, even in the last third of the last century) prove to be very helpful for daily work. But nevertheless, digital editions in a narrow sense are very rare in the field of music. Looking for the real reason for the delay of musicology in this field of research one has to admit that music proves to be very resistant to all endeavours of encoding. The mixture of graphical and letter symbols or ›normal‹ words in musical notation and the ambiguity of many musical signs lead to a lot of problems when trying to find a good, comprehensive and lasting representing system – which is the precondition for all sorts of analytical operations and for long-term editions too. Today all editions are done with proprietary software often totally out-dated a few years after publication. So one of the most urgent tasks is to find a standard representing system for music (notation). {MusicXML} and the {XML}-Code of the Music Encoding Initiative ({MEI}) are important steps on the way to a form of encoding which will allow the integration of versions and variants and their future re-visualization on the screen. A further problem is the electronic ›mapping‹ of music facsimiles. (Such facsimiles are of the utmost importance for editorial arguing.) But musicologists share these problems with all scholars dealing with ›written‹ documents, and this should be a cause to take joint action.},
	pages = {67--92},
	journaltitle = {Jahrbuch für Computerphilologie 7 (2005)},
	author = {Veit, Joachim},
	date = {2005},
	langid = {german},
	keywords = {meta\_GiveOverview, obj\_Literature, obj\_Music},
}

@article{goodwin_national_2014,
	title = {National Security},
	url = {http://www.jgoodwin.net/?p=1338},
	abstract = {Remarks about Brian Lennon's "The Digital Humanities and National Security."},
	pages = {6},
	journaltitle = {jgoodwin.net (Blog)},
	author = {Goodwin, Jonathan},
	date = {2014},
	langid = {english},
	keywords = {obj\_DigitalHumanities, obj\_Language},
}

@online{clarin_persistent_2009,
	title = {Persistent Identifier Service, Short Guide.},
	url = {http://www.clarin.eu/files/pid-CLARIN-ShortGuide.pdf},
	abstract = {A major problem when citing and using digital (language) resources is the stability of the links used. After some time many {URLs} start decaying because e.g. the resource is moved to another server or the domain name of an organisation changes – a process widely known as link rot. Using persistent identifiers ({PIDs}) it is possible to anticipate to such unavoidable changes. Instead of citing the direct {URL}, one extra level of indirection is introduced, where each resource gets an own abstract code (a handle). Upon access, the user is redirected to the {URL} that is associated to the handle. If the physical location of the resources changes, the only thing that needs to be done is making a change to the associated {URL}. As such, using persistent identifiers lead to citable references that are immune to link rot.},
	author = {{CLARIN}},
	urldate = {2013-02-11},
	date = {2009},
	langid = {english},
	keywords = {act\_Identifying, obj\_Research},
}

@article{heinrich_parameter_2004,
	title = {Parameter estimation for text analysis},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.6555},
	abstract = {This primer presents parameter estimation methods common in Bayesian statistics and apply them to discrete probability distributions, which commonly occur in text modeling. Presentation starts with maximum likelihood and a posteriori estimation approaches and the full Bayesian approach. This presentation is completed by an overview of Bayesian networks, a graphical language to express probabilistic models. As an application, the model of latent Dirichlet allocation is explained and a full derivation of an approximate inference algorithm given based on Gibbs sampling.},
	author = {Heinrich, Gregor},
	date = {2004},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@report{commission_on_cyberinfrastructure_for_the_humanities_and_social_sciences_our_2006,
	location = {New York},
	title = {Our cultural commonwealth : the report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities and Social Sciences},
	url = {http://www.acls.org/cyberinfrastructure/ourculturalcommonwealth.pdf},
	abstract = {With support from The Andrew W. Mellon Foundation, {ACLS} appointed a national Commission on Cyberinfrastructure in the Humanities and Social Sciences. John Unsworth, dean of the Graduate School of Library and Information Science at the University of Illinois, Urbana-Champaign, chaired the commission. The commission carried out research, hearings, and consultations to gather information and develop perspective during the calendar year 2004. A draft report was issued in 2005 for public comment, the intended audience being the scholarly community and the societies that represent it, university provosts, federal funding agencies (including but not limited to the {NSF}), and private foundations. The final report, Our Cultural Commonwealth , was released in the fall of 2006.},
	institution = {American Council of Learned Societies},
	author = {{Commission on Cyberinfrastructure for the Humanities and Social Sciences}},
	date = {2006},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_Methods, obj\_Research},
}

@collection{ferwerda_open_2013,
	location = {London},
	title = {Open Access monographs in the humanities and social sciences conference},
	rights = {{CC}-{BY}},
	url = {jisc-­‐collections.ac.uk/reports/oabooksreport},
	abstract = {On the 1 and 2 July 2013, {JISC} Collections, in partnership with {OAPEN} Foundation, held the Open Access Monographs in the Humanities and Social Sciences conference (\#{OAbooks}). Hosted at the British Library and sponsored by Jisc, {AHRC}, {ESRC}, {FWF} and {NWO}, the conference was attended by over 250 delegates from across the world and from all areas of scholarly communications.

Chaired by Martin Hall, Vice Chancellor of the University of Salford, the conference provided the opportunity to discuss the challenges of moving to an open access model for monographs, learn about new initiatives and to share ideas for where collaboration could help support the adoption of open access monograph publishing in the humanities and social sciences ({HSS}).},
	publisher = {{JISC}},
	editor = {Ferwerda, Eelco and Milloy, Caren},
	date = {2013},
	langid = {english},
}

@incollection{cimiano_ontology_2010,
	location = {Boca Raton, {FL}},
	title = {Ontology Construction},
	isbn = {978-1420085921},
	url = {http://www.amazon.de/Handbook-Language-Processing-Learning-Recognition/dp/1420085921},
	abstract = {The Handbook of Natural Language Processing, Second Edition presents practical tools and techniques for implementing natural language processing in computer systems. Along with removing outdated material, this edition updates every chapter and expands the content to include emerging areas, such as sentiment analysis. New to the Second Edition Greater prominence of statistical approaches New applications section Broader multilingual scope to include Asian and European languages, along with English An actively maintained wiki (http://handbookofnlp.cse.unsw.edu.au) that provides online resources, supplementary information, and up-to-date developments Divided into three sections, the book first surveys classical techniques, including both symbolic and empirical approaches. The second section focuses on statistical approaches in natural language processing. In the final section of the book, each chapter describes a particular class of application, from Chinese machine translation to information visualization to ontology construction to biomedical text mining. Fully updated with the latest developments in the field, this comprehensive, modern handbook emphasizes how to implement practical language processing tools in computational systems.},
	booktitle = {Handbook of Natural Language Processing, Second Edition},
	publisher = {{CRC} Press, Taylor and Francis Group},
	author = {Cimiano, Philipp and Buitelaar, Paul and Völker, Johanna},
	editor = {Indurkhya, Nitin and Damerau, Fred J.},
	date = {2010},
	langid = {english},
	keywords = {act\_Annotating, act\_Conceptualizing, obj\_AnyObject},
}

@report{european_commission_online_2012,
	location = {Luxembourg},
	title = {Online survey on scientific information in the digital age},
	url = {http://ec.europa.eu/research/science-society/document_library/pdf_06/survey-on-scientific-information-digital-age_en.pdf},
	abstract = {In 2012, the European Commission intends to adopt a communication and recommendation on access to and preservation of scientific information in the digital age. This initiative builds on earlier policy developments in this area, and is being developed within the policy contexts of the {EU} flagship initiatives ‘Innovation Union’ and ‘Digital agenda for Europe’, and of the push for improved knowledge circulation in the European research area ({ERA}).},
	institution = {Publications Office of the European Union},
	author = {{European Commission}},
	date = {2012},
	langid = {english},
	keywords = {goal\_Dissemination, obj\_ResearchResults},
}

@article{van_halteren_new_2005,
	title = {New Machine Learning Methods Demonstrate the Existence of a Human Stylome},
	volume = {12},
	issn = {0929-6174, 1744-5035},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09296170500055350},
	doi = {10.1080/09296170500055350},
	abstract = {Earlier research has shown that established authors can be distinguished by measuring specific properties of their writings, their stylome as it were. Here, we examine writings of less experienced authors. We succeed in distinguishing between these authors with a very high probability, which implies that a stylome exists even in the general population. However, the number of traits needed for so successful a distinction is an order of magnitude larger than assumed so far. Furthermore, traits referring to syntactic patterns prove less distinctive than traits referring to vocabulary, but much more distinctive than expected on the basis of current generativist theories of language learning.},
	pages = {65--77},
	number = {1},
	journaltitle = {Journal of Quantitative Linguistics},
	author = {van Halteren, Hans and Baayen, Harald and Tweedie, Fiona and Haverkort, Marco and Neijt, Anneke},
	urldate = {2013-03-19},
	date = {2005-04},
	keywords = {t\_MachineLearning, t\_Stylometry},
}

@book{wanske_musiknotation_1988,
	location = {Mainz [u.a.]},
	title = {Musiknotation : von der Syntax des Notenstichs zum {EDV}-gesteuerten Notensatz},
	isbn = {3-7957-2886-X},
	url = {http://www.amazon.de/Musiknotation-Syntax-Notenstichs-EDV-gesteuerten-Notensatz/dp/379572886X},
	pagetotal = {459},
	publisher = {Schott},
	author = {Wanske, Helene},
	date = {1988},
	langid = {german},
	keywords = {act\_Publishing, obj\_Music, t\_Encoding},
}

@article{hoover_multivariate_2003,
	title = {Multivariate Analysis and the Study of Style Variation},
	volume = {18},
	url = {http://llc.oxfordjournals.org/content/18/4/341.short},
	abstract = {This paper investigates style variation in George Orwell's Nineteen Eighty-Four and William Golding's The Inheritors using multivariate analysis, specifically, cluster analysis of the frequencies of frequent words. Baseline tests on a corpus including these and four other novels show that traditional authorship attribution techniques correctly distinguish all sections of each novel from all sections of the other five and correctly cluster all sections of each novel. They are also very successful in distinguishing the section of Nineteen Eighty-Four that purports to be a political tract by Emmanuel Goldstein from the rest of the novel. They are less successful in distinguishing the style of the final chapter of The Inheritors, where critics have argued that a sudden shift of point of view leads to a radical variation in style. The nature of this stylistic variation suggests a modification in the way that frequent words are selected for analysis—a modification that gives improved results for both novels and sharply distinguishes the final chapter from the rest of The Inheritors. A further test of the modified technique on an unusual section of The Picture of Dorian Gray suggests that it may be more widely useful in studies of style variation.},
	pages = {341--360},
	journaltitle = {Literary and Linguistic Computing},
	author = {Hoover, David L.},
	date = {2003},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@collection{crawford_modern_2009,
	location = {Farnham, England ; Burlington, {VT}},
	title = {Modern methods for musicology: prospects, proposals, and realities},
	isbn = {9780754673026},
	url = {http://books.google.de/books?hl=de&lr=&id=uani7DPZn24C&oi=fnd&pg=PA1&dq=Modern+methods+for+musicology:+prospects,+proposals,+and+realities&ots=5h1n8W039J&sig=ixgqhhjgJVfAbTg2HxCCXJd1Frk#v=onepage&q=Modern%20methods%20for%20musicology%3A%20prospects%2C%20proposals%2C%20and%20realities&f=false},
	series = {Digital research in the arts and humanities},
	shorttitle = {Modern methods for musicology},
	abstract = {Written by leading experts, this volume provides a picture of the realities of current {ICT} use in musicology as well as prospects and proposals for how it could be fruitfully used in the future. Through its coverage of topics spanning content-based sound searching/retrieval, sound and content analysis, markup and text encoding, audio resource sharing, and music recognition, this book highlights the breadth and inter-disciplinary nature of the subject matter and provides a valuable resource to technologists, musicologists, musicians and music educators. It facilitates the identification of worthwhile goals to be achieved using technology and effective interdisciplinary collaboration.},
	pagetotal = {185},
	publisher = {Ashgate},
	editor = {Crawford, Tim and Gibson, Lorna},
	date = {2009},
	langid = {english},
	keywords = {X-{CHECK}, obj\_Music},
}

@report{rockwell_mind_2010,
	title = {Mind the Gap (Bridging the Gap between {HPC} and the Humanities)},
	url = {https://docs.google.com/document/preview?id=1wSEesXjAKj8x56AoqfF_fwSkCQuBXp82C0IWJrVgDTQ},
	abstract = {There is a gap between research in the Humanities and Canadian high-performance computing ({HPC}) facilities, but it is not what we thought it was. We used to think humanists didn't need supercomputing - they were happy with a wordprocessor, email and the Web. Now it is clear that humanists have large multimedia datasets and big questions to ask of the history of human culture. Then we used to think the gap was primarily between facilities set up for queued batch programs and practices in the Humanities of asking questions repeatedly of "always-on" web services. Though there is still some truth to that gap, many {HPC} facilities have begun to support "portal" or "cloud" facilities that are always-on and can thus support Humanities practices. The gap now is really one of research culture and support. On the one hand we have to find ways of training and preparing humanities research teams to be able to imagine using existing {HPC} facilties, and on the other we have to develop the ability of {HPC} consortia to be able to reach out and support humanists.},
	institution = {University of Alberta},
	author = {Rockwell, Geoffrey and Meredith-Lobay, Megan},
	date = {2010},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Advocating, meta\_Assessing, obj\_AnyObject},
}

@article{dieckmann_meta-image_2012,
	title = {Meta-Image – Forschungsumgebung für den Bilddiskurs in der Kunstgeschichte},
	url = {http://edoc.hu-berlin.de/cmsj/35/dieckmann-lisa-11/PDF/dieckmann.pdf},
	abstract = {Das Projekt Meta-Image fügt bestehende Komponenten zu einer virtuellen Arbeits- und Forschungsumgebung für den Bilddiskurs in der Kunstgeschichte zusammen. Meta-Image verbindet dazu die digitalen Bildbestände des Verbundes kunsthistorischer Bilddatenbanken prometheus und ein auf die Verlinkung und Anreicherung von Bildcorpora spezialisiertes Bild-Beschreibungs-Tool mit dem Ziel einer integrativen Cyberinfrastructure für die kunsthistorische Forschungsarbeit direkt am Bild. Meta-Image reichert die formellen dokumentarisch-archivarischen Informationen und Sacherschließungen des Repositorys um die Ergebnisse der informellen Forschungstätigkeiten der Fachcommunity an, wobei Ursprung und Kontext der Meta-Informationen stets transparent bleiben.},
	pages = {11--17},
	number = {35},
	journaltitle = {cms-journal},
	author = {Dieckmann, Lisa and Anita, Kliemann and Warnke, Martin},
	date = {2012},
	langid = {german},
	keywords = {act\_Annotating, act\_ContentAnalysis, goal\_Analysis, goal\_Collaboration, goal\_Enrichment, obj\_Images},
}
@article{rockwell_seeing_1999,
	title = {Seeing the text through the trees: visualization and interactivity in text applications},
	volume = {14},
	url = {http://llc.oxfordjournals.org/content/14/1/115.short},
	abstract = {In this paper, we discuss two interactive text visualization systems and then discuss the rhetorical effects of interactivity. The first model is {SIMWeb}, a data visualization system with connections to {TACTweb} for full-text searching. {SIMWeb} provides a graphical representation of the results of statistical processes that can be used to explore a text. The second model, Eye-{ConTact}, is a prototype for a process visualization environment for research applications in the study of electronic texts. The paper then discusses the effects of visualization, with particular attention to the contribution of interactivity to the process of textual research. We argue that this allows for pragmatic experimentation with processes and information, and conclude by discussing some of the dangers of interactive visualization systems.},
	pages = {115--130},
	journaltitle = {Literary and Linguistic Computing},
	author = {Rockwell, Geoffrey and Bradley, J. and Monger, P.},
	date = {1999},
	langid = {english},
	keywords = {act\_Visualizing, meta\_Theorizing},
}

@book{fiormonte_scrittura_2003,
	location = {Turino},
	title = {Scrittura e filologia nell'era digitale},
	url = {http://www.bollatiboringhieri.it/scheda.php?codice=9788833957135},
	abstract = {Questo libro è un’introduzione completa e accessibile al fenomeno della testualità digitale rivolta a studenti e ricercatori dell’area umanistica. Il volume è strutturato in quattro parti più un’appendice didattica disponibile in rete e costantemente aggiornata (www.bollatiboringhieri.it). Nella prima parte viene discusso il rapporto fra la comunicazione scritta e i suoi supporti. Lo scopo è mostrare come l’impatto di ciascun sistema di media – informatica inclusa – dia luogo a risultati complessi e contradditori intrecciandosi con fenomeni sociali, culturali, economici. Nella seconda parte l’autore analizza le principali forme della testualità digitale, dalle sue origini (la videoscrittura, l’ipertesto) ai giorni nostri (la comunicazione interattiva, i giochi di ruolo, la retorica del Web). Vengono poi affrontati i nodi teorici e pratici del rapporto fra critica testuale e digitalizzazione del documento. Prendendo spunto dallo studio delle varianti d’autore viene presentato Varianti digitali, un archivio on-line per la didattica e lo studio del processo di scrittura. Accompagna e conclude il volume una rassegna delle più importanti risorse on-line e off-line per la filologia digitale.},
	publisher = {Bollati Boringhieri},
	author = {Fiormonte, Domenico},
	date = {2003},
	langid = {italian},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@book{jockers_macroanalysis:_2013,
	title = {Macroanalysis: Digital Methods and Literary History},
	isbn = {978-0252079078},
	url = {http://books.google.de/books?hl=de&lr=&id=mPOdxQgpOSUC&oi=fnd&pg=PP2&dq=Macroanalysis:+Digital+Methods+and+Literary+History&ots=R8NZyYyjSh&sig=OkLlZGTVZZMmpYgvq8CQgRzlRSw#v=onepage&q=Macroanalysis%3A%20Digital%20Methods%20and%20Literary%20History&f=false},
	series = {Topics in the Digital Humanities},
	abstract = {In this volume, Matthew L. Jockers introduces readers to large-scale literary computing and the revolutionary potential of macroanalysis--a new approach to the study of the literary record designed for probing the digital-textual world as it exists today, in digital form and in large quantities. Using computational analysis to retrieve key words, phrases, and linguistic patterns across thousands of texts in digital libraries, researchers can draw conclusions based on quantifiable evidence regarding how literary trends are employed over time, across periods, within regions, or within demographic groups, as well as how cultural, historical, and societal linkages may bind individual authors, texts, and genres into an aggregate literary culture.
 
Moving beyond the limitations of literary interpretation based on the "close-reading" of individual works, Jockers describes how this new method of studying large collections of digital material can help us to better understand and contextualize the individual works within those collections.},
	publisher = {University of Illinois Press},
	author = {Jockers, Matthew L.},
	date = {2013-06},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@book{peer_scientific_2012,
	location = {Amsterdam, Netherlands; Philadelphia, {PA}},
	title = {Scientific methods for the humanities},
	isbn = {9789027233479  9027233470  9789027233486  9027233489},
	url = {https://benjamins.com/#catalog/books/lal.13/main},
	abstract = {Here is a much needed introductory textbook on empirical research methods for the Humanities. Especially aimed at students and scholars of Literature, Applied Linguistics, and Film and Media, it stimulates readers to reflect on the problems and possibilities of testing the empirical assumptions and offers hands-on learning opportunities to develop empirical studies. It explains a wide range of methods, from interviews to observation research, and guides readers through the choices researchers have to make. It discusses the essence of experiments, illustrates how studies are designed, how to develop questionnaires, and helps readers to collect and analyze data by themselves. The book presents qualitative approaches to research but focuses mostly on quantitative methods, detailing the workings of basic statistics. At the end, the book also shows how to give papers at international conferences, how to draft a report, and what is involved in the preparation of a publishable article.},
	publisher = {John Benjamins Pub.},
	author = {Peer, Willie van and Hakemulder, Jèmeljan and Zyngier, Sonia},
	date = {2012},
	langid = {english},
	keywords = {{AnalyzeStatistically}, goal\_Analysis, meta\_GiveOverview, obj\_Literature, obj\_Methods},
}

@article{pinker_science_2013,
	title = {Science Is Not Your Enemy},
	issn = {0028-6583},
	url = {http://www.newrepublic.com/article/114127/science-not-enemy-humanities},
	abstract = {An impassioned plea to neglected novelists, embattled professors, and tenure-less historians},
	journaltitle = {The New Republic},
	author = {Pinker, Steven},
	urldate = {2014-01-22},
	date = {2013-08-06},
	langid = {english},
}

@online{burnard_science_2014,
	title = {Science and the Humanities},
	url = {http://foxglove.hypotheses.org/468},
	titleaddon = {Foxglove},
	author = {Burnard, Lou},
	urldate = {2014-03-17},
	date = {2014-03-16},
	langid = {english},
	keywords = {act\_Theorizing, obj\_DigitalHumanities},
}

@article{laster_scholars_2010,
	title = {Scholars Scale Up Music Studies},
	issn = {0009-5982},
	url = {http://chronicle.com/article/Scholars-Scale-Up-Music/65709/},
	journaltitle = {The Chronicle of Higher Education},
	author = {Laster, Jill},
	urldate = {2013-03-21},
	date = {2010-05-28},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@book{borgman_scholarship_2010,
	location = {Cambridge  Mass. ;;London},
	title = {Scholarship in the digital age : information, infrastructure, and the Internet},
	isbn = {9780262514903},
	url = {http://mitpress.mit.edu/books/scholarship-digital-age},
	shorttitle = {Scholarship in the digital age},
	abstract = {Scholars in all fields now have access to an unprecedented wealth of online information, tools, and services. The Internet lies at the core of an information infrastructure for distributed, data-intensive, and collaborative research. Although much attention has been paid to the new technologies making this possible, from digitized books to sensor networks, it is the underlying social and policy changes that will have the most lasting effect on the scholarly enterprise. In Scholarship in the Digital Age, Christine Borgman explores the technical, social, legal, and economic aspects of the kind of infrastructure that we should be building for scholarly research in the twenty-first century.
Borgman describes the roles that information technology plays at every stage in the life cycle of a research project and contrasts these new capabilities with the relatively stable system of scholarly communication, which remains based on publishing in journals, books, and conference proceedings. No framework for the impending “data deluge” exists comparable to that for publishing. Analyzing scholarly practices in the sciences, social sciences, and humanities, Borgman compares each discipline’s approach to infrastructure issues. In the process, she challenges the many stakeholders in the scholarly infrastructure--scholars, publishers, libraries, funding agencies, and others--to look beyond their own domains to address the interaction of technical, legal, economic, social, political, and disciplinary concerns. Scholarship in the Digital Age will provoke a stimulating conversation among all who depend on a rich and robust scholarly environment.

Christine L. Borgman is Professor and Presidential Chair in Information Studies at the University of California, Los Angeles. She is the author of From Gutenberg to the Global Information Infrastructure: Access to Information in the Networked World ({MIT} Press, 2000).},
	publisher = {{MIT} Press},
	author = {Borgman, Christine},
	date = {2010},
	langid = {english},
	keywords = {act\_Conceptualizing, meta\_GiveOverview, obj\_DigitalHumanities, obj\_Infrastructures, obj\_Methods},
}

@article{blanke_scholarly_2011,
	title = {Scholarly primitives: Building institutional infrastructure for humanities e-Science},
	issn = {0167739X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167739X11001178},
	doi = {10.1016/j.future.2011.06.006},
	shorttitle = {Scholarly primitives},
	abstract = {In this article we bring together the results of a number of humanities e-research projects at King’s College London. This programme of work was not carried out in an ad hoc manner, but was built on a rigorous methodological foundation, firstly by ensuring that the work was thoroughly grounded in the practice of humanities researchers (including ‘digitally-aware’ humanists), and secondly by analysing these practices in terms of ‘scholarly primitives’, basic activities common to research across humanities disciplines. The projects were then undertaken to provide systems and services that support various of these primitives, with a view to developing a research infrastructure constructed from these components, which may be regarded as a ‘production line’ for humanities research, supporting research activities from the creation of primary sources in digital form through to the publication of research outputs for discussion and re-use.},
	journaltitle = {Future Generation Computer Systems},
	author = {Blanke, Tobias and Hedges, Mark},
	urldate = {2012-04-25},
	date = {2011-07},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_Infrastructures, obj\_Methods},
}

@misc{palmer_scholarly_2009,
	title = {Scholarly Information Practices in the Online Environment: Themes from the Literature and Implications for Library Service Development},
	url = {http://www.oclc.org/programs/publications/reports/2009-02.pdf},
	shorttitle = {Scholarly Information Practices in the Online Environment},
	publisher = {{OCLC} Research},
	author = {Palmer, Carole and Teffeau, Lauren and Pirmann, Carrie},
	urldate = {2011-05-03},
	date = {2009-01},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, obj\_Methods},
}

@online{bailey_scholarly_2011,
	title = {Scholarly Electronic Publishing Bibliography},
	url = {http://www.digital-scholarship.org/sepb/sepb.html},
	abstract = {The Scholarly Electronic Publishing Bibliography ({SEPB}) presents selected English-language articles, books, and other printed and electronic sources that are useful in understanding scholarly electronic publishing efforts on the Internet. The bibliography covers a wide range of topics, such as digital copyright, digital libraries, digital preservation, digital repositories, e-books, e-journals, license agreements, metadata, and open access.},
	titleaddon = {Houston: Digital Scholarship},
	author = {Bailey, Charles W.},
	date = {2011},
	langid = {english},
	keywords = {act\_Publishing, obj\_Research},
}

@book{luyckx_scalability_2010,
	location = {Brussel},
	title = {Scalability issues in authorship attribution.},
	isbn = {9789054878230 9054878231},
	abstract = {This book is about authorship attribution, the task that aims to identify the author of a text, given a model of authorial style based on texts of known authorship. Computational authorship attribution does not rely on in-depth reading, but rather automates the process. This book investigates the behavior of a text categorization approach to the task when confronted with scalability issues. By addressing the issues of experimental design, data size, and author set size, the dissertation demonstrates whether the approach taken is valid in experiments with limited or sufficient data, and with small or large sets of authors. "Scalability issues in authorship attribution" provides the first in-depth and systematic study of the so-called scalability issues in authorship attribution. Above all, the book stresses the importance of methodology: even the smallest of decisions can significantly influence the outcome and reliability of an experiment.},
	publisher = {{ASA} Publishers},
	author = {{Luyckx}},
	date = {2010},
	langid = {english},
}

@online{schmidt_sapping_2011,
	title = {Sapping Attention: Theory First},
	url = {http://sappingattention.blogspot.com/2011/11/theory-first.html},
	shorttitle = {Sapping Attention},
	abstract = {Natalie Cecire recently started an important debate about the role of theory in the digital humanities. She's rightly concerned that the {THATcamp} motto--"more hack, less yack"--promotes precisely the wrong understanding of what digital methods offer:},
	titleaddon = {Sapping Attention},
	author = {Schmidt, Ben},
	urldate = {2011-11-04},
	date = {2011-11-03},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@collection{sagw-assh-assm_sagw-bulletin:_2012,
	location = {Bern},
	title = {{SAGW}-Bulletin: Dossier "Digital Humanities und Web 2.0"},
	volume = {1},
	url = {http://www.sagw.ch/sagw/oeffentlichkeitsarbeit/bulletin.html},
	publisher = {{SAGW}-{ASSH}-{ASSM}},
	editor = {{SAGW-ASSH-ASSM}},
	date = {2012-02},
	langid = {german},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@misc{gros_r_2009,
	title = {R Reader. Arbeiten mit dem Statistikprogramm R},
	url = {http://cran.r-project.org/doc/contrib/Grosz+Peters-R-Reader.pdf},
	author = {Groß, Peter and Peters, Benjamin},
	date = {2009},
	langid = {german},
	keywords = {{AnalyzeStatistically}},
}

@article{kestemont_robust_2012,
	title = {Robust Rhymes? The Stability of Authorial Style in Medieval Narratives*},
	volume = {19},
	issn = {0929-6174, 1744-5035},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09296174.2012.638796},
	doi = {10.1080/09296174.2012.638796},
	shorttitle = {Robust Rhymes?},
	abstract = {We explore the application of stylometric methods developed for modern texts to rhymed medieval narratives (Jacob van Maerlant and Lodewijk van Velthem, ca. 1260–1330). Because of the peculiarities of medieval text transmission, we propose to use highly frequent rhyme words for authorship attribution. First, we shall demonstrate that these offer important benefits, being relatively content-independent and well-spread over texts. Subsequent experimentation shows that correspondence analyses can indeed detect authorial differences using highly frequent rhyme words. Finally, we demonstrate for Maerlant's oeuvre that this highly frequent rhyme words' stylistic stability should not be exaggerated since their distribution significantly correlates with the internal structure of that oeuvre.},
	pages = {54--76},
	number = {1},
	journaltitle = {Journal of Quantitative Linguistics},
	author = {Kestemont, Mike and Daelemans, Walter and Sandra, Dominiek},
	urldate = {2012-10-16},
	date = {2012-02},
	langid = {english},
}

@misc{moats_rfc_1997,
	title = {{RFC} 2141: {URN} {SYNTAX}},
	url = {http://www.ietf.org/rfc/rfc2141.txt},
	abstract = {Uniform Resource Names ({URNs}) are intended to serve as persistent,
   location-independent, resource identifiers. This document sets
   forward the canonical syntax for {URNs}.  A discussion of both existing
   legacy and new namespaces and requirements for {URN} presentation and
   transmission are presented.  Finally, there is a discussion of {URN}
   equivalence and how to determine it.},
	author = {Moats, R.},
	date = {1997-05},
	langid = {english},
	keywords = {act\_Identifying},
}

@article{francis_review_2010,
	title = {Review Article: A modest proposal for a reorientation in literary studies},
	volume = {19},
	issn = {0963-9470, 1461-7293},
	url = {http://lal.sagepub.com/cgi/doi/10.1177/0963947010368318},
	doi = {10.1177/0963947010368318},
	shorttitle = {Review Article},
	abstract = {This essay offers a review of Literature, Science and a New Humanities by Jonathan Gottschall. The author challenges the field of literary studies to rethink long-held assumptions about the nature of knowledge and human nature. In particular, what appear to be dominant tendencies in the field have strongly resisted the inclusion of findings from biology and genetics specifically. This resistance is tied to a view that scientific methods do not apply to the study of literature, and that radical social-constructivist theory presents a general alternative to the traditional methods of scientific inquiry. A proposal is made to incorporate recent advances in the cognitive sciences into the analysis of literature. Specifically, concepts drawn from evolutionary psychology might be especially useful in the analysis of narrative themes.},
	pages = {305--317},
	number = {3},
	journaltitle = {Language and Literature},
	author = {Francis, N.},
	urldate = {2013-07-03},
	date = {2010-08-04},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{frabetti_rethinking_2011,
	title = {Rethinking the digital humanities in the context of originary technicity.},
	volume = {12},
	url = {http://www.culturemachine.net/index.php/cm/article/view/431/461},
	abstract = {a radical engagement with digitality is not meant to interrupt the humanities’ ongoing experimentation with digitality. However, it does call for critical reflection on digitality (for instance, on the conceptual foundations of the algorithms we use) while we experiment with it. Even more importantly, in a world in which the university is becoming more and more just another knowledge- based organization governed by rules of efficiency and flexibility, the digital humanities might become the ideal context in which to reaffirm the role of the university as a public sphere. Questioning instrumentality is an essential step toward questioning the idea of knowledge as a commodity, and of ‘the neoliberal logic that views schools as malls, students as consumers, and faculty as entrepreneurs’ (Giroux, 2010: non-pag.). If academic labour must resist instrumentality in order to remain political, then the digital humanities become an ideal place for a persistent critique of all instrumental modes of thinking.},
	pages = {20},
	issue = {Digital Humanities: beyond computing},
	journaltitle = {Culture Machine},
	author = {Frabetti, Federica },
	date = {2011},
	langid = {english},
	keywords = {meta\_Theorizing, obj\_Code, obj\_DigitalHumanities},
}

@online{mcgann_rethinking_2000,
	title = {Rethinking Textuality},
	url = {http://www2.iath.virginia.edu/jjm2f/old/jj2000aweb.html},
	abstract = {This is a report and an analysis of an experimental project I recently began with Johanna Drucker at U. of Virginia. We have been calling it "Metalogics of the Book". Briefly, it involves using computers to explore how the graphic features of textual documents function in a signifying field. The experiment analyzes and manipulates such graphic features through mark?up protocols (page description languages, tags) and other computer tools ({OCR})},
	titleaddon = {Homepage Jerome {McGann}},
	author = {{McGann}, Jerome},
	date = {2000},
	langid = {english},
	keywords = {act\_Conceptualizing},
}

@online{jisc_research_2013,
	title = {Research Lifecycle Diagram},
	rights = {Copyright Higher Education Funding Council for England ({HEFCE}), on behalf of the Joint Information Systems Commmittee ({JISC}), unless explicitly acknowledged otherwise.},
	url = {http://www.jisc.ac.uk/whatwedo/campaigns/res3/jischelp.aspx},
	abstract = {{JISC}’s work supports all stages of the research lifecycle. Use the diagram below to find out about the {JISC} services that can help at each stage, or about the development work {JISC} is funding to enable researchers to make even better use of advanced information and communications technologies.},
	titleaddon = {How {JISC} is helping researchers},
	author = {{JISC}},
	urldate = {2012-04-19},
	date = {2013},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_Methods, obj\_Research},
}

@article{borda_report_2006,
	title = {Report of the working group on virtual research communities for the ost e-infrastructure steering group},
	url = {http://eprints.soton.ac.uk/42074},
	abstract = {Virtual Research Communities ({VRC}) are a new concept but early research suggests that they have the potential to open exciting new opportunities to collaborate in research and thus realise significant gains at institutional, national and international levels. International comparisons have revealed that the {UK} is well advanced in its understanding of the area and has the world’s best structured programme of developments under way. Further programmes to develop their full potential need to examine issues of human behaviour, the role of government and other policy makers and closer links with commercial organisations, as well as continuing to pursue development of technology and standards. Five inter-related programmes of work are recommended to maintain the {UK}’s leading position in this area, and retain our ability to carry out world-class research:

1.Establish a major programme of activities to understand the behavioural and social issues associated with greater take-up and transferability of developments in {VRCs}. The importance of reflecting the real needs, habits, preferences and aspirations of researchers themselves cannot be underestimated (see 6.1).
2.Continue and enhance current {VRE} development programmes to explore and understand concepts, techniques and their applications to e-Science and research, using opportunities for joint international programmes where possible (see 6.2).
3.Extend the e-framework activities of the {JISC} to encompass the full range of requirements of a {VRC} and establish whether a single, generic framework is possible or whether several, discipline-based frameworks are necessary (see 6.3)
4.Encourage greater cooperation between research and the commercial sector to ensure good practice in computer-based collaboration in business enterprises can be transferred into e-Science, to provide a vehicle for developing user-friendly commercial {VRE} applications and to enhance knowledge transfer activities (see 6.4).
5.Establish a task force to monitor developments in {VRCs} and similar activities in e-Science to recommend to government and funding organisations how policies and reward mechanisms can be shaped to promote take-up of opportunities, and to encourage the development of young researchers able to use the full capabilities of e-Science when they enter their field (see 6.5).},
	author = {Borda, A. and Careless, J. and Dimitrova, M. and Fraser, M. and Frey, J. and Hubbard, P. and Goldstein, S. and Pung, C. and Shoebridge, M. and Wiseman, N.},
	urldate = {2013-01-21},
	date = {2006},
	langid = {english},
	keywords = {act\_Conceptualizing, meta\_Assessing, obj\_VREs},
}

@report{brown_repah:_2006,
	title = {{RePAH}: A User Requirements Analysis for Portals in the Arts and Humanities (Final Report)},
	url = {repah.dmu.ac.uk/report/pdfs/RePAHReport-Complete.pdf},
	abstract = {Commissioned and Funded by the Arts and Humanities Research Council [{AHRC}] {ICT} in Arts and Humanities Programme},
	pages = {275},
	institution = {{AHRC}},
	author = {Brown, Stephen and Ross, Rob and Gerrard, David and Greengrass, Mark and Bryson, Jared},
	date = {2006},
	langid = {english},
	keywords = {act\_Conceptualizing, meta\_Assessing, obj\_Infrastructures, obj\_Research},
}

@article{english_rent--crowd?_2013,
	title = {Rent-a-crowd? Crowdfunding academic research},
	volume = {19},
	rights = {Authors submitting a paper to First Monday automatically agree to confer a limited license to First Monday if and when the manuscript is accepted for publication. This license allows First Monday to publish a manuscript in a given issue. Authors have a choice of: 1. Dedicating the article to the public domain. This allows anyone to make any use of the article at any time, including commercial use. A good way to do this is to use the Creative Commons Public Domain Dedication Web form; see  http://creativecommons.org/license/publicdomain-2?lang=en . 2. Retaining some rights while allowing some use. For example, authors may decide to disallow commercial use without permission. Authors may also decide whether to allow users to make modifications (e.g. translations, adaptations) without permission. A good way to make these choices is to use a Creative Commons license. * Go to  http://creativecommons.org/license/ . * Choose and select a license. * What to do next — you can then e–mail the license html code to yourself. Do this, and then forward that e–mail to First Monday’s editors. Put your name in the subject line of the e–mail with your name and article title in the e–mail. Background information about Creative Commons licenses can be found at  http://creativecommons.org/about/licenses/ . 3. Retaining full rights, including translation and reproduction rights. Authors may use the statement: © Author 2008 All Rights Reserved. Authors may choose to use their own wording to reserve copyright. If you choose to retain full copyright, please add your copyright statement to the end of the article. Authors submitting a paper to First Monday do so in the understanding that Internet publishing is both an opportunity and challenge. In this environment, authors and publishers do not always have the means to protect against unauthorized copying or editing of copyright–protected works.},
	issn = {13960466},
	url = {http://firstmonday.org/ojs/index.php/fm/article/view/4818},
	doi = {10.5210/fm.v19i1.4818},
	shorttitle = {Rent-a-crowd?},
	abstract = {This paper examines the use of crowdfunding platforms to fund academic research. Looking specifically at the use of a Pozible campaign to raise funds for a small pilot research study into home education in Australia, the paper reports on the success and problems of using the platform. It also examines the crowdsourcing of literature searching as part of the package. The paper looks at the realities of using this type of platform to gain start-up funding for a project and argues that families and friends are likely to be the biggest supporters. The finding that family and friends are likely to be the highest supporters supports similar work in the arts communities that are traditionally served by crowdfunding platforms. The paper argues that, with exceptions, these platforms can be a source of income in times where academics are finding it increasingly difficult to source government funding for projects.},
	number = {1},
	journaltitle = {First Monday},
	author = {English, Rebecca},
	urldate = {2014-01-28},
	date = {2013-12-21},
	langid = {english},
}

@report{bulger_reinventing_2011,
	location = {{UK}},
	title = {Reinventing research? Information practices in the humanities},
	url = {http://www.rin.ac.uk/our-work/using-and-accessing-information-resources/information-use-case-studies-humanities},
	abstract = {The {RIN} has completed a second series of case studies to provide a detailed analysis of how humanities’ researchers discover, use, create and manage their information resources.},
	institution = {Research Information Network},
	author = {Bulger, Monica},
	date = {2011-04},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Research},
}
@book{biber_register_2009,
	location = {Cambridge, {UK}; New York},
	title = {Register, genre, and style},
	isbn = {9780521860604  0521860601  0521677890  9780521677899},
	abstract = {"This book describes the most important kinds of texts in English and introduces the methodological techniques used to analyze them. Three analytical approaches are introduced and compared, describing a wide range of texts from the perspectives of register, genre, and style. The primary focus of the book is on the analysis of registers. Part 1 introduces an analytical framework for studying registers, genre conventions, and styles. Part 2 provides detailed descriptions of particular text varieties in English, including spoken interpersonal varieties (conversation, university office hours, service encounters), written varieties (newspapers, academic prose, fiction), and emerging electronic varieties (e-mail, internet forums, text messages). Finally, Part 3 introduces advanced analytical approaches using corpora, and discusses theoretical concerns, such as the place of register studies in linguistics, and practical applications of register analysis. Each chapter ends with three types of activities: reflection and review activities, analysis activities, and larger project ideas"--Provided by publisher.},
	publisher = {Cambridge University Press},
	author = {Biber, Douglas and Conrad, Susan},
	date = {2009},
	langid = {english},
	keywords = {act\_StylisticAnalysis, bigdata, meta\_GiveOverview, obj\_Language},
}

@article{jockers_reassessing_2008,
	title = {Reassessing authorship of the Book of Mormon using delta and nearest shrunken centroid classification},
	volume = {23},
	url = {http://llc.oxfordjournals.org/content/23/4/465.abstract},
	doi = {10.1093/llc/fqn040},
	abstract = {Mormon prophet Joseph Smith (1805–44) claimed that more than two-dozen ancient individuals (Nephi, Mormon, Alma, etc.) living from around 2200 {BC} to 421 {AD} authored the Book of Mormon (1830), and that he translated their inscriptions into English. Later researchers who analyzed selections from the Book of Mormon concluded that differences between selections supported Smith's claim of multiple authorship and ancient origins. We offer a new approach that employs two classification techniques: ‘delta’ commonly used to determine probable authorship and ‘nearest shrunken centroid’ ({NSC}), a more generally applicable classifier. We use both methods to determine, on a chapter-by-chapter basis, the probability that each of seven potential authors wrote or contributed to the Book of Mormon. Five of the seven have known or alleged connections to the Book of Mormon, two do not, and were added as controls based on their thematic, linguistic, and historical similarity to the Book of Mormon. Our results indicate that likely nineteenth century contributors were Solomon Spalding, a writer of historical fantasies; Sidney Rigdon, an eloquent but perhaps unstable preacher; and Oliver Cowdery, a schoolteacher with editing experience. Our findings support the hypothesis that Rigdon was the main architect of the Book of Mormon and are consistent with historical evidence suggesting that he fabricated the book by adding theology to the unpublished writings of Spalding (then deceased).},
	pages = {465 --491},
	number = {4},
	journaltitle = {Literary and Linguistic Computing},
	author = {Jockers, Matthew L. and Witten, Daniela M. and Criddle, Craig S.},
	urldate = {2011-12-14},
	date = {2008-12},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@collection{mounier_read/write_2012,
	title = {Read/Write Book 2 : Une introduction aux humanités numériques},
	rights = {© {OpenEdition} Press, 2012 Conditions d'utilisation : http://www.openedition.org/6540},
	url = {http://books.openedition.org/oep/226},
	shorttitle = {Read/Write Book 2},
	abstract = {Qu’est-ce que les humanités numériques ? Apparue en 2006, l’expression connaît depuis un véritable succès. Mais au-delà du slogan à la mode, quelle est la réalité des pratiques qu’il désigne ? Si tout le monde s’accorde sur une définition minimale à l’intersection des technologies numériques et des sciences humaines et sociales, les vues divergent lorsqu’on entre dans le vif du sujet. Les humanités numériques représentent-elles une véritable révolution des pratiques de recherche et des paradigmes intellectuels qui les fondent ou, plus simplement, une optimisation des méthodes existantes ? Constituent-elles un champ suffisamment structuré pour justifier une réforme des modes de financement de la recherche, des cursus de formation, des critères d’évaluation ? L’archive numérique offre-t-elle à la recherche suffisamment de garanties ? Quelle place la recherche « dirigée par les données » laisse-t-elle à l’interprétation ? Telles sont quelques-unes des questions abordées par ce deuxième opus de la collection « Read/Write Book ». Ces dix-huit textes essentiels, rédigés ou traduits en français par des chercheurs de différentes nationalités, proposent une introduction aux humanités numériques accessible à tous ceux qui souhaitent en savoir plus sur ce domaine de recherche en constante évolution.},
	publisher = {{OpenEdition} Press},
	editor = {Mounier, Pierre},
	urldate = {2013-10-19},
	date = {2012},
	langid = {french},
}

@article{brown_reading_2011,
	title = {Reading Orlando with the Mandala Browser: A Case Study in Algorithmic Criticism via Experimental Visualization},
	volume = {2},
	issn = {1918-3666},
	url = {http://www.digitalstudies.org/ojs/index.php/digital_studies/article/view/191},
	shorttitle = {Reading Orlando with the Mandala Browser},
	abstract = {This paper describes the preliminary results of combining two complementary technologies: Orlando, a semantically-tagged {XML} collection of born-digital scholarly resources, and the Mandala Browser, an {XML} visualization tool. Orlando's current delivery system privileges text as an approach to literary historical scholarship. The Mandala browser represents a radically different way of mediating between the user and the text, translating a text or set of texts into a circular visual form and pushing the user towards a more distant, or at least a more selective, reading of the materials than that associated with conventional print or screen rendering. Through experimental visualizations of Orlando content, we began to address questions concerning the participation of Victorian and Renaissance writers in various genres, the relationship between reproduction and literary production, the connection of censorship to the destruction of literary works, and the relationship between suffrage and liberal or conservative political groups. We argue that, just as a postcolonialist or a new historicist needs to learn about the tenets and processes involved in a postcolonial or new historical critical framework, so too an algorithmic critic should expect to invest some time learning the techniques of a given approach and how to apply them to a particular text or body of texts. These investigations may interest other humanities scholars working with online digital collections, as well as those thinking through the question of how to involve computational processes in complex inquiries using large quantities of texts.},
	number = {1},
	journaltitle = {Digital Studies / Le champ numérique},
	shortjournal = {{DS}/{CN}},
	author = {Brown, Susan Irene and Ruecker, Stan and Antoniuk, Jeffery and Farnel, Sharon and Gooding, Matt and Sinclair, Stéfan and Patey, Matt and Gabriele, Sandra},
	date = {2011-05-17},
	langid = {english},
	keywords = {act\_Visualizing, bigdata, goal\_Analysis},
}

@article{marniemi_radiochemical_1975,
	title = {Radiochemical assay of glutathione S-epoxide transferase and its enhancement by phenobarbital in rat liver in vivo},
	volume = {24},
	issn = {0006-2952},
	url = {http://www.sciencedirect.com/science/article/pii/0006295275900805},
	abstract = {A rapid and sensitive assay of glutathione S-epoxide transferase was developed using tritium-labeled styrene oxide as the epoxide substrate. The method is based on the separation of unreacted styrene oxide from the glutathione conjugate formed after the incubation by the extraction with light petroleum. The radioactivity of the conjugate remaining in the aqueous phase can be determined subsequently by liquid scintillation. Using the present radiochemical method we found, after the intraperitoneal administration of phenobarbital (80 mg/kg) in vivo, a significant enhancement (about 56\%) in the conjugation of styrene oxide with glutathione catalyzed by rat liver soluble fraction. No enhancement in the enzyme activity could be detected, on the other hand, after administration with styrene (1 g/kg) or 20-methylcholanthrene (20 mg/kg).},
	pages = {1569--1572},
	number = {17},
	journaltitle = {Biochemical pharmacology},
	shortjournal = {Biochem. Pharmacol.},
	author = {Marniemi, J and Parkki, M G},
	date = {1975-09-01},
	langid = {english},
}

@book{jones_emergence_2013,
	location = {New York},
	title = {The Emergence of the Digital Humanities},
	isbn = {9780415635516},
	url = {http://www.routledge.com/books/details/9780415635523/},
	abstract = {The past decade has seen a profound shift in our collective understanding of the digital network. What was once understood to be a transcendent virtual reality is now experienced as a ubiquitous grid of data that we move through and interact with every day, raising new questions about the social, locative, embodied, and object-oriented nature of our experience in the networked world.   In The Emergence of the Digital Humanities, Steven E. Jones examines this shift in our relationship to digital technology and the ways that it has affected humanities scholarship and the academy more broadly. Based on the premise that the network is now everywhere rather than merely "out there," Jones links together seemingly disparate cultural events—the essential features of popular social media, the rise of motion-control gaming and mobile platforms, the controversy over the "gamification" of everyday life, the spatial turn, fabrication and 3D printing, and electronic publishing—and argues that cultural responses to changes in technology provide an essential context for understanding the emergence of the digital humanities as a new field of study in this millennium.},
	pagetotal = {224},
	publisher = {Routledge},
	author = {Jones, Steven E.},
	date = {2013},
	langid = {english},
	keywords = {meta\_Assessing, meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{markham_undermining_2013,
	title = {Undermining ‘data’: A critical examination of a core term in scientific inquiry},
	volume = {18},
	rights = {Authors submitting a paper to First Monday automatically agree to confer a limited license to First Monday if and when the manuscript is accepted for publication. This license allows First Monday to publish a manuscript in a given issue. Authors have a choice of: 1. Dedicating the article to the public domain. This allows anyone to make any use of the article at any time, including commercial use. A good way to do this is to use the Creative Commons Public Domain Dedication Web form; see  http://creativecommons.org/license/publicdomain-2?lang=en . 2. Retaining some rights while allowing some use. For example, authors may decide to disallow commercial use without permission. Authors may also decide whether to allow users to make modifications (e.g. translations, adaptations) without permission. A good way to make these choices is to use a Creative Commons license. * Go to  http://creativecommons.org/license/ . * Choose and select a license. * What to do next — you can then e–mail the license html code to yourself. Do this, and then forward that e–mail to First Monday’s editors. Put your name in the subject line of the e–mail with your name and article title in the e–mail. Background information about Creative Commons licenses can be found at  http://creativecommons.org/about/licenses/ . 3. Retaining full rights, including translation and reproduction rights. Authors may use the statement: © Author 2008 All Rights Reserved. Authors may choose to use their own wording to reserve copyright. If you choose to retain full copyright, please add your copyright statement to the end of the article. Authors submitting a paper to First Monday do so in the understanding that Internet publishing is both an opportunity and challenge. In this environment, authors and publishers do not always have the means to protect against unauthorized copying or editing of copyright–protected works.},
	issn = {13960466},
	url = {http://firstmonday.org/ojs/index.php/fm/article/view/4868},
	doi = {10.5210/fm.v18i10.4868},
	shorttitle = {Undermining ‘data’},
	abstract = {The term ‘data’ functions as a powerful frame for discourse about how knowledge is derived and privileges certain ways of knowing over others. Through its ambiguity, the term can foster a self–perpetuating sensibility that ‘data’ is incontrovertible, something to question the meaning or the veracity of, but not the existence of. This article critically examines the concept of ‘data’ within larger questions of research method and frameworks for scientific inquiry. The current dominance of the term ‘data’ and ‘big data’ in discussions of scientific inquiry as well as everyday advertising focuses our attention on only certain aspects of the research process. The author suggests deliberately decentering the term, to explore nuanced frames for describing the materials, processes, and goals of inquiry.},
	number = {10},
	journaltitle = {First Monday},
	author = {Markham, Annette N.},
	urldate = {2014-01-28},
	date = {2013-09-21},
	langid = {english},
}

@online{collective_young_2013,
	title = {Young Researchers in Digital Humanities: A Manifesto},
	url = {http://dhdhi.hypotheses.org/1855},
	shorttitle = {Young Researchers in Digital Humanities},
	titleaddon = {Digital Humanities am {DHIP}},
	author = {{Collective}},
	urldate = {2013-07-08},
	date = {2013-07-04},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{lusignan_quelques_1985,
	title = {Quelques réflexions sur le statut épistémologique du texte électronique},
	volume = {19},
	url = {http://www.jstor.org/discover/10.2307/30200019?uid=3737864&uid=2&uid=4&sid=21104991406237},
	pages = {209--212},
	journaltitle = {Computers and the Humanities},
	author = {Lusignan, S.},
	date = {1985},
	langid = {french},
	keywords = {meta\_Assessing, meta\_Theorizing},
}

@report{allison_quantitative_2011,
	location = {Stanford},
	title = {Quantitative Formalism: an Experiment (Standford Literary Lab, Pamphlet 1)},
	url = {http://litlab.stanford.edu/LiteraryLabPamphlet1.pdf},
	abstract = {This paper is the report of a study conducted by five people – four at Stanford, and one at the University of Wisconsin – which tried to establish whether computer-generated algorithms could “recognize” literary genres. You take David Copperfield, run it through a program without any human input – “unsupervised”, as the expression goes – and … can the program figure out whether it’s a gothic novel or a Bildungsroman? The answer is, fundamentally, Yes: but a Yes with so many complications that make it necessary to look at the entire process of our study. These are new methods we are using, and with new methods the process is almost as important as the results.},
	institution = {Standford Literary Lab},
	author = {Allison, Sarah and Heuser, Ryan and Jockers, Matthew L. and Moretti, Franco and Witmore, Michael},
	date = {2011-01-15},
	langid = {english},
	keywords = {{AnalyzeStatistically}, goal\_Analysis},
}

@article{michel_quantitative_2011,
	title = {Quantitative Analysis of Culture Using Millions of Digitized Books},
	volume = {331},
	url = {http://www.sciencemag.org/content/331/6014/176.abstract},
	doi = {10.1126/science.1199644},
	abstract = {We constructed a corpus of digitized texts containing about 4\% of all books ever printed. Analysis of this corpus enables us to investigate cultural trends quantitatively. We survey the vast terrain of ‘culturomics,’ focusing on linguistic and cultural phenomena that were reflected in the English language between 1800 and 2000. We show how this approach can provide insights about fields as diverse as lexicography, the evolution of grammar, collective memory, the adoption of technology, the pursuit of fame, censorship, and historical epidemiology. Culturomics extends the boundaries of rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities.},
	pages = {176 --182},
	number = {6014},
	journaltitle = {Science},
	author = {Michel, Jean-Baptiste and Shen, Yuan Kui and Aiden, Aviva Presser and Veres, Adrian and Gray, Matthew K. and The Google Books Team and Pickett, Joseph P. and Hoiberg, Dale and Clancy, Dan and Norvig, Peter and Orwant, Jon and Pinker, Steven and Nowak, Martin A. and Aiden, Erez Lieberman},
	urldate = {2011-11-20},
	date = {2011-01-14},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@report{crow_publishing_2006,
	location = {Washington, D.C.},
	title = {Publishing cooperatives: An alternative for society publishers: A {SPARC} discussion paper},
	url = {http://www.sparc.arl.org/sites/default/files/media_files/cooperatives_v1-2.pdf},
	abstract = {The author thanks the Scholarly Publishing and Academic Resources Coalition and {SPARC}
Director Heather Joseph for funding the research on which this paper is based. The author
also thanks the following people for their feedback on previous drafts of this paper: Karla
L. Hahn (Director of the Offi ce of Scholarly Communication, the Association of Research
Libraries), Bruce L. Anderson (Professor Emeritus, Business Management and Marketing,
Cornell University), Brian M. Henehan (Senior Extension Associate, Department of Applied
Economics and Management, Cornell University), Howard Goldstein (Senior Consultant,
{SPARC} Consulting Group), John Willinsky (Professor, Language and Literacy Education,
University of British Columbia), Rebecca Simon (Assistant Director for Journals Publishing,
University of California Press), Tom Moritz (Associate Director for Administration of the
Getty Research Institute and Chief, Knowledge Management), David Prosser (Director,
{SPARC} Europe), Rick Johnson ({SPARC} Senior Advisor), and Alison Buckholtz ({SPARC}
Communications Consultant). Their insightful comments have strengthened and clarifi ed
the presentation of the proposal set forth here. Any errors of fact or misinterpretation that
remain are the sole responsibility of {SPARC} and the author.
{SPARC} thanks James P. {McGinty}, Vice-Chairman of the Cambridge Information Group,
for generously extending access to the
Ulrich’s Periodicals Directory
via ulrichsweb.com for
Ulrich’s Periodicals Directory
v
i
a
u
l
r
i
c
h
s
w
e
b
.
c
o
m
f
o
r
Ulrich’s Periodicals Directory
the purpose of the market analysis provided in this paper, and Laurie Kaplan of R.R. Bowker
for providing guidance and support in the use of
Ulrich’s
. Responsibility for the construction
of the search queries and interpretation of the results remains that of the autho},
	institution = {Scholarly Publishing and Academic Resources Coalition},
	author = {Crow, R},
	date = {2006},
	langid = {english},
}

@incollection{meister_projekt_2005,
	location = {München},
	title = {Projekt Computerphilologie. Über Geschichte, Verfahren und Theorie rechnergestützter Literaturwissenschaft},
	isbn = {9783770541799},
	url = {http://www1.uni-hamburg.de/DigiLit/meister/computerphilologie_druck.html},
	shorttitle = {Digitalität und Literalität},
	booktitle = {Digitalität und Literalität : zur Zukunft der Literatur},
	publisher = {Fink},
	author = {Meister, Jan Christoph},
	date = {2005},
	langid = {german},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, obj\_Literature},
}

@article{milic_progress_1991,
	title = {Progress in Stylistics: Theory, Statistics, Computers},
	volume = {25},
	url = {http://link.springer.com/article/10.1007/BF00141189?no-access=true},
	doi = {10.1007/BF00141189},
	abstract = {This paper attempts to assess the progress made in computational stylistics dyring the course of the past twenty-five years. First, we discuss some theoretical notions of style, and then we sketch certain trends that emerge from relevant articles appearing in a variety of publications including conference proceedings and academic journals (other than {CHum}). The conclusion is that progress has been mixed.},
	pages = {393--400},
	journaltitle = {Computers and the Humanities},
	author = {Milic, Louis},
	date = {1991},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview},
}

@article{hofman_probabilistic_1999,
	title = {Probabilistic latent semantic analysis},
	url = {http://cs.brown.edu/~th/papers/Hofmann-UAI99.pdf},
	pages = {8},
	journaltitle = {Uncertainty in Artificial Intelligence},
	author = {Hofman, T.},
	date = {1999},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview},
}

@book{lutoslawski_principes_1898,
	location = {Paris},
	title = {Principes de stylométrie appliqués a la chonologie des oeuvres de Platon},
	url = {http://books.google.de/books/about/Principes_de_stylom%C3%A9trie_appliqu%C3%A9s_%C3%A0.html?id=fgQXYAAACAAJ&redir_esc=y},
	pagetotal = {21},
	publisher = {Ernets Leroux},
	author = {Lutoslawski, Wincenty},
	date = {1898},
	langid = {french},
}

@article{terras_present_2011,
	title = {Present, not voting: Digital Humanities in the Panopticon: closing plenary speech, Digital Humanities 2010},
	volume = {26},
	url = {http://llc.oxfordjournals.org/content/26/3/257.abstract},
	doi = {10.1093/llc/fqr016},
	shorttitle = {Present, not voting},
	abstract = {Digital Humanities faces many issues in the current financial and educational climate. In this closing plenary from the Digital Humanities conference 2010 at King’s College London, major concerns about the current role and function of Digital Humanities are raised, demonstrating the practical and theoretical aspects of Digital Humanities research in regard to an individual project at University College London: Transcribe Bentham. It is suggested that those in the Digital Humanities have to be more aware of our history, impact, and identity, if the discipline is to continue to flourish in tighter economic climes, and that unless we maintain and establish a more professional attitude towards our scholarly outputs, we will remain ‘present, not voting’ within the academy. The plenary ends with suggestions as to how the individual, institution, and funding body can foster and aid the Digital Humanities, ensuring the field’s relevance and impact in today’s academic culture. This article is a transcript of what was planned to be said at {DH}2010, although the spoken plenary digresses from the following in places. The video of the speech can be viewed at http://www.arts-humanities.net/video/dh2010\_keynote\_melissa\_terras\_present\_not\_voting\_digital\_humanities\_panopticon.},
	pages = {257 --269},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	author = {Terras, Melissa},
	urldate = {2011-08-19},
	date = {2011},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{timothy_allen_plundering_2010,
	title = {Plundering Philosophers: Identifying Sources of the Encyclopédie},
	url = {http://hdl.handle.net/2027/spo.3310410.0013.107},
	shorttitle = {Plundering Philosophers},
	abstract = {Denis Diderot and Jean le Rond d’Alembert’s Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers stands as one of the crowning achievements of the French Enlightenment. This monumental work, containing some 77,000 articles written by no less than 140 contributors, was published in Paris between 1751 and 1772 in seventeen in-folio volumes of text and eleven volumes of engravings. As with all reference works, the authors and editors of the Encyclopédie made extensive use of a vast array of contemporary reference works and scholarship to complete their massive compendium of enlightened knowledge. The identification of sources material used by the philosophes is a massive undertaking in itself, as the authors rarely acknowledged the works upon which they relied in writing their contributions. This paper describes two different experiments to identify sources of the Encyclopédie. The first applies the "Vector Space Model" ({VSM}) to identify articles that may have been borrowed from the Dictionnaire de Trévoux (1743) – an intellectual rival of the Encyclopédie compiled by French Jesuits in the first half of the 18th century. We find that the Vector Space Model can be an effective means of identifying "similar" passages in documents, in this case, potentially borrowed articles that were then examined by human evaluators. Overall, we conclude that 5.32 percent of all of the articles in the Encyclopédie that were examined were borrowed from the Jesuit critics of the philosophes. The second experiment, building on the first, applies what we call Pairwise Alignment of Intertextual Relations ({PAIR}) to detect passages borrowed from another important predecessor of the Encyclopédie, Louis Moréri's popular Grand dictionnaire historique (1671-1759), which was also a product of Jesuit scholarship. Given the genealogical character of the Moréri dictionary, which represented an understanding of knowledge radically different than that of the encyclopédistes, we were nonetheless able to identify more than 400 shared passages between the two works using the {PAIR} approach. These findings shed new light on the composition process of the Encyclopédie and suggest that the intellectual battle lines between the Jesuits and the philosophes may not have been as firmly established as previously understood. We conclude by outlining improvements to both the {VSM} and {PAIR} models, which we expect will make further identification of similar passages more effective.},
	journaltitle = {Journal of the Association for History and Computing},
	author = {Timothy Allen, Charles Cooney},
	date = {2010-06-18},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, act\_RelationalAnalysis, bigdata},
}

@article{rietveld_pitfalls_2004,
	title = {Pitfalls in Corpus Research},
	volume = {38},
	issn = {1574-020X},
	url = {http://www.springerlink.com/openurl.asp?id=doi:10.1007/s10579-004-1919-1},
	doi = {10.1007/s10579-004-1919-1},
	abstract = {This paper discusses some pitfalls in corpus research and suggests solutions on the basis of examples and computer simulations. We first address reliability problems in language transcriptions, agreement between transcribers, and how disagreements can be dealt with. We then show that the frequencies of occurrence obtained from a corpus cannot always be analyzed with the traditional χ 2 test, as corpus data are often not sequentially independent and unit independent. Next, we stress the relevance of the power of statistical tests, and the sizes of statistically significant effects. Finally, we point out that a t-test based on log odds often provides a better alternative to a χ 2 analysis based on frequency counts.},
	pages = {343--362},
	number = {4},
	journaltitle = {Computers and the Humanities},
	shortjournal = {Comput Hum},
	author = {Rietveld, Toni and hout, Roeland Van and Ernestus, Mirjam},
	urldate = {2011-05-23},
	date = {2004-11},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Literature},
}

@article{meyer_perspektiven_2012,
	title = {Perspektiven der Fachinformation und -kommunikation: Virtuelle Forschungsumgebungen in den Geschichtswissenschaften},
	url = {http://universaar.uni-saarland.de/journals/index.php/zdg/article/view/290},
	shorttitle = {Perspektiven der Fachinformation und -kommunikation},
	abstract = {Perspektiven der Fachinformation und -kommunikation: Virtuelle Forschungsumgebungen in den Geschichtswissenschaften},
	number = {1},
	journaltitle = {Zeitschrift für digitale Geschichtswissenschaften},
	author = {Meyer, Thomas},
	urldate = {2012-12-17},
	date = {2012-08-06},
	langid = {german},
	keywords = {act\_Communicating, act\_Discovering, meta\_Assessing, obj\_ResearchResults, obj\_VREs},
}

@article{pattuelli_personal_2012,
	title = {Personal name vocabularies as linked open data: A case study of jazz artist names},
	issn = {0165-5515, 1741-6485},
	url = {http://jis.sagepub.com/cgi/doi/10.1177/0165551512455989},
	doi = {10.1177/0165551512455989},
	shorttitle = {Personal name vocabularies as linked open data},
	abstract = {This article describes the process of constructing a vocabulary of personal names of jazz artists in the form of Linked Open Data ({LOD}). Created as a name directory to support the development of the Linked Jazz project, it provides a case study that demonstrates the value and the challenges of developing a domain-specific vocabulary tool that draws upon the semantics of {DBpedia}, a major {LOD} dataset. The article also addresses possible strategies for enhancing the directory to make it a more robust personal name vocabulary.},
	journaltitle = {Journal of Information Science},
	author = {Pattuelli, M. C.},
	urldate = {2012-10-06},
	date = {2012-10-05},
	langid = {english},
	keywords = {obj\_NamedEntities},
}

@book{boonstra_past_2006,
	location = {Amsterdam},
	title = {Past, present and future of historical information science},
	isbn = {9069844133 9789069844138},
	url = {http://www.dans.knaw.nl/sites/default/files/file/publicaties/Past-present.pdf},
	abstract = {This book evaluates the results of two decades of research in ‘history and computing’. In spite of the fact that a lot has been accomplished, the report indicates critical places for improvement. Many historians and other humanities scholars seem satisfied with standard office tools, which do not always suit their complex sources and research questions. While more and more archival sources have become available in digital form, there has not been enough attention to the development of computational methods to process and analyse them.

As a result, the level of technical sophistication in many historical studies has remained rather low. The authors propose a new research agenda and provide requirements for an adequate research infrastructure for future historical information science.},
	publisher = {Data Archiving and Networked Services},
	author = {Boonstra, Onno and Breure, Leen and Doorn, Peter},
	urldate = {2012-03-21},
	date = {2006},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@book{kee_pastplay:_2014,
	location = {Ann Arbor},
	title = {Pastplay: Teaching and Learning History with Technology},
	isbn = {http://dx.doi.org/10.3998/dh.12544152.0001.001},
	url = {http://www.digitalculture.org/books/pastplay-teaching-and-learning-history-with-technology/},
	series = {Digital Culture},
	abstract = {In the field of history, the Web and other technologies have become important tools in research and teaching of the past. Yet the use of these tools is limited—many historians and history educators have resisted adopting them because they fail to see how digital tools supplement and even improve upon conventional tools (such as books). In Pastplay, a collection of essays by leading history and humanities researchers and teachers, editor Kevin Kee works to address these concerns head-on. How should we use technology? Playfully, Kee contends. Why? Because doing so helps us think about the past in new ways; through the act of creating technologies, our understanding of the past is re-imagined and developed. From the insights of numerous scholars and teachers, Pastplay argues that we should play with technology in history because doing so enables us to see the past in new ways by helping us understand how history is created; honoring the roots of research, teaching, and technology development; requiring us to model our thoughts; and then allowing us to build our own understanding.},
	publisher = {Univ. of Michigan Press},
	author = {Kee, Kevin},
	date = {2014},
	langid = {english},
	keywords = {meta\_Teaching, obj\_DigitalHumanities},
}
@misc{pincemin_panorama_2009,
	title = {Panorama bref et pragmatique des outils de textométrie et apparentés », Fiche réalisée à l’intention des participants, Ecole thématique {CNRS} {MISAT} (Méthodes Informatiques et Statistiques en Analyse de Textes), Besançon, 15-19 juin 2009.},
	url = {http://icar.univ-lyon2.fr/membres/bpincemin/biblio/pincemin_misat09_logiciels.pdf},
	author = {Pincemin, Bénédicte},
	date = {2009},
	langid = {french},
	keywords = {goal\_Analysis, meta\_GiveOverview, obj\_Tools},
}

@article{bouillier_opinion_2012,
	title = {Opinion mining et ‎Sentiment analysis},
	url = {http://press.openedition.org/198},
	abstract = {L’ « opinion mining » est en passe de devenir une véritable industrie, tout aussi stratégique que celle des sondages. Les promesses avancées sont impressionnantes : la puissance de calcul des outils informatiques permettrait de suivre toutes les évolutions de l’opinion sur le web en temps réel, quel qu’en soit le volume. Plus encore, les capacités de traitement linguistique permettraient de détecter les tonalités de tous les verbatims recueillis, grâce aux méthodes dites de « sentiment analysis ». L’état de l’art des offres commerciales et technologiques présenté dans cet ouvrage rend compte de cette effervescence mais en souligne aussi la démesure, en veillant à distinguer les résultats réels des slogans promotionnels quelquefois trompeurs. L’ouvrage écrit par des chercheurs du médialab de Sciences Po, laboratoire spécialisé dans le traitement des masses de données disponibles sur le web pour les sciences sociales, permet aussi de resituer l’intérêt de ces nouveaux moyens techniques pour la recherche, dans le cadre de ce qu’on appelle désormais les « humanités numériques ». Enfin, soucieux de permettre à chaque lecteur de prendre en main ces outils, certes puissants mais aux limites de validité bien réelles, les auteurs décrivent pas à pas toutes les phases d’un projet mobilisant les méthodes d’opinion mining, en précisant les pièges et les impératifs d’intervention de l’expertise humaine, toujours nécessaire. Largement illustré, cet ouvrage devrait encourager les chercheurs, les professionnels de l’opinion et du marketing tout autant que les informaticiens et spécialistes des « web studies » à échanger pour faire progresser ces outils communs.},
	journaltitle = {{OpenEdition}},
	author = {Bouillier, Dominique and Lohard, Audrey},
	urldate = {2012-05-15},
	date = {2012},
	langid = {french},
	keywords = {t\_SentimentAnalysis},
}

@report{frosio_open_2014,
	title = {Open Access Publishing: A Literature Review},
	url = {http://www.univie.ac.at/voeb/blog/?p=32496},
	abstract = {Within the con­text of the Centre for Copyright and New Business Models in the Creative Economy ({CREATe}) rese­arch scope, this lite­ra­ture review inves­ti­ga­tes the cur­rent trends, advan­ta­ges, disad­van­ta­ges, pro­blems and solu­ti­ons, oppor­tu­nities and bar­ri­ers in Open Access Publishing ({OAP}), and in par­ti­cu­lar Open Access ({OA}) aca­de­mic publis­hing.  This study is inten­ded to scope and eva­luate cur­rent theory and prac­tice con­cerning models for {OAP} and engage with intel­lec­tual, legal and eco­no­mic per­spec­tives on {OAP}. It is also aimed at map­ping the field of aca­de­mic publis­hing in the {UK} and abroad, dra­wing spe­ci­fi­cally upon the expe­ri­en­ces of {CREATe} indus­try part­ners as well as other initia­ti­ves such as {SSRN}, open source soft­ware, and Creative Commons. As a final cri­ti­cal goal, this sco­ping study will iden­tify any mea­ningful gaps in the rele­vant lite­ra­ture with a view to deve­lo­ping fur­ther rese­arch ques­ti­ons. The results of this sco­ping exer­cise will then be pre­sen­ted to rele­vant indus­try and aca­de­mic part­ners at a work­shop inten­ded to assist in fur­ther deve­lo­ping the cri­ti­cal rese­arch ques­ti­ons per­ti­nent to {OAP}.},
	institution = {{CREATe}},
	type = {2014/1},
	author = {Frosio, Giancarlo},
	editora = {Derclaye, Estelle},
	editoratype = {collaborator},
	date = {2014},
	langid = {english},
	keywords = {act\_Publishing, goal\_Dissemination, t\_OpenAccess},
}

@collection{mcgann_online_2010,
	title = {Online Humanities Scholarship: The Shape of Things to Come},
	url = {http://cnx.org/content/col11199/latest/},
	shorttitle = {Online Humanities Scholarship},
	abstract = {The papers, responses, and proceedings of the Online Humanities Scholarship: The Shape of Things to Come conference, held at the University of Virginia on March 26-28, 2010.},
	editor = {{McGann}, Jerome},
	urldate = {2012-09-08},
	date = {2010},
	langid = {english},
	keywords = {X-{CHECK}, meta\_GiveOverview, obj\_AnyObject, obj\_DigitalHumanities},
}

@article{craig_old_2010,
	title = {Old spellings, new methods: automated procedures for indeterminate linguistic data},
	volume = {25},
	url = {http://llc.oxfordjournals.org/content/25/1/37.abstract},
	doi = {10.1093/llc/fqp033},
	shorttitle = {Old spellings, new methods},
	abstract = {The authors have worked over several years on a software tool to make word counts from an archive of old-spelling early modern English plays and poems. In this article we present the outcome, a computational model for dealing automatically with variant spelling, implemented in an application which we call an ‘Intelligent Archive’. We also reflect on the perspective on Early Modern English, and on the probabilistic aspect of language in general, gained from working through the practical problems which arose in establishing the model.},
	pages = {37 --52},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	author = {Craig, Hugh and Whipp, R.},
	urldate = {2011-12-14},
	date = {2010-04},
	langid = {english},
	keywords = {act\_Annotating, goal\_Enrichment, obj\_Text},
}

@book{dufrene_numerisation_2013,
	location = {Paris},
	title = {Numérisation du patrimoine: quelles médiations? quels accès? quelles cultures?},
	isbn = {9782705687427 2705687424},
	url = {http://www.amazon.de/Num%C3%A9risation-patrimoine-Quelles-m%C3%A9diations-cultures/dp/2705687424},
	shorttitle = {Numérisation du patrimoine},
	publisher = {Hermann},
	author = {Dufrêne, Bernadette and Ihadjadene, Madjid and Bruckmann, Denis},
	date = {2013},
	langid = {french},
}

@article{burrows_not_1992,
	title = {Not Unless You Ask Nicely: The Interpretative Nexus Between Analysis and Information},
	volume = {7},
	url = {http://llc.oxfordjournals.org/content/7/2/91.abstract},
	doi = {10.1093/llc/7.2.91},
	shorttitle = {Not Unles You Ask Nicely},
	abstract = {Computer-based evidence, especially when it incorporates statistical analysis, is too often regarded with special deference or special scepticism. It is better assessed upon its merits, like any other application of inductive logic. The nexus between the inductive process and the information available is studied in two paradigmatic attempts to interpret sets of statistically based distinctions between different texts.},
	pages = {91 --109},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Burrows, John F.},
	urldate = {2011-12-08},
	date = {1992-01},
	langid = {english},
	keywords = {meta\_Theorizing, t\_Stylometry},
}

@article{rudman_non-traditional_1998,
	title = {Non-traditional Authorship Attribution Studies in the Historia Augusta: Some Caveats},
	volume = {13},
	url = {http://llc.oxfordjournals.org/content/13/3/151.abstract},
	doi = {10.1093/llc/13.3.151},
	shorttitle = {Non-traditional Authorship Attribution Studies in the Historia Augusta},
	abstract = {This article discussion some of the problems that are inherent in the non-traditional authorship attribution studies of {theHistoria} Augusta those using statistics, stylistics, and the computer) and some of the problems that I feel are due to practioner error in these studies. Although several problems with some of the studies are pointed out, the major problem and, therefore, the major thrust of the paper is the primary data—the text itself.},
	pages = {151 --157},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	author = {Rudman, Joseph},
	urldate = {2011-12-14},
	date = {1998},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Theorizing, t\_Stylometry},
}

@incollection{zollner-weber_noctua_2007,
	location = {Tübingen},
	title = {Noctua literaria - A System for a Formal Description of Literary Characters},
	url = {http://pub.uni-bielefeld.de/luur/download?func=downloadFile&recordOId=2301795&fileOId=2301798},
	abstract = {Literary characters are just as old as the stories they appear in. They form the active part
of narration and are important to drive the plot further. Often, characters are remembered
longer than the story they belong to. One can assume that many readers are fascinated,
maybe even inspired by them.1 Several characters, or parts of them, survived centuries and
were transported from oral tales to written stories. Thus, characters like the Knights of the
Round Table, Doctor Faustus or creatures of fairy tales still appear in current stories or film
productions.},
	pages = {113--12},
	booktitle = {Data Structures for Linguistic Resources and Applications},
	author = {Zöllner-Weber, Amélie},
	editor = {Rehm, Georg and Witt, Andreas and Lemnitzer, Lothar},
	date = {2007},
	langid = {english},
	keywords = {act\_Conceptualizing, act\_Modeling, goal\_Analysis, goal\_Interpretation, obj\_Literature},
}

@collection{clir_working_2009,
	location = {Washington, {DC}},
	title = {Working Together or Apart: Promoting the Next Generation of Digital Scholarship},
	isbn = {978-1-932326-33-8},
	url = {http://www.clir.org/pubs/reports/pub145/pub145.pdf},
	publisher = {Council on Library and Information Resources ({CLIR})},
	editor = {{CLIR}},
	date = {2009},
	langid = {english},
	note = {Report of a Workshop Cosponsored by the Council on Library and Information Resources and The National Endowment for the Humanities},
	keywords = {meta\_Advocating, meta\_Assessing, obj\_Infrastructures},
}

@online{dougherty_writing_2011,
	title = {Writing History in the Digital Age},
	url = {http://writinghistory.trincoll.edu/},
	abstract = {Has the digital revolution transformed how we write about the past — or not? Have new technologies changed our essential work-craft as scholars, and the ways in which we think, teach, author, and publish? Does the digital age have broader implications for individual writing processes, or for the historical profession at large? Explore these questions in Writing History in the Digital Age, an open peer-reviewed volume published in open-access online format (for free) and in print (for sale) from the University of Michigan Press, as part of its Digital Humanities Series and the digitalculturebooks imprint.},
	author = {Dougherty, Jack and Nawrotzki, Kristen},
	urldate = {2011-11-18},
	date = {2011-05-22},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{tweedie_neural_1996,
	title = {Neural network applications in stylometry: The Federalist Papers},
	volume = {30},
	issn = {0010-4817, 1572-8412},
	url = {http://www.springerlink.com/content/v4572742727jh052/},
	doi = {10.1007/BF00054024},
	shorttitle = {Neural network applications in stylometry},
	abstract = {Neural Networks have recently been a matter of extensive research and popularity. Their application has increased considerably in areas in which we are presented with a large amount of data and we have to identify an underlying pattern. This paper will look at their application to stylometry. We believe that statistical methods of attributing authorship can be coupled effectively with neural networks to produce a very powerful classification tool. We illustrate this with an example of a famous case of disputed authorship, The Federalist Papers. Our method assigns the disputed papers to Madison, a result which is consistent with previous work on the subject.},
	pages = {1--10},
	number = {1},
	journaltitle = {Computers and the Humanities},
	author = {Tweedie, F. J. and Singh, S. and Holmes, D. I.},
	urldate = {2011-12-08},
	date = {1996},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@article{merriam_neural_1994,
	title = {Neural Computation in Stylometry {II}: An Application to the Works of Shakespeare and Marlowe},
	volume = {9},
	url = {http://llc.oxfordjournals.org/content/9/1/1.abstract},
	doi = {10.1093/llc/9.1.1},
	shorttitle = {Neural Computation in Stylometry {II}},
	abstract = {Using principles set out in an earlier paper, a neural network was constructed to discriminate between the works of Shakespeare and his contemporary Christopher Marlowe. Once trained using works from the core canon of the two dramatists, the network successfully classified works to which it had not been previously exposed. In the light of these favourable results, we used the network to classify a number of anonymous works. Strong support emerged for Tucker Brooke's view that The True Tragedy is the Marlovian original of Henry {VI}, Part 3, the latter being the product of subsequent revisions by Shakespeare.},
	pages = {1 --6},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	author = {Merriam, Thomas V. N. and Matthews, Robert a. J.},
	urldate = {2011-12-14},
	date = {1994-01},
	langid = {english},
	keywords = {obj\_Literature, t\_Stylometry},
}

@article{matthews_neural_1993,
	title = {Neural Computation in Stylometry I: An Application to the Works of Shakespeare and Fletcher},
	volume = {8},
	url = {http://llc.oxfordjournals.org/content/8/4/203.abstract},
	doi = {10.1093/llc/8.4.203},
	shorttitle = {Neural Computation in Stylometry I},
	abstract = {We consider the stylometric uses of a pattern recognition technique inspired by neurological research known as neural computation. This involves the training of so-called neural networks to classify data even in the presence of noise and non-linear interactions within data sets. We provide an introduction to this technique, and show how to tailor it to the needs of stylometry. Specifically, we show how to construct so-called multi-layer perceptron neural networks to investigate questions surrounding purported works of Shakespeare and Fletcher. The Double Falsehood and The London Prodigal are found to have strongly Fletcherian characteristics, Henry {VIII} strongly Shakespearian characteristics, and The Two Noble Kinsmen characteristics suggestive of collaboration.},
	pages = {203 --209},
	number = {4},
	journaltitle = {Literary and Linguistic Computing},
	author = {Matthews, Robert a. J. and Merriam, Thomas V. N.},
	urldate = {2011-12-14},
	date = {1993-01},
	langid = {english},
	keywords = {obj\_Literature, t\_Stylometry},
}

@book{newman_networks:_2010,
	location = {Oxford ; New York},
	edition = {1 edition},
	title = {Networks: An Introduction},
	isbn = {9780199206650},
	url = {http://global.oup.com/uk/academic/physics/admin/solutions},
	shorttitle = {Networks},
	abstract = {The scientific study of networks, including computer networks, social networks, and biological networks, has received an enormous amount of interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on a large scale, and the development of a variety of new theoretical tools has allowed us to extract new knowledge from many different kinds of networks.The study of networks is broadly interdisciplinary and important developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together for the first time the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas.  Subjects covered include the measurement and structure of networks in many branches of science, methods for analyzing network data, including methods developed in physics, statistics, and sociology, the fundamentals of graph theory, computer algorithms, and spectral methods, mathematical models of networks, including random graph models and generative models, and theories of dynamical processes taking place on networks.To request a copy of the Solutions Manual, visit: http://global.oup.com/uk/academic/physics/admin/solutions},
	pagetotal = {720},
	publisher = {Oxford University Press},
	author = {Newman, Mark},
	date = {2010-05-20},
	langid = {english},
	keywords = {act\_RelationalAnalysis, goal\_Analysis, obj\_Data, t\_NetworkAnalysis},
}

@article{brunet_navigation_2006,
	title = {Navigation dans les rafales},
	url = {http://lexicometrica.univ-paris3.fr/jadt/JADT2006-PLENIERE/JADT2006_EB.pdf},
	abstract = {The chief object of lexicometry sofar has been frequen
cies, little attention being paid to sequences. Yet, the
divisions of a corpus, on which numbers and frequen
cies are based are an a priori notion which should
sometimes be questioned. The study of sequences makes it possible to ignore divisions and follow the progress
of a word or any other linguistic object throughout a corpus without having to stop at the barriers of texts. The
graphic representation of each occurr
ence enables the eye to spot distri
butions in bursts or "rafales", to
appreciate the lulls and
the rhythm of speech. Several tests are sugg
ested and experimented in order to measure
the corresponding distribution, am
ong which the probability measure devi
sed by Pierre Lafon within the
framework of the hypergeometric model. The remaining ta
sk is to compare the distributions of two or several
words and to measure the distances between them. In orde
r to achieve this, the hypergeometric model is again
used rather than other well-known indices such as Dunning's “likelihood ratio" or Church's "mutual
information".},
	pages = {16},
	journaltitle = {Actes {JADT} 2006},
	author = {Brunet, Étienne},
	date = {2006},
	langid = {french},
	keywords = {X-{CHECK}},
}

@article{van_dalenoskam_modelling_2004,
	title = {Modelling Features of Characters: Some Digital Ways to Look at Names in Literary Texts},
	volume = {19},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/19/3/289},
	doi = {10.1093/llc/19.3.289},
	shorttitle = {Modelling Features of Characters},
	abstract = {In the context of ongoing research into new methods and techniques for literary research we describe a primary implementation of a web application called {\textless}it{\textgreater}Autonom{\textless}/it{\textgreater}, intended to be developed into a framework for textual parsing algorithms that may be used by literary researchers to trace literary phenomena in texts. We describe the technical parsing fundamentals and good practices the development of the framework is based upon, we clarify different design considerations and choices and we present an overview of the current state of implementation and functionality. We also demonstrate the application of a proper name parsing algorithm implemented within the framework, meant to be the first step in a new method for the research of names in literary texts. The algorithm is tested on Karel Glastra van Loon’s novel {\textless}it{\textgreater}Lisa’s adem{\textless}/it{\textgreater} ({\textless}it{\textgreater}Lisa’s Breath{\textless}it{\textgreater}, 2001). We go into the results that this test has yielded so far and summarily describe some of the consequences for the analysis of the names in the novel. We conclude with a short description of the directions new developments could take.},
	pages = {289--301},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {van Dalen‐Oskam, Karina and van Zundert, Joris},
	urldate = {2012-02-21},
	date = {2004-09-01},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_Annotating, act\_Modeling, goal\_Analysis, goal\_Interpretation, obj\_Literature},
}

@book{rajaraman_mining_2011,
	title = {Mining of massive datasets},
	url = {http://books.google.com/books?hl=en&lr=&id=OefRhZyYOb0C&oi=fnd&pg=PR5&dq=%22an+algorithmic+point+of+view:+data+mining+is+about+applying%22+%22data,+rather+than+using+data+to+%E2%80%9Ctrain%E2%80%9D+a+machine-learning+engine+of%22+%22The+principal+topics+covered%22+&ots=aMuxkeDvCS&sig=FhC4dp7zMuuQjl2QDDBiukZVngI},
	abstract = {The popularity of the Web and Internet commerce provides many extremely large datasets from which information can be gleaned by data mining. This book focuses on practical algorithms that have been used to solve key problems in data mining and which can be used on even the largest datasets. It begins with a discussion of the map-reduce framework, an important tool for parallelizing algorithms automatically. The authors explain the tricks of locality-sensitive hashing and stream processing algorithms for mining data that arrives too fast for exhaustive processing. The {PageRank} idea and related tricks for organizing the Web are covered next. Other chapters cover the problems of finding frequent itemsets and clustering. The final chapters cover two applications: recommendation systems and Web advertising, each vital in e-commerce. Written by two authorities in database and Web technologies, this book is essential reading for students and practitioners alike.},
	publisher = {Cambridge University Press},
	author = {Rajaraman, A. and Ullman, J. D.},
	urldate = {2012-12-12},
	date = {2011},
	langid = {english},
	keywords = {bigdata},
}

@article{pasanek_mining_2008,
	title = {Mining Millions of Metaphors},
	volume = {23},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/23/3/345},
	doi = {10.1093/llc/fqn010},
	abstract = {One of the first decisions made in any research concerns the selection of an appropriate scale of analysis—are we looking out into the heavens, or down into atoms? To conceive a digital library as a collection of a million books may restrict analysis to only one level of granularity. In this article, we examine the consequences and opportunities resulting from a shift in scale, where the desired unit of interpretation is something smaller than a text: it is a keyword, a motif, or a metaphor. A million books distilled into a billion meaningful components become raw material for a history of language, literature, and thought that has never before been possible. While books herded into genres and organized by period remain irregular, idiosyncratic, and meaningful in only the most shifting and context-dependent ways, keywords or metaphors are lowest common denominators. At the semantic level—the level of words, images, and metaphors—long-term regularity and patterns emerge in collection, analysis, and taxonomy. This article follows the foregoing course of thought through three stages: first, the manual curation of a high quality database of metaphors; second, the expansion of this database through automated and human-assisted techniques; finally, the description of future experiments and opportunities for the application of machine learning, data mining, and natural language processing techniques to help find patterns and meaning concealed at this important level of granularity.},
	pages = {345--360},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Pasanek, Brad and Sculley, D.},
	urldate = {2012-04-19},
	date = {2008-09-01},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, meta\_GiveOverview},
}

@article{horton_mining_2009,
	title = {Mining Eighteenth Century Ontologies: Machine Learning and Knowledge Classification in the Encyclopédie},
	volume = {3},
	url = {http://www.digitalhumanities.org/dhq/vol/3/2/000044.html},
	shorttitle = {Mining Eighteenth Century Ontologies},
	abstract = {The Encyclopédie of Denis Diderot and Jean le Rond d'Alembert was one of the most important and revolutionary intellectual products of the French Enlightenment. Mobilizing many of the great – and the not-so-great – philosophes of the 18th century, the Encyclopédie was a massive reference work for the arts and sciences, which sought to organize and transmit the totality of human knowledge while at the same time serving as a vehicle for critical thinking. In its digital form, it is a highly structured corpus; some 55,000 of its 77,000 articles were labeled with classes of knowledge by the editors making it a perfect sandbox for experiments with supervised learning algorithms. In this study, we train a Naive Bayesian classifier on the labeled articles and use this model to determine class membership for the remaining articles. This model is then used to make binary comparisons between labeled texts from different classes in an effort to extract the most important features in terms of class distinction. Re-applying the model onto the original classified articles leads us to question our previous assumptions about the consistency and coherency of the ontology developed by the Encyclopedists. Finally, by applying this model to another corpus from 18th century France, the Journal de Trévoux, or Mémoires pour l'Histoire des Sciences \& des Beaux-Arts, new light is shed on the domain of Literature as it was understood and defined by 18th century writers.},
	number = {2},
	journaltitle = {Digital Humanities Quarterly},
	author = {Horton, Russell and Morrissey, Robert and Olsen, Mark and Roe, Glenn and Voyer, Robert},
	urldate = {2012-01-08},
	date = {2009-06},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, bigdata, obj\_Knowledge, t\_MachineLearning},
}

@article{anderson_methodological_2010,
	title = {Methodological commons: arts and humanities e-Science fundamentals},
	volume = {368},
	url = {http://rsta.royalsocietypublishing.org/content/368/1925/3779.abstract},
	doi = {10.1098/rsta.2010.0156},
	shorttitle = {Methodological commons},
	abstract = {The application of e-Science technologies to disciplines in the arts and humanities raises major questions as to how those technologies can be most usefully exploited, what tools and infrastructures are needed for that exploitation, and what new research approaches can be generated. This paper reviews a number of activities in the {UK} and Europe in the last 5 years which have sought to address these questions through processes of experimentation and targeted infrastructure development. In the {UK}, the {AHeSSC} (Arts and Humanities e-Science Support Centre) has played a coordinating role for seven projects funded by the Arts and Humanities e-Science Initiative. In Europe, {DARIAH} (Digital Research Infrastructure for the Arts and Humanities) has sought to develop a deeper understanding of research information and communication in the arts and humanities, and to inform the development of e-infrastructures accordingly. Both sets of activity have indicated a common requirement: to construct a framework which consistently describes the methods and functions of scholarly activity which underlie digital arts and humanities research, and the relationships between them. Such a ‘methodological commons’ has been formulated in the field of the digital humanities. This paper describes the application of this approach to arts and humanities e-Science, with reference to the early work of {DARIAH} and {AHeSSC}.},
	pages = {3779 --3796},
	number = {1925},
	journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Anderson, Sheila and Blanke, Tobias and Dunn, Stuart},
	urldate = {2011-07-06},
	date = {2010},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_Methods},
}

@incollection{jannidis_methoden_2010,
	location = {Stuttgart \& Weimar},
	title = {Methoden der computergestützten Textanalyse},
	url = {http://www.amazon.de/Methoden-literatur-kulturwissenschaftlichen-Textanalyse-Modellanalysen/dp/3476021629},
	abstract = {Wie werden Texte wissenschaftlich interpretiert? Kompakt und übersichtlich stellt das Lehrbuch die gängigen literaturtheoretischen Ansätze vor – darunter z. B. Strukturalismus, Dekonstruktion, Diskursanalyse und Systemtheorie. Der Schwerpunkt des übersichtlich gestalteten Bandes liegt auf der Anwendung: Ausführliche Beispielanalysen zeigen, wie mit den verschiedenen Theorien und den daraus abzuleitenden Methoden Texte – und auch Filme – analysiert werden. Unerlässliches Handwerkszeug für das literaturwissenschaftliche Studium.},
	pages = {109--132},
	booktitle = {Methoden der literatur- und kulturwissenschaftlichen Textanalyse},
	publisher = {Metzler},
	author = {Jannidis, Fotis},
	editor = {Nünning, Ansgar and Nünning, Vera},
	date = {2010},
	langid = {german},
}

@article{albrecht_meaux_2011,
	title = {Meaux \& Paris - Mithilfe des 3D-Laserscanverfahrens lösen Forscher Rätsel mittelalterlichen Bauschaffens},
	volume = {17},
	url = {http://www.uni-bamberg.de/fileadmin/uni/verwaltung/presse/Publikationen/uni.vers/univers_forschung_2011/06_Meaux_und_Paris.pdf},
	abstract = {In der Kathedrale von Meaux befindet sich am Südquerhaus ein Portal aus der Zeit um 1280, das sich bei genauerer Betrachtung als Kopie des Staphanusportals an der 30 km entfernten Kathedrale von Paris herausstellt. Umso erstaunlicher ist dies, weil die Kathedrale von Meaux ursprünglich bereits ein gotisches Portal besaß. Eine Bamberger Forschungsgruppe geht diese und andere Fragen zum Bauschaffen der Gotik nun in einem interdisziplinär angelegten Projekt und unter Zuhilfenahme der neuen digitalen Möglichkeiten an. Das Portal in Meaux wurde eingehend untersucht, und zwar zunächst anhand eines 3D-Laserscans.},
	pages = {20--23},
	issue = {Mai 2011},
	journaltitle = {uni.vers, Magazin der Otto-Friedrich-Universität Bamberg},
	author = {Albrecht, Stephan and Breitling, Stefan},
	date = {2011},
	langid = {german},
	keywords = {act\_Visualizing, goal\_Capture, obj\_Architecture},
}

@article{tanner_measuring_2009,
	title = {Measuring Mass Text Digitization Quality and Usefulness: Lessons Learned from Assessing the {OCR} Accuracy of the British Library's 19th Century Online Newspaper Archive},
	volume = {15},
	issn = {1082-9873},
	url = {http://www.dlib.org/dlib/july09/munoz/07munoz.html},
	doi = {10.1045/july2009-munoz},
	shorttitle = {Measuring Mass Text Digitization Quality and Usefulness},
	abstract = {This article will discuss how to measure the accuracy of Optical Character Recognition ({OCR}) output in a way that is relevant to the needs of the end users of digital resources. A case study measuring the {OCR} accuracy of the British Library's 19th Century Newspapers Database provides a clear example of the benefits to be gained from measuring not just character accuracy but also word and significant word accuracy. As {OCR} primarily facilitates searching, indexing and other means of structuring the user experience of online newspaper archives, measuring the word and significant word accuracy of the {OCR} output is very revealing of a resource's likely performance for these functions. Having such data is therefore extremely helpful for planning and quality assurance assessment. After briefly discussing the role of {OCR} in the text capture process and how {OCR} works, we give a detailed description of the methodology, statistical data gathering techniques and analysis used in this study. Our conclusions point the way forward with suggested actions to assist other mass digitization projects in applying these techniques.},
	number = {7},
	journaltitle = {D-Lib Magazine},
	author = {Tanner, Simon and Muñoz, Trevor and Ros, Pich Hemy},
	urldate = {2014-09-05},
	date = {2009},
	langid = {english},
	keywords = {act\_DataRecognition, bigdata{\textasciitilde}, goal\_Capture, obj\_Text},
}

@article{mealand_measuring_1997,
	title = {Measuring genre differences in Mark with correspondence analysis},
	volume = {12},
	url = {http://llc.oxfordjournals.org/content/12/4/227.abstract},
	doi = {10.1093/llc/12.4.227},
	abstract = {This article reports a series of tests on samples from the Gospel of Mark. The first set of results show that groups of samples divided by genre display clear between-group differences. Using correspondence analysis, it is possible to see which variables contribute most to these genre differences. These tests are corroborated by further use of other multivariate statistical tests. These tests established that there are differences, and which function words and other high frequency words are most implicated in those differences. A further analysis of the texts was conducted. This used a program called Word Cluster to extract striking examples of clusters of high frequency words characteristic of each genre. These extracts from the texts show in a more traditional literary way just how effective the result of statistical analysis and text searching systems can be. The entire series of tests show that, as with other literature, so in the gospels, before decisions about authorship are made, attention must be paid to differences of genre. This consideration seems to be particularly acute in the case of Mark, but may affect other literary analyses also. We can show not only that style varies with genre, but also which stylistic markers are used most heavily in which passages. We can also discover other stylistic variation which seems not to be explained by the genre differences examined here.},
	pages = {227 --245},
	number = {4},
	journaltitle = {Literary and Linguistic Computing},
	author = {Mealand, D},
	urldate = {2011-12-14},
	date = {1997-11},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}
@article{koppel_measuring_2007,
	title = {Measuring Differentiability: Unmasking Pseudonymous Authors},
	volume = {8},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1314498.1314541},
	shorttitle = {Measuring Differentiability},
	abstract = {In the authorship verification problem, we are given examples of the writing of a single author and are asked to determine if given long texts were or were not written by this author. We present a new learning-based method for adducing the "depth of difference" between two example sets and offer evidence that this method solves the authorship verification problem with very high accuracy. The underlying idea is to test the rate of degradation of the accuracy of learned models as the best features are iteratively dropped from the learning process.},
	pages = {1261--1276},
	journaltitle = {J. Mach. Learn. Res.},
	author = {Koppel, Moshe and Schler, Jonathan and Bonchek-Dokow, Elisheva},
	urldate = {2012-12-04},
	date = {2007-12},
	langid = {english},
	keywords = {X-{CHECK}},
}

@article{sculley_meaning_2008,
	title = {Meaning and Mining: the Impact of Implicit Assumptions in Data Mining for the Humanities},
	volume = {23},
	url = {http://llc.oxfordjournals.org/cgi/doi/10.1093/llc/fqn019},
	doi = {10.1093/llc/fqn019},
	shorttitle = {Meaning and mining},
	abstract = {As the use of data mining and machine learning methods in the humanities becomes more common, it will be increasingly important to examine implicit biases, assumptions, and limitations these methods bring with them. This article makes explicit some of the foundational assumptions of machine learning methods, and presents a series of experiments as a case study and object lesson in the potential pitfalls in the use of data mining methods for hypothesis testing in literary scholarship. The worst dangers may lie in the humanist's; ability to interpret nearly any result, projecting his or her own biases into the outcome of an experiment—perhaps all the more unwittingly due to the superficial objectivity of computational methods. We argue that in the digital humanities, the standards for the initial production of evidence should be even more rigorous than in the empirical sciences because of the subjective nature of the work that follows. Thus, we conclude with a discussion of recommended best practices for making results from data mining in the humanities domain as meaningful as possible. These include methods for keeping the the boundary between computational results and subsequent interpretation as clearly delineated as possible.},
	pages = {409--424},
	number = {4},
	journaltitle = {Literary and Linguistic Computing},
	author = {Sculley, D. and Pasanek, Bradley M.},
	date = {2008-09-29},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, bigdata, meta\_Theorizing},
}

@article{dambeck_mathematiker_2011,
	title = {Mathematiker setzt lückenhaftes Mega-Puzzle zusammen},
	url = {http://www.spiegel.de/wissenschaft/technik/0,1518,776015,00.html},
	abstract = {Das Fresko in einer Kirche in Padua war fast 1000 Quadratmeter groß und wurde durch einen Bombenangriff 1944 fast vollständig zerstört. 88.000 winzige Putzstücke wurden gerettet - ein Mathematiker konnte sie jetzt zusammensetzen und so ein Zehntel des Meisterwerks restaurieren.},
	journaltitle = {Spiegel Online},
	author = {Dambeck, Holger},
	urldate = {2011-10-18},
	date = {2011-10-18},
	langid = {german},
	keywords = {goal\_Enrichment, obj\_Artefacts, obj\_Images},
}

@article{marusenko_mathematical_2010,
	title = {Mathematical Methods for Attributing Literary Works when Solving the "Corneille-Molière" Problem},
	volume = {17},
	issn = {0929-6174, 1744-5035},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09296170903395924},
	doi = {10.1080/09296170903395924},
	abstract = {This research work focuses on developing a linguistic approach in the field of attribution of literary works using the material of plays written in verse for which Molière is thought to be the author. In this research work, a solution of the problem “Corneille–Molière” is suggested by using methods of mathematical modelling and quantitative description of individual author styles on a syntactic level.},
	pages = {30--54},
	number = {1},
	journaltitle = {Journal of Quantitative Linguistics},
	author = {Marusenko, Mikhail and Rodionova, Elena},
	urldate = {2012-07-28},
	date = {2010-02},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, obj\_Language, obj\_Literature},
}

@article{sebastiani_machine_2002,
	title = {Machine Learning in Automated Text Categorization},
	volume = {34},
	url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.6513},
	abstract = {The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.},
	pages = {1--47},
	journaltitle = {{ACM} Computing Surveys},
	author = {Sebastiani, Fabrizio and Ricerche, Consiglio Nazionale Delle},
	date = {2002},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, goal\_Enrichment, t\_MachineLearning},
}

@article{martindale_utility_1995,
	title = {On the utility of content analysis in author attribution: The Federalist Papers},
	volume = {29},
	url = {http://link.springer.com/article/10.1007/BF01830395},
	doi = {10.1007/BF01830395},
	shorttitle = {On the utility of content analysis in author attribution},
	abstract = {In studies of author attribution, measurement of differential use of function words is the most common procedure, though lexical statistics are often used. Content analysis has seldom been employed. We compare the success of lexical statistics, content analysis, and function words in classifying the 12 {disputedFederalist} papers. Of course, Mosteller and Wallace (1964) have presented overwhelming evidence that all 12 were by James Madison rather than by Alexander Hamilton. Our purpose is not to challenge these attributions but rather to {useThe} Federalist as a test case. We found lexical statistics to be of no use in classifying the disputed papers. Using both classical canonical discriminant analysis and a neural-network approach, content analytic measures — the Harvard {III} Psychosociological Dictionary and semantic differential indices — were found to be successful at attributing most of the disputed papers to Madison. However, a function-word approach is more successful. We argue that content analysis can be useful in cases where the function-word approach does not yield compelling conclusions and, perhaps, in preliminary screening in cases where there are a large number of possible authors.},
	pages = {259--270},
	number = {4},
	journaltitle = {Computers and the Humanities},
	author = {Martindale, Colin and {McKenzie}, Dean},
	date = {1995},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, t\_Stylometry},
}

@online{adema_crowd_2011,
	title = {On crowd funding Open Access scholarly books},
	url = {http://openreflections.wordpress.com/2011/08/03/on-crowd-funding-open-access-scholarly-books/},
	abstract = {With academia increasingly being abused by budget cuts whilst at the same time being overtaken by the language of business, profit, and sustainability, new ways are being sought to gain funds to subsidize academic projects and publications. Especially scholarly publishers within the Humanities and Social Sciences (be they not-for-profit or commercial) have become accustomed to the mixing of and the experimenting with business and revenue models. As the specialized scholarly book has developed into a format from which it has become very hard to gain a profit (mainly due to library budget cuts, the main buyers of academic books), in most cases (cross-) subsidizing schemes are now a necessity for publishers.},
	titleaddon = {Open Reflections},
	author = {Adema, Janneke},
	date = {2011-08-03},
	langid = {english},
}

@online{pannapacker_dark_2013,
	title = {On ‘The Dark Side of the Digital Humanities’},
	url = {http://chronicle.com/blogs/conversation/2013/01/05/on-the-dark-side-of-the-digital-humanities/},
	abstract = {The digital humanities continues to gain to prominence at the Modern Language Association, but it seems like it might be reaching the top of its growth curve.  There was even some talk of what will happen after the “{DH} bubble” bursts.  Mark Sample’s annual list of {DH}-related sessions notes that there are 66 sessions this year, a slight increase over the 58 that were held last year (with 44 and 27 in the two previous years).  It’s still only 8\% of the total number of sessions, but the prominence of {DH} may owe a lot of its visibility to social media and the excited attention it has been given by regular media. At the same time, there seems to be a growing backlash against {DH}, right on schedule.},
	titleaddon = {The Conversation},
	type = {The Chronicle of Higher Education},
	author = {Pannapacker, William},
	urldate = {2013-01-06},
	date = {2013-01-06},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{rudman_non-traditional_2002,
	title = {Non-Traditional Authorship Attribution Studies in Eighteenth Century. Stylistics Statistics and the Computer},
	url = {http://computerphilologie.uni-muenchen.de/jg02/rudman.html},
	abstract = {Non-traditional authorship attribution studies are those attribution studies that make use of the computer, statistics, and stylistics. The hypothesis behind these studies is that an author has a unique and identifiable style. The computer has now become ubiquitous in eighteenth century literary studies and is the main reason why non-traditional authorship studies have advanced to where they are. David Holmes gives a good overview of the field in, The Analysis of Literary Style – A Review.[1] This article surveys a representative sample of authorship studies of eighteenth century literature and gives an exemplum of an ongoing study.},
	pages = {151--166},
	number = {4},
	journaltitle = {Jahrbuch für Computerphilologie},
	author = {Rudman, Joseph},
	date = {2002},
	langid = {english},
	note = {Rudman, Joseph: Non-Traditional Authorship Attribution Studies in Eighteenth Century. Stylistics Statistics and the Computer. In: Jahrbuch für Computerphilologie 4 (2002), S. 151-166.},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview, obj\_Literature, t\_Stylometry},
}

@article{burgess_new_2011,
	title = {New Media in the Academy: Labor and the Production of Knowledge in Scholarly Multimedia},
	volume = {5},
	url = {http://www.digitalhumanities.org/dhq/vol/5/3/000102/000102.html},
	shorttitle = {New Media in the Academy},
	abstract = {Despite a general interest in exploring the possibilities of multimedia and web-based research, the humanities profession has been slow to accept digital scholarship as a valid form of intellectual endeavor. Questions about labor, peer-review, and co-authorship often arise in academic departments’ attempts to evaluate digital research in the tenure and promotion process. In this essay, we argue that these tensions stem from a general misunderstanding of the kinds of "work" that goes into producing scholarship in multimedia form. Multimedia work, we suggest, places scholars in an extended network that combines minds, bodies, machines, and institutional practices, and lays bare the fiction that scholars are disembodied intellectuals who labor only with the mind. We argue that while traditional ideas of what "counts" as scholarship continue to privilege content over form, intellectual labor over physical labor, and print over digital media, new media’s functional (and in some cases even biological) difference from old media contributes to a double erasure, for scholars working in multimedia, of both their intellectual contributions and their material labor.},
	number = {3},
	author = {Burgess, Helen J. and Hamming, Jeanne},
	urldate = {2011-11-22},
	date = {2011},
	langid = {english},
}

@inproceedings{eder_mind_2012,
	location = {Hamburg},
	title = {Mind your corpus: systematic errors in authorship attribution},
	url = {https://sites.google.com/site/computationalstylistics/preprints/m-eder_mind_your_corpus.pdf?attredirects=0},
	abstract = {Non-traditional authorship attribution relies on advanced statistical procedures to distil
significant markers of authorial style from a large pool of stylistic features that are not distinctive
enough to provide reliable information about authorial uniqueness. In other words, the goal is to
find as much order in ‘randomness’ as possible. The better the method applied, the more regularities
can be extracted from a population that seems to contain nothing but noise. However, it does not
mean that one can overcome the impact of randomness: noise is an inherent feature of all natural
languages. In particular, word frequencies in a corpus are random variables; the same can be said
about any written authorial text, like a novel or poem.},
	eventtitle = {Digital Humanities 2012},
	pages = {181--185},
	booktitle = {Digital Humanities 2012: Conference Abstracts},
	publisher = {Hamburg Univ. Press},
	author = {Eder, Maciej},
	date = {2012},
	langid = {english},
}

@online{coombs_markup_1987,
	title = {Markup Systems and the Future of Scholarly Text Processing},
	url = {http://xml.coverpages.org/coombs.html},
	abstract = {In the last few years, scholarly text processing has entered a reactionary stage. Previously, developers were working toward systems that would support scholars in their roles as researchers and authors. Building on the ideas of Vannevar Bush, people such as Theodor H. Nelson and Andries van Dam (Drucker, van Dam, Yankelovitch) prototyped systems designed to parallel the associative thought processes of researching scholars. Similarly, Douglas C. Engelbart sought to augment human intellect by providing concept-manipulation aids (Engelbart and English, Engelbart et al.). Brian K. Reid developed Scribe, freeing authors from formatting concerns and providing them with integrated tools for bibliography and citation management. Although only a small percentage of scholars were exposed to these ideas, the movement was toward developing new strategies for research and composition.},
	titleaddon = {Cover Pages. Online resource for markup language technologies},
	author = {Coombs, James H. and Renear, Allen H. and {DeRose}, Steven J.},
	date = {1987},
	langid = {english},
	keywords = {meta\_Theorizing, t\_Encoding},
}

@online{mccarty_mapping_2002,
	title = {Mapping the field},
	url = {http://www.eadh.org/mapping-field},
	abstract = {Mapping, as we all know, is a powerful tool for representing a complex terrain so that we can apprehend it as a whole and quickly grasp the interrelation of its parts. A “roadmap” is of course for going places, but prior to that, it’s for seeing what the choices are and what might be involved in getting there. Prior again to that, as recent work in the history of mapping has argued, the making of a map takes possession of and literally deﬁnes the mapped terrain: features are named (or re-named), relationships shown, boundaries indicated, unknown parts labelled – and so marked for exploration.},
	titleaddon = {{EADH}},
	author = {{McCarty}, Willard and Short, Harold},
	date = {2002},
	langid = {english},
}

@book{corti_managing_2014,
	location = {Thousand Oaks, {CA}},
	title = {Managing and Sharing Research Data: A Guide to Good Practice},
	isbn = {9781446267264},
	url = {http://www.sagepub.com/books/Book240297?subject=B00&fs=1#tabview=title},
	shorttitle = {Managing and Sharing Research Data},
	abstract = {Research funders in the {UK}, {USA} and across Europe are implementing data management and sharing policies to maximize openness of data, transparency and accountability of the research they support. Written by experts from the {UK} Data Archive with over 20 years experience, this book gives post-graduate students, researchers and research support staff the data management skills required in today’s changing research environment.   The book features guidance on:      how to plan your research using a data management checklist     how to format and organize data     how to store and transfer data     research ethics and privacy in data sharing and intellectual property rights     data strategies for collaborative research     how to publish and cite data     how to make use of other people’s research data, illustrated with six real-life case studies of data use.},
	pagetotal = {240},
	publisher = {{SAGE} Publications Ltd},
	author = {Corti, Louise and Bishop, Libby and Woollard, Matthew},
	date = {2014-03-20},
	langid = {english},
	keywords = {act\_Archiving, act\_Conceptualizing, act\_Organizing, act\_Sharing, goal\_Storage, obj\_Data},
}

@article{king_making_2000,
	title = {Making the Most of Statistical Analyses: Interpretation and Presentation},
	volume = {44},
	url = {http://gking.harvard.edu/files/making.pdf},
	abstract = {Social Scientists rarely take full advantage of the information available in their statistical results. As a consequence, they miss opportunities to present quantities that are of greatest substantive interest for their research and express the appropriate degree of certainty about these quantities. In this article, we offer an approach, built on the technique of statistical simulation, to extract the currently overlooked information from any statistical method and to interpret and present it in a reader-friendly manner. Using this technique requires some expertise, which we try to provide herein, but its application should make the results of quantitative articles more informative and transparent. To illustrate our recommendations, we replicate the results of several published works, showing in each case how the authors’ own conclusions can be expressed more sharply and informatively, and, without changing any data or statistical assumptions, how our approach reveals important new information about the research questions at hand. We also offer very easy-to-use Clarify software that implements our suggestions},
	pages = {341--355},
	number = {2},
	journaltitle = {American Journal of Political Science},
	author = {King, Gary and Tomz, Michael},
	date = {2000},
	langid = {english},
}

@online{prescott_making_2012,
	title = {Making the Digital Human: Anxieties, Possibilities, Challenges},
	url = {http://digitalriffs.blogspot.co.uk/2012/07/making-digital-human-anxieties.html},
	abstract = {The idea that texts could be arbitrarily arranged according to an abstract system such as the letters of the alphabet was a startling one in the middle ages, which had previously sought in arranging texts to illustrate their relationship to the natural order. But the distinctiones showed the advantages of more abstract methods, and they paved the way for the first concordance to the scriptures, which was compiled under the supervision of the Dominican Hugh of St Cher between 1235 and 1249 at the Dominican monastery of St Jacques in Paris. This is a manuscript of the first verbal concordance from St Jacques. The creation of this concordance, which organized every word in the bible alphabetically, was one of the greatest-ever feats of information engineering. It is said that about 500 Dominicans worked on compiling the concordance.},
	titleaddon = {Digital Riffs},
	author = {Prescott, Andrew},
	date = {2012-07-05},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@online{kelly_making_2008,
	title = {Making Digital Scholarship Count},
	url = {http://edwired.org/2008/06/13/making-digital-scholarship-count/},
	abstract = {Today I am inaugurating an extended series of posts on the question of how digital scholarship should “count” in the ways that things count at colleges and universities. As more and more scholars do work in the digital environment they are expecting this work to count toward tenure, promotion, and other types of formal evaluation (in the hiring process, for instance). It is because I have attended numerous conference sessions and meetings over the past several years where this topic has come up (often in very passionate ways), that I have decided to write this series.},
	titleaddon = {edwired},
	author = {Kelly, T. Mills},
	urldate = {2011-11-28},
	date = {2008-06-13},
	langid = {english},
}

@online{moulin_zum_2014,
	title = {Zum Buchbegriff in der Diskussion um das digitale Publizieren in den Geisteswissenschaften. Überlegungen auch aus linguistischer und mediävistischer Sicht},
	url = {http://annotatio.hypotheses.org/376},
	titleaddon = {annot@tio},
	author = {Moulin, Claudine},
	urldate = {2014-05-08},
	date = {2014-04-06},
	langid = {german},
	keywords = {act\_Publishing, goal\_Dissemination, obj\_ResearchResults},
}

@book{murphy_machine_2012,
	location = {Cambridge, {MA}},
	title = {Machine learning: a probabilistic perspective},
	isbn = {9780262018029},
	url = {http://www.cs.ubc.ca/~murphyk/MLbook/},
	series = {Adaptive computation and machine learning series},
	shorttitle = {Machine learning},
	abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a {MATLAB} software package--{PMTK} (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
	pagetotal = {1067},
	publisher = {{MIT} Press},
	author = {Murphy, Kevin P.},
	date = {2012},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@book{numerico_umanista_2010,
	location = {Bologna},
	title = {L'umanista digitale},
	isbn = {9788815134257},
	url = {http://books.google.it/books?id=78sNQgAACAAJ},
	series = {Itinerari: Scienze umanistiche},
	abstract = {{II} manuale mostra come l'informatica, fin dalla sua nascita, si sia nutrita di un ricco contributo interdisciplinare proveniente da filosofi, linguisti, filologi, antropologi, psicologi e sociologi, e fornisce gli strumenti teorici e le competenze pratiche per affrontare le sfide che le tecnologie pongono al laureato e al ricercatore in discipline umanistiche. Il problema della trasformazione digitale della cultura è affrontato in tre diverse prospettive: elaborazione e ricerca, scrittura, conservazione dei saperi. Ciascuno di questi ambiti è affrontato da uno specialista del tema in un linguaggio semplice e rigoroso. Ne risulta una mappa dei cambiamenti e dei processi di riorganizzazione della produzione della conoscenza umanistica, che illustra le ragioni che dovrebbero indurre gli studiosi ad acquisire l'esperienza e le competenze necessarie a dialogare con la tecnologia digitale. Arricchisce il volume la presenza di un sito web, nel quale docenti e studenti troveranno materiali utili per integrare lo studio e la didattica.},
	publisher = {Il mulino},
	author = {Numerico, Teresa and Fiormonte, Domenico and Tomasi, Francesca},
	date = {2010},
	langid = {italian},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{burrows_lucy_2001,
	title = {Lucy Hutchinson and the authorship of two seventeenth-century poems: a computational approach},
	url = {http://novaprd-lb.newcastle.edu.au/vital/access/manager/Repository/uon:1333;jsessionid=E7927641DFE6192F07375353DA28D9AA?exact=title%3A%22Lucy+Hutchinson+and+the+authorship+of+two+seventeenth-century+poems%3A+a+computational+approach%22},
	doi = {10.1080/0268117X.2001.10555493},
	abstract = {Two poems attributed to Lucy Hutchinson by David Norbrook prove to have strong affinities with the style of her known verse on quantitative measures. A twenty-canto versification of Genesis titled Order and Disorder and a much shorter rejoinder to Waller's 'Panegyrick' of Cromwell were tested, using data from the frequencies of very common words, and with a set of twenty-five poets and a group of other long poems as controls. With or without the selection of Hutchinson 'marker words' the target poems clustered with Hutchinson's other work in a way which leaves little doubt as to their authorship. These results confirm Norbrook's attribution and help establish quite a varied canon for Hutchinson, and one which is of considerable importance in Civil War and Restoration literary history.},
	pages = {259--282},
	number = {16},
	journaltitle = {The Seventeenth Century},
	author = {Burrows, John and Craig, Hugh},
	date = {2001},
	langid = {english},
	keywords = {t\_Stylometry},
}

@book{liu_local_nodate,
	location = {Chicago},
	title = {Local Transcendence: Essays on Postmodern Historicism and the Database},
	isbn = {978-0226486963},
	url = {http://www.press.uchicago.edu/ucp/books/book/chicago/L/bo5867451.html},
	abstract = {Book of essays on the methodology of the new historicism and other modes of postmodern cultural criticism in the age of the network and the database.},
	pagetotal = {392},
	publisher = {University of Chicago Press},
	author = {Liu, Alan},
	langid = {english},
	keywords = {goal\_Create, goal\_Interpretation, meta\_GiveOverview, meta\_Theorizing, obj\_Data/Databases, t\_Narratology},
}

@article{fanta_literaturwissenschaft_2001,
	title = {Literatur(wissenschaft) und Computer: Entwicklung einer Beziehung},
	pages = {73--89},
	journaltitle = {Jahrbuch der Ungarischen Germanistik 2001},
	author = {Fanta, Walter},
	date = {2001},
	langid = {german},
	note = {Fanta, Walter: Literatur(wissenschaft) und Computer: Entwicklung einer Beziehung. In: Jahrbuch der Ungarischen Germanistik 2001, S. 73-89.},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{marche_literature_2012,
	title = {Literature is not Data: Against Digital Humanities},
	url = {http://lareviewofbooks.org/essay/literature-is-not-data-against-digital-humanities#},
	abstract = {Data banks are the Encyclopedia of tomorrow. They transcend the capacity of each of their users. They are "nature" for postmodern man ...},
	journaltitle = {Los Angeles Review of Books},
	author = {Marche, Stephen},
	date = {2012-10-28},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{potter_literary_1988,
	title = {Literary Criticism and Literary Computing: The Difficulties of a Synthesis},
	url = {http://www.jstor.org/discover/10.2307/30200105?uid=3737864&uid=2&uid=4&sid=21105148337633},
	abstract = {Currently most literary critics reject the use of science and technology to gain information about texts, while most computer text-analysts have become absorbed in science and technology and forgotten they were seeking information about literature. Whether these two trends will continue into the 1990's remains to be seen; that they explain a good deal about the world we work in now can, I think, be demonstrated. This essay looks at the questions of what literary computing could offer to literary critics, why computer users get lost in scientific jargon, what happens when text becomes input and, most importantly, what happens when text becomes output; it closes with a discussion of why the synthesis will be so difficult.},
	pages = {91--97},
	number = {22},
	journaltitle = {Computers and the Humanities},
	author = {Potter, Rosanne G.},
	date = {1988},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Assessing, meta\_GiveOverview, meta\_Theorizing, obj\_DigitalHumanities},
}
@book{kammer_literarische_1995,
	title = {Literarische Datenbanken. Anwendungen der Datenbanktechnologie in der Literaturwissenschaft},
	isbn = {978-3770529834},
	pagetotal = {259},
	publisher = {München: Fink 1995},
	author = {Kammer, Manfred},
	date = {1995},
	langid = {german},
	keywords = {{AnalyzeStatistically}, obj\_Data/Databases, obj\_Literature},
}

@collection{clivaz_lire_2012,
	location = {Lausanne},
	title = {Lire demain: des manuscrits antiques a l'ère digitale Jérôme , François Vallotton, Joseph Verheyden and Benjamin Bertho},
	isbn = {9782880749583},
	shorttitle = {Lire demain},
	pagetotal = {192},
	publisher = {Presses polytechniques et universitaires romandes},
	editor = {Clivaz, Claire and Meizozéô},
	date = {2012},
	langid = {french},
}

@article{guichard_linternet_2013,
	title = {L’internet et les épistémologies des sciences humaines et sociales},
	rights = {© Revue Sciences/Lettres},
	issn = {2271-6246},
	url = {http://rsl.revues.org/389},
	abstract = {Les ordinateurs et les réseaux nous rappellent à quel point notre pensée est instrumentée et nous font prendre conscience qu’elle l’a toujours été. Pour le dire autrement : la culture propre à l’informatique apparaît surtout technique. Mais elle n’est que la traduction contemporaine de l’ensemble des savoir-faire liés à la maîtrise de l’écriture. Nous (re)découvrons alors un lien étroit entre culture technique et culture des savants et des érudits, et les anthropologues ont montré la relation entre cette dernière et la culture au sens large : par effet de domination (le pouvoir de l’écrit) et parce que l’écriture invite à la réflexion sur les objets qu’elle manipule ou met en évidence. Il y a donc un lien direct entre culture technique propre à l’écriture et culture d’une société. Nous montrons alors comment l’écriture électronique et en réseau infléchit les problématiques et les épistémologies de disciplines communément regroupées sous l’étiquette « sciences humaines et sociales » ({SHS}) : nouvelles méthodes, potentialités combinatoires, questions posées par les usages du « numérique », etc., mais aussi savoir-faire élémentaires (écrire ou repérer un signe dans un texte). Certaines de ces problématiques commencent à être abordées par des personnes qui se revendiquent du mouvement des « humanités numériques ». Nous montrons que la faiblesse argumentative des représentants de ce mouvement est moins préoccupante pour les scientifiques que la facilité avec laquelle ils se font entendre : outre le dévoilement sociologique du monde universitaire actuel, toujours instructif, ce n’est pas tant l’essor des « humanités numériques » qui pose problème (parce qu’elles seraient mal définies ou joueraient d’un oxymore peu efficace épistémologiquement) que le silence de représentants des « {SHS} » quant à l’évolution des contours de chacune de leur discipline sous l’effet de l’écriture contemporaine. Pourtant, l’étude de cet effet, déjà balisée par des épistémologues, est prometteuse. Et elle permet de comprendre ce qui se « fabrique », de façon profane comme savante, en matière de culture numérique.},
	number = {2},
	journaltitle = {Revue Sciences/Lettres},
	author = {Guichard, Éric},
	editora = {Poibeau, Thierry},
	editoratype = {collaborator},
	urldate = {2014-03-06},
	date = {2013-10-07},
	langid = {french},
	keywords = {goal\_Interpretation, obj\_DigitalHumanities},
}

@book{wievorka_imperatif_2013,
	location = {Paris},
	title = {L'impératif numérique ou La nouvelle ère des sciences humaines et sociales ?},
	isbn = {978-2-271-07981-7},
	url = {http://lectures.revues.org/12837},
	abstract = {Et si l’omniprésence du numérique signait non seulement une nouvelle ère, un changement culturel majeur mais aussi une rupture profonde, une transformation radicale de l’humanité ? Et si le numérique constituait, après l’invention de l’écriture et celle de l’imprimerie, la troisième grande révolution ? A ces questions, Michel Wieviorka en ajoute une autre : l’ère du numérique ne pourrait-elle pas susciter en France aussi le renouveau des sciences humaines et sociales ({SHS}) ? Sa réponse est oui. Dans ce manifeste, le sociologue met en évidence le retard des {SHS} françaises dans la prise en compte et l’utilisation des nouveaux outils numériques. Sans doute ceux-ci représentent-ils un risque pour les libertés individuelles. Mais ils sont aussi un formidable outil de travail pour le chercheur en sciences sociales, fertile en innovations, nouvelles pratiques et nouvelles méthodes : le monde virtuel ne pourrait-il pas, par exemple, constituer un nouveau terrain d’enquête ? L’histoire, dont Fukuyama prédisait la fin, ne devient-elle pas une histoire à la fois globale et individuelle grâce aux Big data et leurs millions de données consultables en open access ? De sujet d’étude, l’homme et la société ne deviennent-ils pas acteurs/auteurs de leur propre récit ? Pour Michel Wieviorka, les sciences humaines et sociales doivent être partie prenante des nouvelles technologies de l’information. Comment ? En utilisant données et réseaux sociaux du web 2.0 pour échanger, communiquer, travailler de manière collaborative et produire des Humanités numériques - référence aux anciennes Humanités. Exemples à l’appui, il explique tout le parti qu’elles pourraient tirer du numérique et propose une nouvelle organisation de la recherche afin de dépasser le système des disciplines, véritable frein à l’innovation intellectuelle. Un saut qualitatif propice à l’éclosion des figures d’intellectuels qui nous font aujourd’hui défaut…},
	publisher = {{CNRS}},
	author = {Wievorka, Michel},
	date = {2013},
	langid = {french},
}

@article{brunet_exploitation_1989,
	title = {L'exploitation des grands corpus: Le bestiaire de la littérature française},
	volume = {4},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/4/2/121},
	doi = {10.1093/llc/4.2.121},
	shorttitle = {L'exploitation des grands corpus},
	abstract = {The present study is based on the data of the Institut National de la langue francaise. It tends to show the profit that can be derived from a very large corpus, when one chooses both a theme and certain texts The subject selected—animal representation in great writers—serves as an illustration of the versatility, the precision and the power of the {FRANTEXT} database},
	pages = {121--134},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Brunet, Etienne},
	urldate = {2012-09-23},
	date = {1989-01-01},
	langid = {english},
	keywords = {X-{CHECK}, bigdata},
}

@collection{dillen_lexicon_2014,
	location = {Antwerp},
	title = {Lexicon of Scholarly Editing. A Multilingual Lexicon for a Multilingual Discipline},
	url = {http://uahost.uantwerpen.be/lse/},
	publisher = {Centre for Manuscript Genetics},
	editor = {Dillen, Wout},
	date = {2014},
	langid = {english},
	keywords = {act\_Annotating, act\_Editing, goal\_Enrichment, obj\_Manuscripts, obj\_Text},
}

@article{gelman_lets_2002,
	title = {Lets practice what we preach: Turning tables into graphs},
	volume = {56},
	url = {http://www.stat.ncsu.edu/people/fuentes/courses/st810a/gelman.pdf},
	abstract = {Statisticians recommend graphical displays but often use tables to present their own research results. Could graphs do better? We study the question by going through the tables in a recent issue of the Journal of the American Statistical Association. We show how it is possible to improve the presentation using graphs that actually take up less space than the original tables. We find a particularly effective tool to be multiple repeated line plots, with comparisons of interest connected by lines and separate comparisons isolated on different plots.},
	pages = {121--130},
	number = {2},
	journaltitle = {The American Statistician},
	author = {Gelman, Andrew and Pascarica, Cristian and Dodhia, Rahul},
	date = {2002},
	langid = {english},
}

@book{vial_letre_2013,
	location = {Paris},
	title = {L'être et l'écran comment le numérique change la perception},
	publisher = {Presses universitaires de France},
	author = {Vial, Lévy, Pierre, Stéphane},
	date = {2013},
	langid = {french},
}

@report{thibault_les_2012,
	title = {Les infrastructures de recherche en sciences humaines et sociales. Rapport du groupe Infrastructures de l'Alliance Athena},
	institution = {Alliance Athena},
	author = {Thibault, Françoise and Jouve, Bertrand},
	date = {2012-09},
	langid = {french},
	keywords = {meta\_Assessing, meta\_ProjectManagement, obj\_Infrastructures},
}

@inproceedings{genet_les_2011,
	title = {Les historiens et l'informatique: un métier à réinventer},
	isbn = {9782728309047  2728309043},
	shorttitle = {Les historiens et l'informatique},
	publisher = {École française de Rome},
	author = {Genêt, Jean-Philippe and Zorzi, Andrea},
	date = {2011},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@incollection{mounier_les_2012,
	title = {Les digital humanities aujourd'hui : centres, réseaux, pratiques, enjeux},
	rights = {© {OpenEdition} Press, 2012 Conditions d'utilisation : http://www.openedition.org/6540},
	url = {http://books.openedition.org/oep/226},
	abstract = {Qu’est-ce que les humanités numériques ? Apparue en 2006, l’expression connaît depuis un véritable succès. Mais au-delà du slogan à la mode, quelle est la réalité des pratiques qu’il désigne ? Si tout le monde s’accorde sur une définition minimale à l’intersection des technologies numériques et des sciences humaines et sociales, les vues divergent lorsqu’on entre dans le vif du sujet. Les humanités numériques représentent-elles une véritable révolution des pratiques de recherche et des paradigmes intellectuels qui les fondent ou, plus simplement, une optimisation des méthodes existantes ? Constituent-elles un champ suffisamment structuré pour justifier une réforme des modes de financement de la recherche, des cursus de formation, des critères d’évaluation ? L’archive numérique offre-t-elle à la recherche suffisamment de garanties ? Quelle place la recherche « dirigée par les données » laisse-t-elle à l’interprétation ? Telles sont quelques-unes des questions abordées par ce deuxième opus de la collection « Read/Write Book ». Ces dix-huit textes essentiels, rédigés ou traduits en français par des chercheurs de différentes nationalités, proposent une introduction aux humanités numériques accessible à tous ceux qui souhaitent en savoir plus sur ce domaine de recherche en constante évolution.},
	booktitle = {Read/Write Book 2 : Une introduction aux humanités numériques},
	publisher = {{OpenEdition} Press},
	author = {Welger-Barboza, Corine},
	editor = {Mounier, Pierre},
	urldate = {2013-10-19},
	date = {2012},
	langid = {french},
}

@book{lardellier_reseau_2007,
	location = {Dijon},
	title = {Le réseau pensant: pour comprendre la société numérique},
	isbn = {9782915552256},
	series = {Collection Sociétés},
	shorttitle = {Le réseau pensant},
	pagetotal = {190},
	publisher = {Editions universitaires de Dijon},
	editora = {Lardellier, Pascal and Ricaud, Philippe},
	editoratype = {collaborator},
	date = {2007},
	langid = {french},
}

@article{saleh_numerique_2012,
	title = {Le numérique comme catalyseur épistémologique},
	rights = {© {SFSIC}},
	issn = {2263-0856},
	url = {http://rfsic.revues.org/168?lang=en},
	doi = {10.4000/rfsic.168},
	abstract = {This paper presents a discussion on the challenges brought out by digital technologies. Objects, methods and research topics in the humanities and social sciences, and especially in information and communication sciences ({ICS}), have been impacted by digital media. Digital media are defined with regards to their relation to hypertext and hypermedia, and the transverse influence of information technology on mediations is analyzed in order to address the question of the convergence between digital technologies and humanities, and the need of transdisciplinarity that results from it. The holistic transformations caused by digital technologies require a more complex approach to human activities, including scientific research itself, and thus integrating multiple theories. Emerging problematic in the field of {ICS} could constitute an opportunity to position themselves as a pivotal discipline that can focus (themselves on) the relationship between, and within the engineering sciences and humanities. The discussion concludes with prospects concerning future transverse issues about locating and accessing cloud-stored data, collective memories and digital traces, and the new boundaries, or “borders,” which result from the spreading of {ITs}. The growing impact of digital technologies in human activities confirms the importance of developing a deep reflection on the “ethics” of the information society.},
	number = {1},
	journaltitle = {Revue française des sciences de l'information et de la communication},
	author = {Saleh, Imad and Hachour, Hakim},
	editora = {Miège, Bernard},
	editoratype = {collaborator},
	urldate = {2014-05-26},
	date = {2012-07-06},
	langid = {french},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{vonfelt_graphonaute_2009,
	title = {Le graphonaute ou Molière retrouvé},
	url = {http://lexicometrica.univ-paris3.fr/numspeciaux/special9/Vanfelt.pdf},
	abstract = {Did Corneille write the plays of Molière? The
answers contradict each other, including those
refering to the objectivity of figures. In this vein, our
study bases on the distributi
on of characters composing
a text. Between two works, the resu
lting distance renders the contribution of
the author, but also the gender,
the form and the chronology. The measurement does not in
cite to melt Molière into Corneille and highlights
the variety of his work, probably
influenced by se
veral sources.},
	journaltitle = {Lexicometrica},
	author = {Vonfelt, Stephan},
	date = {2009},
	langid = {french},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@collection{cresson_ledition_2012,
	location = {Paris},
	title = {L'Édition du manuscrit},
	abstract = {Cet ouvrage expose la relation tissée entre l'archive patrimoniale du conservateur, la valorisation scientifique assumée par le chercheur et la transmission des savoirs dont l'éditeur est garant. Il réunit des généticiens du texte, des conservateurs, des éditeurs, chacun faisant part de son expérience concernant la numérisation, l'appropriation des manuscrits et l'édition de genèse.},
	publisher = {L'Harmattan},
	editor = {Cresson, Aurèle},
	date = {2012},
	langid = {french},
	keywords = {act\_Publishing, obj\_Manuscripts, t\_Encoding},
}

@inproceedings{brunet_corpus_2006,
	location = {Paris},
	title = {Le corpus conçu comme une boule},
	url = {http://www.revue-texto.net/Parutions/Livres-E/Albi-2006/Brunet.pdf},
	abstract = {In: Duteil-Mougel C, Foulquié B, editors. Actes du Colloque international et école d’été ‘Corpus en Lettres et Sciences sociales - Des documents numériques à l'interprétation’,},
	pages = {69--78},
	author = {Brunet, Étienne},
	date = {2006},
	langid = {french},
	keywords = {X-{CHECK}},
}

@article{heuser_learning_2011,
	title = {Learning to Read Data: Bringing out the Humanistic in the Digital Humanities},
	volume = {54},
	url = {http://muse.jhu.edu/login?auth=0&type=summary&url=/journals/victorian_studies/v054/54.1.heuser.html},
	doi = {10.1353/vic.2011.0159},
	abstract = {In lieu of an abstract, here is a brief excerpt of the content:

As humanists, we believe we are trained as expert readers, able to read almost any kind of text closely, deeply, and critically. But on 21 February 2010, the two of us sat staring at a computer screen dumbfounded by a kind of text that for once we had no idea how to read. Working with a corpus of thousands of digitized nineteenth-century British novels, we had just produced a plot, similar to figure 1, showing the usage trends of a massive group of abstract words relating to social values. The plot seemed to show all these words disappearing over the nineteenth century. What exactly were we seeing here? A heated discussion of principles in the fallout of the French Revolution? A death of values in the Victorian period?

The moment was emblematic of how it can feel to encounter digital humanities work. In facing a radically new kind of text, a different kind of evidence, tremendous excitement and real anxiety mix. It’s easy to understand the excitement. These emerging methods promise ways to pursue big questions we’ve always wanted to ask with evidence not from a selection of texts, but from something approaching the entire literary or cultural record. Moreover, the answers produced could have the authoritative backing of empirical data. But it’s also understandable how these possibilities could be unsettling. By offering an entirely different model of humanities scholarship, the digital humanities raise many questions. What do we do with this kind of evidence? Can we leverage quantitative methods in ways that respect the nuance and complexity we value in the humanities? Behind these questions is perhaps a deeper concern. Under the flag of interdisciplinarity, are the digital humanities no more than the colonization of the humanities by the sciences?

While doing digital humanities research over the past two years, we have constantly wrestled with these questions. We have learned, happily, that the answers may not be as troubling as they can seem. We are convinced that, when done well, such research can deliver scale, empirical rigor, and the nuance the humanities value. Yet this will require deep reflection as these methods develop. More importantly, this work will depend on humanistic methods. We hope to substantiate these claims by addressing some key methodological questions and presenting a case study from our own research.},
	pages = {79--86},
	number = {1},
	journaltitle = {Victorian Studies},
	author = {Heuser, Ryan and Le-Khac, Long},
	date = {2011},
	langid = {english},
	keywords = {obj\_Literature},
}

@inproceedings{brunet_base_1999,
	title = {La base textuelle Batelier},
	url = {http://project.cgm.unive.it/events/papers/Brunet.pdf},
	pages = {175--83},
	booktitle = {Proceedings of {VEXTAL}’99, Venezia, San Servolo V.I.U, 22-24 November 1999.},
	publisher = {Venice University Press},
	author = {Brunet, Étienne},
	date = {1999},
	langid = {french},
	keywords = {X-{CHECK}},
}

@article{blei_latent_2003,
	title = {Latent Dirichlet Allocation},
	volume = {3},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=944919.944937},
	abstract = {We describe latent Dirichlet allocation ({LDA}), a generative probabilistic model for collections of discrete data such as text corpora. {LDA} is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an {EM} algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic {LSI} model.},
	pages = {993--1022},
	journaltitle = {J. Mach. Learn. Res.},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	urldate = {2014-03-29},
	date = {2003-03},
}

@collection{irigoin_pratique_1979,
	location = {Paris},
	title = {La pratique des ordinateurs dans la critique des textes. Actes du colloque de Paris (29-31 mars 1978)},
	url = {http://www.persee.fr/web/revues/home/prescript/article/antiq_0770-2817_1982_num_51_1_2078_t1_0509_0000_2},
	publisher = {{CNRS}},
	editor = {Irigoin, Jean and Zarri, Gian Piero},
	date = {1979},
	langid = {french},
	keywords = {act\_RelationalAnalysis, goal\_Analysis, goal\_Enrichment, obj\_Text, t\_Collation, t\_Stemmatology},
}

@article{leroux_notion_2012,
	title = {La notion de modèle en philosophie des sciences},
	volume = {7},
	issn = {1712-8307, 1918-7475},
	url = {http://www.erudit.org/revue/npss/2012/v7/n2/1013054ar.html?vue=resume},
	doi = {10.7202/1013054ar},
	abstract = {We want to highlight the central role played by the notion of model in philosophy of science. Having first underlined the intricate tie existing between models and theories in science, we distinguish, in the philosophical literature, the notion of model as general conception from the structural notion of model currently encountered. We present in informal fashion and critically assess the main approaches adopted by epistemological analysis in its intent to characterize models generally associated with scientific theories. Lastly, and with a view to illustrating the centrality of the notion of model, we show how the current debate over scientific realism essentially hinges on contrasting ways to construe of models.},
	pages = {49},
	number = {2},
	journaltitle = {Nouvelles perspectives en sciences sociales},
	author = {Leroux, Jean},
	urldate = {2014-09-02},
	date = {2012},
	langid = {french},
	keywords = {act\_Modeling, goal\_Interpretation, obj\_Research},
}

@article{marsden_language_2013,
	title = {Language Individuation and Marker Words: Shakespeare and His Maxwell's Demon},
	volume = {8},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0066813},
	doi = {10.1371/journal.pone.0066813},
	shorttitle = {Language Individuation and Marker Words},
	abstract = {Within the structural and grammatical bounds of a common language, all authors develop their own distinctive writing styles. Whether the relative occurrence of common words can be measured to produce accurate models of authorship is of particular interest. This work introduces a new score that helps to highlight such variations in word occurrence, and is applied to produce models of authorship of a large group of plays from the Shakespearean era.},
	pages = {e66813},
	number = {6},
	journaltitle = {{PLoS} {ONE}},
	author = {Marsden, John and Budden, David and Craig, Hugh and Moscato, Pablo},
	editor = {Altmann, Eduardo G.},
	urldate = {2013-10-01},
	date = {2013-06-27},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@article{sinclair_language_1998,
	title = {Language Independent Statistical Software for Corpus Exploration},
	volume = {31},
	url = {http://link.springer.com/article/10.1023%2FA%3A1000911520943},
	doi = {10.1023/A:1000911520943},
	abstract = {In this report two programs for statistical analysis of concordance lines are described. The programs have been developed for analyzing he lexical context of a given word. It is shown how different parameter settings influence the outcome of collocational analysis, and how the concept of collocation can be extended to allow the extraction of lines typical for a word from a set of concordance lines. Even though all the examples are for English, the software is completely language independent and only requires minimal linguistic resources.},
	pages = {229--255},
	number = {3},
	journaltitle = {Computers and the Humanities},
	author = {Sinclair, John and Mason, Oliver and Ball, Jackie and Barnbrook, Geoff},
	date = {1998},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@book{cardon_mocratie_2010,
	location = {Paris},
	title = {La démocratie Internet: promesses et limites},
	isbn = {9782021026917},
	url = {http://www.amazon.fr/La-d%C3%A9mocratie-Internet-Promesses-limites/dp/2021026914},
	shorttitle = {La démocratie Internet},
	abstract = {Disparition de l'espace privé, incitation à la diffamation, menaces sur l'avenir de la presse: dans de nombreux débats, Internet fait figure de coupable. Mais, bien plus qu'un média de communication et d'information, Internet est une forme politique à part entière. En décloisonnant le débat et en l'ouvrant à de nouveaux participants, Internet renouvelle les possibilités de critique et d'action. Surtout, le web constitue à l'échelle planétaire un laboratoire d'expériences démocratiques: auto-organisation des citoyens, délibération élargie à de nouveaux publics, mise en place de collectifs transnationaux, socialisation du savoir, etc. Internet ne permet pas seulement de communiquer davantage; il élargit formidablement l'espace public et transforme la nature même de la démocratie. Avant de la célébrer ou de la dénigrer, il faut penser la révolution numérique.},
	pagetotal = {101},
	publisher = {Seuil},
	author = {Cardon, Dominique},
	date = {2010},
	langid = {french},
}

@article{juola_killer_2008,
	title = {Killer Applications in Digital Humanities},
	volume = {23},
	url = {http://www.box.net/shared/tgn86rke4p},
	doi = {10.1093/llc/fqm042},
	abstract = {The emerging discipline of ‘digital humanities’ has been plagued by a perceived neglect on the part of the broader humanities community. The community as a whole tends not to be aware of the tools developed by {DH} practitioners (as documented by the recent surveys by Siemens et al.), and tends not to take seriously many of the results of scholarship obtained by {DH} methods and tools. This article argues for a focus on deliverable results in the form of useful solutions to common problems that humanities scholars share, instead of simply new representations. The question to address is what needs the humanities community has that can be dealt with using {DH} tools and techniques, or equivalently what incentive humanists have to take up and to use new methods. This can be treated in some respects like the computational quest for the ‘killer application’—a need of the user group that can be filled, and by filling it, create an acceptance of that tool and the supporting methods/results. Some definitions and examples are provided both to illustrate the idea and to support why this is necessary. The apparent alternative is the status quo, where digital research tools are brilliantly developed, only to languish in neglect and disuse.},
	pages = {73 --83},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	author = {Juola, Patrick},
	urldate = {2011-04-26},
	date = {2008-04-01},
	langid = {english},
	note = {http://www.box.net/shared/tgn86rke4p},
	keywords = {meta\_Advocating, obj\_DigitalHumanities, obj\_Tools},
}
@article{fischer-starcke_keywords_2009,
	title = {Keywords and Frequent Phrases of Jane Austen’s Pride and Prejudice A Corpus-stylistic Analysis},
	volume = {14},
	url = {http://dx.doi.org/10.1075/ijcl.14.4.03fis},
	abstract = {Corpus linguistic analyses reveal meanings and structural features of data, that cannot be detected intuitively. This has been amply demonstrated with regard to non-fiction data, but fiction texts have only rarely been analysed by corpus linguistic techniques. This is the case even though it has been shown by previous analyses that corpus stylistic analyses reveal literary meanings of the data that are left undetected by the intuitive analyses of literary criticism. The analysis of the keywords and most frequent phrases of Jane Austen’s novel Pride and Prejudice presented in this article confirms this claim by uncovering meanings that are not discussed in literary critical secondary sources. This constitutes evidence for the large potential of corpus stylistics for the analysis of literature and its meanings.},
	pages = {492},
	number = {4},
	journaltitle = {International Journal of Corpus Linguistics},
	author = {Fischer-Starcke, Bettina},
	date = {2009},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, obj\_Language, obj\_Literature},
}

@article{culpeper_keyness:_2009,
	title = {Keyness: Words, Parts-of-speech and Semantic Categories in the Character-talk of Shakespeare’s Romeo and Juliet},
	volume = {14},
	url = {http://dx.doi.org/10.1075/ijcl.14.1.03cul},
	abstract = {This paper explores keywords, key part-of-speech categories and key semantic categories and their role in text analysis. The first part of the paper addresses a set of issues relating to the definition of keywords and their history, the settings used in deriving keywords, the choice of reference corpora, the different kinds of keyword that emerge in one’s results and the dispersion of keywords in one’s data. It argues, amongst other things, that keywords are the same as style markers, and that three types of keyword can be identified: interpersonal, textual and ideational. The second part of the paper addresses the question of what precisely is to be gained from analysing key part-of-speech or key semantic domains in addition to keywords. It shows that whilst in general they add little to a keyword analysis, which is in any case methodologically more robust, there are some significant specific benefits. Answers to many of the questions posed in this paper are illustrated by a study of character-talk from Shakespeare’s play Romeo and Juliet, and in this way this paper also makes a contribution to the fledging field of corpus stylistics.},
	pages = {29--},
	number = {1},
	journaltitle = {International Journal of Corpus Linguistics},
	author = {Culpeper, Jonathan},
	date = {2009},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, obj\_Language, obj\_Literature},
}

@article{karatzoglou_kernel-based_2010,
	title = {Kernel-based machine learning for fast text mining in R},
	volume = {54},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947309003533},
	doi = {16/j.csda.2009.09.023},
	abstract = {Recent advances in the field of kernel-based machine learning methods allow fast processing of text using string kernels utilizing suffix arrays. kernlab provides both kernel methods' infrastructure and a large collection of already implemented algorithms and includes an implementation of suffix-array-based string kernels. Along with the use of the text mining infrastructure provided by tm these packages provide R with functionality in processing, visualizing and grouping large collections of text data using kernel methods. The emphasis is on the performance of various types of string kernels at these tasks.},
	pages = {290--297},
	number = {2},
	journaltitle = {Computational Statistics \& Data Analysis},
	author = {Karatzoglou, Alexandros and Feinerer, Ingo},
	urldate = {2011-06-30},
	date = {2010-02},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@book{hennermann_kartographie_2006,
	location = {Darmstadt},
	title = {Kartographie und {GIS} : eine Einführung},
	isbn = {9783534196920},
	url = {http://www.wbg-wissenverbindet.de/shop/de/wbg/kartographie-und-gis-b243952-001},
	shorttitle = {Kartographie und {GIS}},
	abstract = {Diese neuartige Einführung entspricht in ihrem Aufbau dem Grundkurs Kartographie und Datenverarbeitung, wie er für nahezu alle Geowissenschaftler verpflichtend angeboten wird. Das Buch erläutert kompakt aber umfassend, wie Karten Informationen speichern und wiedergeben. Darüber hinaus wird die technische Seite der Kartenherstellung, soweit sie für die Geowissenschaften erforderlich ist, gleich mitbehandelt. Anhand von Beispielen und Übungsaufgaben lernt der Leser, eigene Daten zu verarbeiten und Karten zu erstellen. Technische Basis ist die Software {ESRI} {ArcGIS} 9, die im universitären Bereich überall eingesetzt wird und als Testversion für Übungszwecke umsonst angefordert werden kann. Zusätzliche Übungsdateien können aus dem Internet geladen werden.},
	publisher = {Wiss. Buchges.},
	author = {Hennermann, Karl},
	date = {2006},
	langid = {german},
	keywords = {act\_Visualizing, meta\_GiveOverview, obj\_Artefacts, obj\_Maps},
}

@article{stasko_jigsaw:_2008,
	title = {Jigsaw: supporting investigative analysis through interactive visualization},
	volume = {7},
	url = {http://www.cc.gatech.edu/~stasko/papers/vast07-jigsaw.pdf},
	abstract = {Investigative analysts and researchers acquire clues and connect small bits of evidence to uncover larger plans, stories, or narratives, and to simply gain a better understanding of the information. Often, the individual bits of evidence are short text documents or spreadsheets, and analysts must examine large collections of such documents in order to "put the pieces together" and formulate a well-supported hypothesis about actions that may occur in the future. As the number of documents to examine rises, it becomes more and more challenging for analysts to understand the data and make judgments about it.

We have created Jigsaw, a visual analytics system to help analysts and researchers better explore, analyze, and make sense of such document collections. Our specific objective is to help analysts reach more timely and accurate understandings of the larger stories and important concepts embedded throughout textual reports. Jigsaw provides a collection of visualizations that each portray different aspects of the documents. We particularly focus on presenting the identifiable important entities (people, places, organizations, etc.) and their direct or indirect connections. Textual processing extracts the important entities from the documents and then the visualizations help an analyst to explore the relationships and connections among the entities. The system includes a variety of visualizations such as list, graph, temproal and connection-based views, as well as views of individual document's text and the document collection as a whole. Jigsaw essentially acts as a visual index onto the document collection, helping analysts identify particular documents to read and examine next.

We have used Jigsaw to explore a wide variety of domains and document collections including academic papers, grants, product reviews, business press releases, news articles, intelligence and police reports, statutes, and even books such as the Bible. Jigsaw is available for you to try and use on your own documents and data.},
	pages = {118--132},
	journaltitle = {Information Visualization},
	author = {Stasko, J. and Görg, C. and Spence, R.},
	date = {2008},
	langid = {english},
	note = {http://www.cc.gatech.edu/gvu/ii/jigsaw/},
	keywords = {{AnalyzeStatistically}, act\_Visualizing, obj\_Tools},
}

@report{feinerer_introduction_2014,
	title = {Introduction to the tm Package. Text Mining in R},
	url = {http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf},
	abstract = {This vignette gives a short introduction to text mining in
R
utilizing the text mining framework provided by
the
tm
package. We present methods for data import, corpus handling, preprocessing, metadata management,
and creation of term-document matrices. Our focus is on the main aspects of getting started with text mining
in
R
{\textbar}an in-depth description of the text mining infrastructure o⬚ered by
tm
was published in the
Journal of
Statistical Software
(Feinerer et al., 2008). An introductory article on text mining in
R
was published in
R
News},
	pages = {7},
	author = {Feinerer, Ingo},
	date = {2014},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Tools},
}

@book{alpaydin_introduction_2010,
	location = {Cambridge, Mass},
	edition = {2nd ed},
	title = {Introduction to machine learning},
	isbn = {9780262012430},
	url = {http://mitpress.mit.edu/books/introduction-machine-learning},
	series = {Adaptive computation and machine learning},
	abstract = {The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. Introduction to Machine Learning is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. In order to present a unified treatment of machine learning problems and solutions, it discusses many methods from different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program.

The text covers such topics as supervised learning, Bayesian decision theory, parametric methods, multivariate methods, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, and reinforcement learning. New to the second edition are chapters on kernel machines, graphical models, and Bayesian estimation; expanded coverage of statistical tests in a chapter on design and analysis of machine learning experiments; case studies available on the Web (with downloadable results for instructors); and many additional exercises. All chapters have been revised and updated.

Introduction to Machine Learning can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods.},
	pagetotal = {537},
	publisher = {{MIT} Press},
	author = {Alpaydin, Ethem},
	date = {2010},
	langid = {english},
	keywords = {*****, {AnalyzeStatistically}, obj\_AnyObject},
}

@incollection{craig_shakespeare_2009,
	location = {Cambridge},
	title = {Shakespeare, Computers, and the Mystery of Authorship},
	isbn = {9780511605437, 9780521516235},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511605437A010},
	abstract = {One of the earliest champions of language in Shakespeare's time was Thomas Wilson. In The Arte of Rhetorique (1553) he declares that

    Suche force hath the tongue, and such is the power of eloquence and reason, that most men are forced euen to yelde in that, whiche most standeth againste their will. And therfore the Poetes do feyne that Hercules being a man of greate wisdome, had all men lincked together by the eares in a chaine, to draw them and leade them euen as he lusted. For his witte was so greate, his tongue so eloquente, \& his experience suche, that no one man was able to withstand his reason, but euerye one was rather driuen to do that whiche he woulde, and to wil that whiche he did, agreeing to his aduise both in word \& worke, in all that euer they were able.

    Neither can I see that menne coulde haue bene broughte by anye other meanes to lyue together in felowshyppe of life, to mayntayne Cities, to deale trulye, and willyngelye to obeye one another, if menne at the firste hadde not by Art and eloquence perswaded that, which they ful oft found out by reason.
    (sigs. A3v–A4r)

At the time such ideas were not especially original – the works of Aristotle and Cicero in the Tudor grammar schools had made them commonplace – but Wilson's ambition and vision are nevertheless unusual.},
	pages = {1--14},
	booktitle = {Shakespeare, Computers, and the Mystery of Authorship},
	publisher = {Cambridge University Press},
	author = {Craig, Hugh and Kinney, Arthur F.},
	editor = {Craig, Hugh and Kinney, Arthur F.},
	urldate = {2012-05-15},
	date = {2009},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@book{galina_introduccion_2007,
	location = {Mexico City},
	title = {Introducción a la edición digital},
	url = {http://www.anatomiadelaedicion.com/wordpress/wp-content/uploads/2010/01/manual-de-edicion-digital-1.pdf},
	series = {Colección Biblioteca del Editor},
	pagetotal = {128},
	publisher = {Dirección General de Publicaciones y Fomento Editorial, {UNAM}},
	author = {Galina, Isabel and Ordoñez, C.},
	date = {2007},
	langid = {spanish},
	keywords = {act\_Editing, goal\_Enrichment, obj\_Text},
}

@online{pulizzi_near_2014,
	title = {In the Near Future, Only Very Wealthy Colleges Will Have English Departments},
	url = {http://www.newrepublic.com/article/118025/advent-digital-humanities-will-make-english-departments-pointless},
	abstract = {The change isn’t necessarily an evil to be decried.},
	titleaddon = {New Republic},
	type = {Text},
	author = {Pulizzi, James},
	urldate = {2014-06-10},
	date = {2014-06-08},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{labbe_inter-textual_2001,
	title = {Inter-Textual Distance and Authorship Attribution Corneille and Molière},
	volume = {8},
	issn = {0929-6174},
	url = {http://www.tandfonline.com/doi/abs/10.1076/jqul.8.3.213.4100},
	doi = {10.1076/jqul.8.3.213.4100},
	abstract = {The calculation proposed in this paper measures neighbourhood between several texts. It leads to a normalized metric and a distance scale which can be used for authorship attribution. An experiment is presented on one of the famous cases in French literature: Corneille and Molière. The calculation clearly makes the difference between the two works but it also demonstrates that Corneille contributed to many of Molière’s masterpieces.},
	pages = {213--231},
	number = {3},
	journaltitle = {Journal of Quantitative Linguistics},
	author = {Labbé, Cyril and Labbé, Dominique},
	urldate = {2012-08-18},
	date = {2001-12-01},
	langid = {english},
}

@article{miller_interoperability._2000,
	title = {Interoperability. What is it and Why should I want it?},
	volume = {24},
	url = {http://www.ariadne.ac.uk/issue24/interoperability/},
	abstract = {ogether with terms like "metadata" and "joined-up thinking", this word is increasingly being used within the information management discourse across all of our memory institutions. Its meaning, though, remains somewhat ambiguous, as do many of the benefits of "being interoperable". This paper is an attempt, written from the doubtless biased perspective of someone with the word in their job title, to explain some of what interoperability means, and to begin stating the case for more active efforts towards being truly interoperable across a range of services.},
	journaltitle = {Ariadne},
	author = {Miller, Paul},
	urldate = {2011-08-19},
	date = {2000},
	langid = {english},
	keywords = {goal\_Collaboration, meta\_GiveOverview, t\_Interoperability},
}

@book{jesser_interaktive_1991,
	location = {Bern ; Frankfurt am Main[u.a.]},
	title = {Interaktive Melodieanalyse: Methodik und Anwendung computergestützter Analyseverfahren in Musikethnologie und Volksliedforschung: typologiche Untersuchung der Balladenforschung des {DVA}},
	isbn = {3-261-04360-1},
	series = {Studien zur Volksliedforschung},
	abstract = {Thesis ({PhD} level)},
	pagetotal = {308},
	number = {12},
	publisher = {Lang},
	author = {Jesser, Barbara},
	date = {1991},
	langid = {german},
	note = {Essen, Univ., Diss., 1990},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, obj\_Music},
}

@article{siemens_inke_2012,
	title = {{INKE} Administrative Structure: Omnibus Document},
	volume = {3},
	rights = {{SRC} embraces online publishing and open access to back issues under the Creative Commons Attribution-Noncommercial-No Derivative Works 2.5 Licence. This license allows users to download an article and share it with others as long as authorship and original publication is acknowledged and a link is made (in electronic media) to the original article. The article can be quoted but not changed and presented differently.},
	url = {http://src-online.ca/index.php/src/article/view/50},
	shorttitle = {{INKE} Administrative Structure},
	abstract = {This document reflects the distributed administrative structure to be put into practice by the Implementing New Knowledge Environments ({INKE}) group for the purpose of governing itself as it carries out work on its Major Collaborative Research Initiative ({MCRI})-funded initiative. The {INKE} group consists of academic researchers, academic research partners (many invested as stakeholders as well), an international advisory board, a partners committee, individual research area groups ({RAG}) each with their own (co)leads who act as administrators for the group and form the overall {RAG} administrative group committee, and an executive committee ({EC}) that represents all areas of activity in the research endeavour and also includes an administrative/ management advisor (who carries out work and provides leadership on process, not research content) and a project manager. Taken as a whole, the structure of the group is an embodiment of the distributed administrative and authoritative principles that have evolved over the several years of the project's foundation, and the materials that follow have been assembled and authored by the entirety of the administrative team in that spirit. This document is also closely aligned with the processes outlined in two related documents: the annual calendar and the annual {RAG} planning process.},
	number = {1},
	journaltitle = {Scholarly and Research Communication},
	author = {Siemens, Lynne},
	urldate = {2014-07-23},
	date = {2012-03-26},
	langid = {english},
	keywords = {goal\_Collaboration},
}

@report{herold_initial_2009,
	title = {Initial Requirements Analysis (D-{SPIN} Report R 3.1)},
	url = {http://weblicht.sfs.uni-tuebingen.de/Reports/D-SPIN_R3.1.pdf},
	abstract = {D
-
{SPIN}
is
the
Germ
an
counterpart
of
the
European
Research
Inf
rastructure
project
{CLA}
{RIN}
(Com
m
on
Language
Resources
and
Technology
Inf
rastructure,
http:/
/
w
w
w
.
clarin.
eu/
)
.
The
ultim
ate
objective
of
{CLA}
{RIN}
is
to
create
a
European
f
ederation
of
existing
digital
repositories
that
include
language
-
based
data,
to
provide
unif
orm
access
to
the
data,
w
herever
it
is,
and
to
provide
existing
language
and
speech
technology
tools
as
w
eb
services
to
retrieve,
m
anipulate,
enhance,
explore
and
exploit
the
data.
The
prim
ary
target
audience
is
researchers
in
the
hum
anities
and
social
sciences
and
the
aim
is to cover all languages relevant f
or the user com
m
unity.
Sim
ilar goals are pursued by the D
-
{SPIN} project on the national level.
Work in D
-
{SPIN} is carried out
in
close
collaboration
w
ith
{CLA}
{RIN}.
Within
the
{CLA}
{RIN}
f
ederation,
the
f
ocus
of
D
-
{SPIN}
is
on
Germ
an
resources,
tools
and
their
integration
through
w
eb
services.
Besides
these
localization
ef
f
orts,
D
-
{SPIN}
has
a
special
f
ocus
in
addressing
potential
users
of
the
inf
rastructure
w
ith
the
preparation of
training m
aterial and teaching activities.
This
report
sum
m
arizes
the
requirem
ents
and
expectations
of
the
user
com
m
unity
w
ith
regard
to
the
D
-
{SPIN}
inf
rastructure.
The
data
are
still
prelim
inary,
though,
as
som
e
of
the
m
ethods
f
or
requirem
ents analysis necessarily rely on existing prototype im
plem
entati
ons},
	pages = {34},
	institution = {D-{SPIN}/{BBAW}},
	author = {Herold, Axel and Geyken, Alexander and Lemnitzer, Lothar},
	date = {2009},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_Research, obj\_VREs},
}

@article{mcgann_information_2005,
	title = {Information Technology and the Troubled Humanities},
	volume = {14},
	url = {http://texttechnology.mcmaster.ca/archives.html},
	abstract = {Where will Information Technology Leave Humanities Education Five,
Ten, Twenty. . .Years from Now. This essay addresses that question in
the context of several current “crises” facing humanities scholarship and
education. These crises have followed from the displacement of traditional philological work from the center of the literary and cultural studies’ curriculum—in particular editorial theory and method, history of the
language, and bibliographical studies. The coming of digital technology
to the humanities has revealed the historical necessity of recovering these
basic disciplinary skills},
	number = {2},
	journaltitle = {Text Technology},
	author = {{McGann}, Jerome},
	date = {2005},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{snow_information_2006,
	title = {Information Science Enhanced: Cybertools and Archaeology},
	volume = {311},
	url = {http://clgiles.ist.psu.edu/papers/Science-cyber-historical_2006.pdf},
	pages = {958--959},
	number = {5763},
	journaltitle = {Science},
	author = {Snow, D R},
	date = {2006-02},
	langid = {english},
	keywords = {obj\_Artefacts, obj\_Tools},
}

@inproceedings{bowman_information-rich_2003,
	location = {Osaka, Japan},
	title = {Information-rich virtual environments: theory, tools, and research agenda},
	isbn = {1-58113-569-6},
	url = {http://portal.acm.org/citation.cfm?doid=1008653.1008669},
	doi = {10.1145/1008653.1008669},
	shorttitle = {Information-rich virtual environments},
	abstract = {Virtual environments ({VEs}) allow users to experience and interact with a rich sensory environment, but most virtual worlds contain only sensory information similar to that which we experience in the physical world. Information-rich virtual environments ({IRVEs}) combine the power of {VEs} and information visualization, augmenting {VEs} with additional abstract information such as text, numbers, or graphs. {IRVEs} can be useful in many contexts, such as education, medicine, or construction. In our work, we are developing a theoretical foundation for the study of {IRVEs} and tools for their development and evaluation. We present a working definition of {IRVEs}, a discussion of information display and interaction in {IRVEs}. We also describe a software framework for {IRVE} development and a testbed enabling evaluation of text display techniques for {IRVEs}. Finally, we present a research agenda for this area.},
	pages = {81--90},
	publisher = {{ACM}},
	author = {Bowman, Doug A. and North, Chris and Chen, Jian and Polys, Nicholas F. and Pyla, Pardha S. and Yilmaz, Umur},
	urldate = {2009-05-13},
	date = {2003},
	langid = {english},
	keywords = {act\_Visualizing, meta\_GiveOverview, obj\_Infrastructures},
}

@incollection{savoy_information_2010,
	location = {Boca Raton, {FL}},
	title = {Information Retrieval},
	isbn = {978-1420085921},
	url = {http://cgi.cse.unsw.edu.au/~handbookofnlp/index.php?n=Chapter22.Chapter22},
	abstract = {This chapter presents the fundamental concepts of Information Retrieval ({IR}) and shows how this domain is related to various aspects of {NLP}. After explaining some of the underlying and often hidden assumptions and problems of {IR}, we present the notion of indexing. Indexing is the cornerstone of various classical {IR} paradigms (Boolean, vector-space, and probabilistic) which we introduce together with some insights to advanced search strategies used on the Web, such as {PageRank}. The {IR} community relies on a strong empirical tradition and we present the basic notions of {IR} evaluation methodology and show, with concrete examples, why some topic formulations can be hard even with the most advanced search strategies. Various {NLP} techniques can be used to, at least partially, improve the retrieval performance of {IR} models. We devote a section of this chapter to an overview of these techniques},
	booktitle = {Handbook of Natural Language Processing, Second Edition},
	publisher = {{CRC} Press, Taylor and Francis Group},
	author = {Savoy, Jacques and Gaussier, Eric},
	editor = {Indurkhya, Nitin and Damerau, Fred J.},
	date = {2010},
	langid = {english},
	keywords = {*****, {AnalyzeStatistically}, meta\_GiveOverview},
}

@article{yang_information_2003,
	title = {Information categorization approach to literary authorship disputes},
	volume = {329},
	issn = {0378-4371},
	url = {http://www.sciencedirect.com/science/article/pii/S0378437103006228},
	doi = {10.1016/S0378-4371(03)00622-8},
	abstract = {Scientific analysis of the linguistic styles of different authors has generated considerable interest. We present a generic approach to measuring the similarity of two symbolic sequences that requires minimal background knowledge about a given human language. Our analysis is based on word rank order–frequency statistics and phylogenetic tree construction. We demonstrate the applicability of this method to historic authorship questions related to the classic Chinese novel “The Dream of the Red Chamber,” to the plays of William Shakespeare, and to the Federalist papers. This method may also provide a simple approach to other large databases based on their information content.},
	pages = {473--483},
	number = {3},
	journaltitle = {Physica A: Statistical Mechanics and its Applications},
	shortjournal = {Physica A: Statistical Mechanics and its Applications},
	author = {Yang, Albert C.-C and Peng, C.-K and Yien, H.-W and Goldberger, Ary L},
	urldate = {2013-10-01},
	date = {2003-11-15},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@book{orlandi_informatica_1990,
	location = {Roma},
	title = {Informatica umanistica},
	isbn = {9788843008872},
	url = {http://books.google.com/books?id=XHrDPAAACAAJ},
	series = {Studi superiori},
	pagetotal = {188},
	publisher = {Nuova Italia scientifica},
	author = {Orlandi, Tito},
	date = {1990},
	langid = {italian},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@book{orlandi_informatica_2010,
	location = {Roma ; Bari},
	title = {Informatica testuale : teoria e prassi},
	isbn = {9788842093794 8842093793},
	url = {http://www.ibs.it/code/9788842093794/orlandi-tito/informatica-testuale-teoria.html},
	shorttitle = {Informatica testuale},
	abstract = {Tito Orlandi propone una sintesi chiara delle metodologie che governano l'applicazione degli strumenti informatici in ambito testuale, in particolar modo si occupa della cosiddetta testualità riflessa o mediata, ossia di quei testi concepiti per essere diffusi su supporto cartaceo e solo in un secondo momento essere riprodotti in un ambiente informatico, così da diventare anche testi digitali. L'informatica nel suo impatto con le applicazioni umanistiche ha avuto infatti conseguenze innovative e in costante mutamento. Il vasto mondo che ruota attorno alla funzione, produzione e ricezione del testo sta subendo una trasformazione radicale dovuta alle tecnologie, ma pochi sono in grado di apprezzarne compiutamente le origini e le ricadute sulla letteratura tutta. Il volume, insieme alla considerazione teorica del tema, offre una guida pratica alle procedure applicative sempre valide, nonostante i quotidiani cambiamenti nelle macchine informatiche.},
	publisher = {Laterza},
	author = {Orlandi, Tito},
	date = {2010},
	langid = {italian},
	keywords = {X-{CHECK}, meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{mosteller_inference_1963,
	title = {Inference in an Authorship Problem},
	volume = {58},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2283270},
	doi = {10.2307/2283270},
	abstract = {This study has four purposes: to provide a comparison of discrimination methods; to explore the problems presented by techniques based strongly on Bayes' theorem when they are used in a data analysis of large scale; to solve the authorship question of The Federalist papers; and to propose routine methods for solving other authorship problems. Word counts are the variables used for discrimination. Since the topic written about heavily influences the rate with which a word is used, care in selection of words is necessary. The filler words of the language such as an, of, and upon, and, more generally, articles, prepositions, and conjunctions provide fairly stable rates, whereas more meaningful words like war, executive, and legislature do not. After an investigation of the distribution of these counts, the authors execute an analysis employing the usual discriminant function and an analysis based on Bayesian methods. The conclusions about the authorship problem are that Madison rather than Hamilton wrote all 12 of the disputed papers. The findings about methods are presented in the closing section on conclusions. This report, summarizing and abbreviating a forthcoming monograph [8], gives some of the results but very little of their empirical and theoretical foundation. It treats two of the four main studies presented in the monograph, and none of the side studies.},
	pages = {275--309},
	number = {302},
	journaltitle = {Journal of the American Statistical Association},
	author = {Mosteller, Frederick and Wallace, David L.},
	urldate = {2011-12-08},
	date = {1963-06},
	langid = {english},
	note = {{ArticleType}: research-article / Full publication date: Jun., 1963 / Copyright © 1963 American Statistical Association},
	keywords = {{AnalyzeStatistically}, bigdata, meta\_Theorizing, t\_Stylometry},
}

@report{crow_income_2009,
	location = {Washington, D.C.},
	title = {Income Models for Open Access: An Overview of Current Practice},
	url = {http://sparc.arl.org/resources/papers-guides/oa-income-models},
	abstract = {“How do we pay for Open Access?” is a key question faced by publishers, authors, and libraries as awareness and interest in free, immediate, online access to scholarly research increases. {SPARC} (the Scholarly Publishing and Academic Resources Coalition) examines the issue of sustainability for current and prospective open-access publishers in a timely new guide, “Income models for Open Access: An overview of current practice,” by Raym Crow.

“Income models for Open Access: An overview of current practice” examines the use of supply-side revenue streams (such as article processing fees, advertising) and demand-side models (including versioning, use-triggered fees). The guide provides an overview of income models currently in use to support open-access journals, including a description of each model along with examples of journals currently employing it.},
	institution = {Scholarly Publishing and Academic Resources Coalition},
	author = {Crow, Raym},
	date = {2009},
	langid = {english},
	keywords = {meta\_GiveOverview, t\_Sustainability},
}

@article{smith_improving_2011,
	title = {Improving Authorship Attribution: Optimizing Burrows' Delta Method*},
	volume = {18},
	issn = {0929-6174, 1744-5035},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09296174.2011.533591},
	doi = {10.1080/09296174.2011.533591},
	shorttitle = {Improving Authorship Attribution},
	abstract = {Burrows' Delta Method (Burrows, 2002) is a leading method of authorship attribution. It can be used to shortlist potential authors from a list or to even identify potential authors. The technique has been extended by Hoover (2004a, 2006). In this investigation, we look at the choice of words for the word vector used, the size of the word vector, the similarity measure and the impact of corpus choice on the accuracy of text classification. Our results show a word frequency vector of between 200 and 300 words give the most accurate results (Aldridge, 2007). We also demonstrate a dramatic improvement in accuracy by adapting Burrows' Delta to the cosine similarity measure. Additionally, our results indicate areas where the word vector can be optimized still further for more accurate results.},
	pages = {63--88},
	number = {1},
	journaltitle = {Journal of Quantitative Linguistics},
	author = {Smith, Peter W. H. and Aldridge, W.},
	urldate = {2012-11-12},
	date = {2011-02},
	langid = {english},
}
@article{faulstich_implementing_2006,
	title = {Implementing a Linguistic Query Language for Historic Texts},
	volume = {4254},
	url = {http://link.springer.com/chapter/10.1007%2F11896548_45},
	abstract = {We describe the design and implementation of the linguistic query language {DDDquery}. This language aims at querying a large linguistic database storing a corpus of richly annotated historic German texts. We use a graph-based data model that supports multiple independent annotation layers on a shared text layer as well as alignments of text layers representing the same text or related texts (e.g., translations). The corpus is stored in an object-relational database system with a text retrieval extension.

{DDDquery} is based on {XPath} to leverage the familiarity of many users with this language. It is translated to {SQL} in a two phase compilation with first order logic as an intermediate language. This approach effectively decouples the query language from the schema of the underlying corpus.

We provide an overview of {DDDquery}, the underlying {ODAG} data model, its implementation as relational schema, the predicates of the intermediate language, and describe both phases of the translation proces},
	pages = {601--612},
	journaltitle = {Springer 2006},
	author = {Faulstich, Lukas C. and Leser, Ulf and Vitt, Thorsten},
	date = {2006},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, act\_Query/Retrieve, obj\_Text},
}

@report{van_der_maaten_identifying_2009,
	location = {Tilburg},
	title = {Identifying the real van Gogh with brushstroke textons},
	url = {http://lyrawww.uvt.nl/~cenv/ticc/reports/TRvdrMaaten.pdf},
	abstract = {The visual examination of paintings is traditionally performed by skilled art histo-
rians using their eyes. Recent advances in intelligent systems may support art his-
torians in determining the authenticity or date of creation of paintings. We propose
the brushstroke textons method that builds a codebook of textons, i.e., representative
patches, of a collection of paintings and represents paintings in terms of texton his-
tograms. In addition, the method visualizes the similarities between the histogram
representations of paintings by employing a recently proposed dimensionality reduc-
tion technique. The effectiveness of the brushstroke textons method is demonstrated
on a collection of digitized high-resolution reproductions of paintings by Van Gogh
and his contemporaries. The results show a clear separation of paintings created by
Van Gogh and those created by other painters. In addition, the period of creation is
faithfully reflected in the visualization. These promising results show that the tex-
ton brushstroke method, in particular when extended with color and global textural
features, offers a new tool for art historians in support of their study of paintings.},
	pages = {10},
	institution = {Tilburg centre for Creative Computing},
	author = {van der Maaten, Laurens and Postma, Eric},
	date = {2009},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Images},
}

@article{mccarty_humanities_2002,
	title = {Humanities Computing: Essential Problems, Experimental Practice},
	volume = {17},
	url = {http://llc.oxfordjournals.org/content/17/1/103.abstract},
	doi = {10.1093/llc/17.1.103},
	shorttitle = {Humanities Computing},
	abstract = {The application of computing to the disciplines of the humanities has two principal outcomes: useful results for the field of application and failures completely to demonstrate what is known. These failures, an inevitable feature of modelling, point to the key question for humanities computing, how we know what we know, and so to the beginning of its own scholarly enquiry. This, I argue, proceeds along three branches, the algorithmic, the metatextual, and the representational. Examining the first of these here I argue for research toward an open‐ended, interoperable set of primitives based on previous work in the field and designed for the emerging digital library environment. To set the stage for their further development I argue that the field as a whole does not wait on a theoretical formulation of what humanists do, rather should look to the tradition of experimental knowledge‐making as this has been illuminated in recent years by historians, philosophers, and sociologists of science.},
	pages = {103 --125},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	author = {{McCarty}, Willard},
	urldate = {2012-02-08},
	date = {2002-04-01},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@article{svensson_humanities_2009,
	title = {Humanities Computing as Digital Humanities},
	volume = {3},
	url = {http://digitalhumanities.org/dhq/vol/3/3/000065/000065.html},
	abstract = {This article presents an examination of how digital humanities is currently conceived and described, and examines the discursive shift from humanities computing to digital humanities. It is argued that this renaming of humanities computing as digital humanities carries with it a set of epistemic commitments that are not necessarily compatible with a broad and inclusive notion of the digital humanities. In particular, the author suggests that tensions arise from the instrumental, textual and methodological focus of humanities computing as well as its relative lack of engagement with the "digital" as a study object. This article is the first in a series of four articles attempting to describe and analyze the field of digital humanities and digital humanities as a transformative practice.},
	number = {3},
	journaltitle = {Digital Humanities Quarterly},
	author = {Svensson, Patrik},
	urldate = {2010-01-13},
	date = {2009},
	langid = {english},
}

@book{mccarty_humanities_2005,
	location = {Basingstoke \& New York},
	title = {Humanities computing},
	isbn = {9781403935045},
	url = {http://www.mccarty.org.uk/essays/McCarty,%20Humanities%20computing.pdf},
	publisher = {Palgrave Macmillan},
	author = {{McCarty}, Willard},
	date = {2005},
	langid = {english},
	keywords = {*****, act\_Conceptualizing, act\_Modeling, meta\_GiveOverview, obj\_DigitalHumanities, obj\_Methods},
}

@article{davidson_humanities_2008,
	title = {Humanities 2.0: Promise, Perils, Predictions},
	volume = {123},
	issn = {0030-8129},
	url = {http://www.mlajournals.org/doi/abs/10.1632/pmla.2008.123.3.707},
	doi = {10.1632/pmla.2008.123.3.707},
	shorttitle = {Humanities 2.0},
	pages = {707--717},
	number = {3},
	journaltitle = {{PMLA}},
	shortjournal = {{PMLA}},
	author = {Davidson, Cathy N.},
	urldate = {2011-07-06},
	date = {2008-05},
	langid = {english},
	keywords = {*****, meta\_Advocating, meta\_GiveOverview, obj\_DigitalHumanities, obj\_Humanities},
}

@book{brossaud_humanites_2007,
	location = {Paris},
	title = {Humanités numériques : Volume 1, Nouvelles technologies cognitives et épistémologie},
	isbn = {9782746216617},
	shorttitle = {Humanités numériques},
	publisher = {Hermes Science Publications},
	author = {Brossaud, Claire and Reber, Bernard},
	date = {2007},
	langid = {french},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@book{hayles_how_2012,
	location = {Chicago; London},
	title = {How we think : digital media and contemporary technogenesis},
	isbn = {9780226321400  9780226321424  0226321401  0226321428},
	shorttitle = {How we think},
	abstract = {"How do we think? " N. Katherine Hayles poses this question at the beginning of this bracing exploration of the idea that we think through, with, and alongside media. As the age of print passes and new technologies appear every day, this proposition has become far more complicated, particularly for the traditionally print-based disciplines in the humanities and qualitative social sciences. With a rift growing between digital scholarship and its print-based counterpart, Hayles argues for contemporary technogenesis-the belief that humans and technics are coevolving-and advocates for what she calls comparative media studies, a new approach to locating digital work within print traditions and vice versa. Hayles examines the evolution of the field from the traditional humanities and how the digital humanities are changing academic scholarship, research, teaching, and publication. She goes on to depict the neurological consequences of working in digital media, where skimming and scanning, or "hyper reading," and analysis through machine algorithms are forms of reading as valid as close reading once was. Hayles contends that we must recognize all three types of reading and understand the limitations and possibilities of each. In addition to illustrating what a comparative media perspective entails, Hayles explores the technogenesis spiral in its full complexity. She considers the effects of early databases such as telegraph code books and confronts our changing perceptions of time and space in the digital age, illustrating this through three innovative digital productions-Steve Tomasula's electronic novel, T.O.C.; Steven Hall's The Raw Shark Texts; and Mark Z. Danielewski's Only Revolutions. Deepening our understanding of the extraordinary transformative powers digital technologies have placed in the hands of humanists, How We Think presents a cogent rationale for tackling the challenges facing the humanities today.},
	publisher = {The University of Chicago Press},
	author = {Hayles, N. Katherine},
	date = {2012},
	langid = {english},
	keywords = {act\_Conceptualizing, meta\_GiveOverview, obj\_DigitalHumanities, obj\_Methods},
}

@online{gauntlett_how_2012,
	title = {How to move towards a system that looks to ‘publish, then filter’ academic research},
	url = {http://blogs.lse.ac.uk/impactofsocialsciences/2012/07/10/publish-then-filter-research/},
	abstract = {Both the ‘green’ and the ‘gold’ models of open access tend to preserve the world of academic journals, where anonymous reviewers typically dictate what may appear. David Gauntlett looks forward to a system which gets rid of them altogether.},
	titleaddon = {Impact of Social Sciences},
	author = {Gauntlett, David},
	date = {2012-07},
	langid = {english},
}

@article{anonymous_how_2013,
	title = {How Rowling was unmasked},
	url = {http://www.bbc.co.uk/news/entertainment-arts-23313074},
	abstract = {Professor Peter Millican of Hertford College, University of Oxford, helped unmask male debut writer Robert Galbraith as {JK} Rowling.},
	journaltitle = {{BBC}},
	author = {{Anonymous}},
	urldate = {2013-07-16},
	date = {2013-07-15},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@incollection{durusau_how_2006,
	location = {New York},
	title = {How and Why to Formalize Your Markup},
	isbn = {9780873529709},
	url = {http://www.tei-c.org/About/Archive_new/ETE/Preview/durusau.xml},
	abstract = {Why markup decisions should be formalized
Formalizing markup: a process of discovery
Formalizing markup: a process of training
Formalizing markup: a process of validation
Formalizing markup: a process of review
Conclusion},
	booktitle = {Electronic textual editing},
	publisher = {Modern Language Association of America},
	author = {Durusau, Patrick},
	editor = {Burnard, Lou},
	date = {2006},
	langid = {english},
	keywords = {act\_Modeling},
}

@online{duering_historical_2013,
	title = {Historical Network Research Bibliography},
	url = {http://historicalnetworkresearch.org/bibliography/},
	abstract = {On this page you will find the to-date largest collection of articles related to the application  of Social Network Analysis in the historical disciplines. However, it is far from perfect and complete. Get involved and help improve it!},
	titleaddon = {Historical Network Research Bibliography},
	author = {Duering, Marten},
	date = {2013},
	langid = {english},
	keywords = {obj\_AnyObject},
}

@inproceedings{rogati_high-performing_2002,
	location = {New York, {NY}, {USA}},
	title = {High-performing feature selection for text classification},
	isbn = {1-58113-492-4},
	url = {http://doi.acm.org/10.1145/584792.584911},
	doi = {10.1145/584792.584911},
	series = {{CIKM} '02},
	abstract = {This paper reports a controlled study on a large number of filter feature selection methods for text classification. Over 100 variants of five major feature selection criteria were examined using four well-known classification algorithms: a Naive Bayesian ({NB}) approach, a Rocchio-style classifier, a k-nearest neighbor ({kNN}) method and a Support Vector Machine ({SVM}) system. Two benchmark collections were chosen as the testbeds: Reuters-21578 and small portion of Reuters Corpus Version 1 ({RCV}1), making the new results comparable to published results. We found that feature selection methods based on chi2 statistics consistently outperformed those based on other criteria (including information gain) for all four classifiers and both data collections, and that a further increase in performance was obtained by combining uncorrelated and high-performing feature selection methods.The results we obtained using only 3\% of the available features are among the best reported, including results obtained with the full feature set.},
	pages = {659--661},
	booktitle = {Proceedings of the eleventh international conference on Information and knowledge management},
	publisher = {{ACM}},
	author = {Rogati, Monica and Yang, Yiming},
	urldate = {2012-12-03},
	date = {2002},
	langid = {english},
}

@article{mcguigan_hateful_2013,
	title = {Hateful metrics and the bitterest pill of scholarly publishing},
	volume = {31},
	issn = {0810-9028, 1470-1030},
	url = {http://www.tandfonline.com/action/showCopyRight?doi=10.1080%2F08109028.2014.891711#tabModule},
	doi = {10.1080/08109028.2014.891711},
	abstract = {Glenn {McGuigan} is a specialist business librarian at Penn State who has written about change in university libraries, and about the impact of the academic publishing industry on scholarly publishing.},
	pages = {249--256},
	number = {3},
	journaltitle = {Prometheus},
	author = {{McGuigan}, Glenn S.},
	urldate = {2014-06-19},
	date = {2013-09},
	langid = {english},
	keywords = {act\_Publishing, goal\_Dissemination, obj\_ResearchResults},
}

@article{heimburger_has_2012,
	title = {Has the Historian’s craft gone digital? Some observations from France},
	volume = {10},
	url = {http://www.studistorici.com/2012/06/29/heimburger-ruiz_numero_10/},
	abstract = {Since the end of the 1980s the historiographical context has changed considerably. Over the course of the last ten years, we have reached the “digital age” and computers as well as resources available via the Internet have become indispensable tools for all researchers. Be it for the stage of documentation or for actual writing, we are now living and working in a context where historians can no longer completely refuse all {IT} tools. As long as there are no solid, durable, large-scale training efforts to equip all historians with the skills to use the new and old {IT} tools, their potential is necessarily limited. While there have been studies on “researchers” in general and also on political scientists in particular, there has, to our knowledge, been no scientific study which would allow us to reach conclusions on the use of {IT} tools and digital resources by French historians. It is thus difficult to reach conclusions on a larger scale and we have decided to base our analysis on our own experience in order to consider what could be the transformations of the historian’s craft in the digital age. We will thus proceed first to a series of conclusions based on our activities in mediation (teaching and blogging), before proposing a typology of the principal evolutions. We will conclude with a certain number of propositions as far as training of historians is concerned.},
	number = {2},
	journaltitle = {Diachronie},
	author = {Heimburger, Franziska and Ruiz, Émilien},
	date = {2012},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{brunner_automatic_2013,
	title = {Automatic recognition of speech, thought, and writing representation in German narrative texts},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/early/2013/05/18/llc.fqt024},
	doi = {10.1093/llc/fqt024},
	abstract = {This article presents the main results of a project, which explored ways to recognize and classify a narrative feature—speech, thought, and writing representation ({ST}\&{WR})—automatically, using surface information and methods of computational linguistics. The task was to detect and distinguish four types—direct, free indirect, indirect, and reported {ST}\&{WR}—in a corpus of manually annotated German narrative texts. Rule-based as well as machine-learning methods were tested and compared. The results were best for recognizing direct {ST}\&{WR} (best F1 score: 0.87), followed by indirect (0.71), reported (0.58), and finally free indirect {ST}\&{WR} (0.40). The rule-based approach worked best for {ST}\&{WR} types with clear patterns, like indirect and marked direct {ST}\&{WR}, and often gave the most accurate results. Machine learning was most successful for types without clear indicators, like free indirect {ST}\&{WR}, and proved more stable. When looking at the percentage of {ST}\&{WR} in a text, the results of machine-learning methods always correlated best with the results of manual annotation. Creating a union or intersection of the results of the two approaches did not lead to striking improvements. A stricter definition of {ST}\&{WR}, which excluded borderline cases, made the task harder and led to worse results for both approaches.},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Brunner, Annelen},
	urldate = {2013-08-12},
	date = {2013-05-18},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@article{dahllof_automatic_2012,
	title = {Automatic prediction of gender, political affiliation, and age in Swedish politicians from the wording of their speeches--A comparative study of classifiability},
	volume = {27},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/cgi/doi/10.1093/llc/fqs010},
	doi = {10.1093/llc/fqs010},
	abstract = {The present study explores automatic classification of Swedish politicians and their speeches into classes based on personal traits—gender, age, and political affiliation—as a means for measuring and analyzing how these traits influence language use. Support Vector Machines classified 200-word passages, represented by binary bag-of-word-forms vectors. Different feature selections were tried. The performance of the classifiers was assessed using test data from authors unseen in the training data. Author-level predictions derived from twenty-one text-level predictions reached an accuracy rate of 81.2\% for gender, 89.4\% for political affiliation, and 78.9\% for age. Classification concerning each basic distinction was applied to general populations of politicians and to cohorts defined by the other classes. The outcomes suggest that the extent to which these personal traits are expressed in language use varies considerably among the different cohorts and that different traits affect different layers of the vocabulary. The accuracy rates for gender classification were higher for the right wing and older cohorts than for the opposite ones. Age prediction gave higher accuracy for the right wing cohort. Political classification gave the highest accuracy rates when all forms were included in the feature sets, whereas feature sets restricted to verbs or function words gave the highest scores for gender prediction, and the lowest ones for political classification.},
	pages = {139--153},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Dahllof, M.},
	urldate = {2012-06-12},
	date = {2012-04-06},
	langid = {english},
	keywords = {bigdata},
}

@article{ball_automated_1994,
	title = {Automated Text Analysis: Cautionary Tales},
	volume = {9},
	url = {http://llc.oxfordjournals.org/content/9/4/295.abstract},
	doi = {10.1093/llc/9.4.295},
	abstract = {The increasing availability of electronic text and text analysis tools has made it possible to analyse vast amounts of data in a short amount of time. However, natural language processing is not a solved problem, and even large research systems representing decades of development do not perform at the level of human language processors. Since such systems are not sufficiently robust for general use, most literary and linguistic corpus analysts make use of heuristics and simple tools for text analysis. But while such ‘shallow’ approaches offer improvements in speed and accuracy over traditional manual methods, there are many pitfalls for the unwary. In this paper we consider some pitfalls and temptations that attend the automated analysis of large text corpora: sample size, the recall problem, analysing only what is easy to find, and counting what is easiest to count. We suggest that, given the state of the art in text processing tools, such tools must be used with a full awareness of their limitations, and should be coupled with or replaced by manual methods when appropriate.},
	pages = {295--302},
	number = {4},
	journaltitle = {Literary and Linguistic Computing},
	author = {Ball, C.N.},
	urldate = {2011-05-17},
	date = {1994},
	langid = {english},
	keywords = {bigdata, goal\_Analysis, meta\_Assessing},
}

@article{craig_authorial_1999,
	title = {Authorial attribution and computational stylistics: if you can tell authors apart, have you learned anything about them?},
	volume = {14},
	url = {http://llc.oxfordjournals.org/content/14/1/103.abstract},
	doi = {10.1093/llc/14.1.103},
	shorttitle = {Authorial attribution and computational stylistics},
	abstract = {Within stylometrics, the disciplines of authorial attribution and descriptive stylistics hitherto have been pursued separately. The first has achieved mainstream status within literary studies, while the second is little recognized. A study of the plays of Thomas Middleton compared with a large control sample shows that it is possible to achieve a good classification of his work in 2,000 word segments as against those of his contemporaries, using frequencies of very common words. Such a result can serve as the basis both for testing Middleton's hand in some disputed plays and for a description of his style. Most of The Revenger's Tragedy, part of The Yorkshire Tragedy, and all of The Second Maiden's Tragedy prove to be in a style similar to that of Middleton's uncontested plays. This style, judging by an examination of instances in context of the ten word types most strongly correlated with the Middleton-other discriminant function, is (among other things) rich in deictics and poor in conjunctions, features readily accommodated to previous descriptions including Middleton's own. It is concluded that classification and description can be mutually supportive: the first confirms the validity of the second, while the second helps to establish the stylistic mechanisms underlying a successful classification.},
	pages = {103 --113},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	author = {Craig, Hugh},
	urldate = {2012-02-08},
	date = {1999-04-01},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, meta\_Theorizing},
}

@article{science_and_technology_art_2011,
	title = {Art criticism and computers: Painting by numbers. Digital analysis is invading the world of the connoisseur},
	url = {http://www.economist.com/node/21524699},
	abstract = {{UDGING} artistic styles, and the similarities between them, might be thought one bastion of human skill that machines could never storm. Not so, if Lior Shamir at Lawrence Technological University in Michigan is correct. A paper he has just published in Leonardo suggests that computers may have just as good an eye for style as humans do—and, in some cases, may see connections between artists that human critics have missed.},
	pages = {67},
	journaltitle = {The Economist},
	author = {Science \{and\} Technology},
	urldate = {2011-08-22},
	date = {2011-07-30},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_RelationalAnalysis, bigdata{\textasciitilde}, obj\_Images, t\_Stylometry},
}

@book{herkt_anwendungsmoglichkeiten_1991,
	location = {Bochum},
	title = {Anwendungsmöglichkeiten computergestützter Erfassungs- und Auswertungshilfen am Beispiel der Güter- und Einkünfteverzeichnisse des Kollegiatstiftes St. Mauritz in Münster},
	isbn = {3-88339-902-7},
	url = {http://brockmeyer-verlag.de/shop/article_406/Herkt,-Matthias%3A-Anwendungsm%C3%B6glichkeiten-computergest%C3%BCtzter-Erfassungs-u.-Auswertungshilfen-am-Beispiel-der-G%C3%BCter--und-Eink%C3%BCnfteverzeichnisse-des-Kollegiatstiftes-St.-Mauritz-in-M%C3%BCnster..html?shop_param=cid%3D20%26aid%3D406%26},
	series = {Bochumer historische Studien / Mittelalterliche Geschichte},
	abstract = {Wirtschaftsgeschichtliche Massenquellen zur Grundbesitz- und Einkünfteentwicklung einer bedeutenden geistlichen Institution des westfälischen Raumes werden hier untersucht. Es geht dabei auch um die Nützlichkeit moderner Datenbank-Verwaltungssysteme für die Organisation von Massenquellen zum Zweck der quantitativen Analyse im Bereich der Historiographie des Mittelalters.},
	number = {9},
	publisher = {Brockmeyer},
	author = {Herkt, Matthias},
	date = {1991},
	langid = {german},
	note = {Bochum, Univ., Diss., 1991},
	keywords = {{AnalyzeStatistically}, bigdata, obj\_Documents},
}

@article{cohen_analyzing_2010,
	title = {Analyzing Literature by Words and Numbers - Humanities 2.0},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2010/12/04/books/04victorian.html},
	abstract = {Victorian Literature, Statistically Analyzed With New Process},
	journaltitle = {The New York Times},
	author = {Cohen, Patricia},
	urldate = {2011-04-06},
	date = {2010-12-03},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata{\textasciitilde}, meta\_GiveOverview},
}

@book{baayen_analyzing_2008,
	location = {Cambridge},
	edition = {1. publ.},
	title = {Analyzing linguistic data. A practical introduction to statistics using R},
	isbn = {978-0-521-70918-7},
	publisher = {Cambridge University Press},
	author = {Baayen, R Harald},
	date = {2008},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, meta\_GiveOverview, obj\_Language},
}

@article{burrows_ocean_1989,
	title = {‘An ocean where each kind. . .’: Statistical analysis and some major determinants of literary style},
	volume = {23},
	issn = {0010-4817, 1572-8412},
	url = {http://www.springerlink.com/content/7121357k22l8u511/},
	doi = {10.1007/BF02176636},
	shorttitle = {‘An ocean where each kind. . .’},
	abstract = {The statistical analysis of literary texts has yielded valuable results, not least when it has treated of the frequency patterns of very common words. But, whereas particular frequency patterns have usually been examined as discrete phenomena, it is possible to correlate the frequency profiles of all the very common words, to subject the resulting correlation matrix to eigen analysis, and to present the results in graphic form. The specimens offered here deal, first, with differences among Jane Austen's characters and, secondly, with differences between authors. The most striking general differences among the authors studied relate to historical eras and authorial gender.},
	pages = {309--321},
	number = {4},
	journaltitle = {Computers and the Humanities},
	author = {Burrows, J. F.},
	urldate = {2011-12-08},
	date = {1989-08},
	langid = {english},
	keywords = {bigdata, t\_Stylometry},
}

@article{guyon_introduction_2003,
	title = {An introduction to variable and feature selection},
	volume = {3},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=944919.944968},
	abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
	pages = {1157--1182},
	journaltitle = {J. Mach. Learn. Res.},
	author = {Guyon, Isabelle and Elisseeff, André},
	urldate = {2012-12-03},
	date = {2003-03},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}
@article{feinerer_introduction_2008,
	title = {An Introduction to Text Mining in R},
	volume = {8},
	url = {http://140.247.115.171/CRAN/doc/Rnews/Rnews_2008-2.pdf#page=19},
	abstract = {Text mining has gained big interest both in aca-
demic research as in business intelligence applica-
tions within the last decade. There is an enormous
amount of textual data available in machine readable
format which can be easily accessed via the Internet
or databases. This ranges from scientific articles, ab-
stracts and books to memos, letters, online forums,
mailing lists, blogs, and other communication media
delivering sensible information.
Text mining is a highly interdisciplinary research
field utilizing techniques from computer science, lin-
guistics, and statistics. For the latter R is one of the
leading computing environments offering a broad
range of statistical methods. However, until recently,
R has lacked an explicit framework for text mining
purposes. This has changed with the
tm
(Feinerer,
2008; Feinerer et al., 2008) package which provides
a text mining infrastructure for R. This allows R
users to work efficiently with texts and correspond-
ing meta data and transform the texts into structured
representations where existing R methods can be ap-
plied, e.g., for clustering or classification.
In this article we will give a short overview of the
tm package. Then we will present two exemplary
text mining applications, one in the field of stylome-
try and authorship attribution, the second is an anal-
ysis of a mailing list. Our aim is to show that
tm provides the necessary abstraction over the actual text
management so that the reader can use his own texts
for a wide variety of text mining techniques.},
	pages = {19--22},
	number = {2},
	journaltitle = {Rnews},
	author = {Feinerer, Ingo},
	date = {2008},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@thesis{deininghaus_interactive_2010,
	location = {Aachen, {RWTH} Aachen University},
	title = {An Interactive Surface for Literary Criticism},
	url = {http://hci.rwth-aachen.de/materials/publications/deininghaus2010b.pdf},
	abstract = {Professionals in literary studies regularly need to work with large amounts of
complex text. Special forms of editions have been developed to make these texts
accessible to the literary critic. Nowadays, there exist both traditional printed
editions and digital editions for use with standard computers.
In this thesis, we present an examination of the characteristics of these editions,
their users, and the users’ work processes from an {HCI} perspective. Based on these
findings, we propose a design for a working environment that enables the user to
read, navigate, and personalize printed and digital information on one integrative
interactive surface, benefiting from the respective advantages of both mediums.
This also leads to a promising new way of structuring content in text editions.
Furthermore, we portray the evaluation and refinement of this proposal through
two full design iterations using prototypes — each iteration backed by a qualitative
user study — and outline possible future work.},
	pagetotal = {120},
	type = {Diploma Thesis},
	author = {Deininghaus, Stephan},
	date = {2010},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, bigdata{\textasciitilde}, obj\_Literature, obj\_Tools, t\_Usability},
}

@article{forman_extensive_2003,
	title = {An extensive empirical study of feature selection metrics for text classification},
	volume = {3},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=944919.944974},
	abstract = {Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, {TREC}, {OHSUMED}, etc. The results are analyzed from multiple goal perspectives-accuracy, F-measure, precision, and recall-since each is appropriate in different situations. The results reveal that a new feature selection metric we call 'Bi-Normal Separation' ({BNS}), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, {BNS} was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, {BNS} is consistently a member of the pair---e.g., for greatest recall, the pair {BNS} + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin.},
	pages = {1289--1305},
	journaltitle = {J. Mach. Learn. Res.},
	author = {Forman, George},
	urldate = {2012-12-03},
	date = {2003-03},
	langid = {english},
	keywords = {bigdata},
}

@online{priem_alt-metrics:_2010,
	title = {Alt-metrics: A Manifesto},
	url = {http://altmetrics.org/manifesto/},
	titleaddon = {altmetrics.org},
	author = {Priem, Jason and Taraborelli, D. and Groth, P. and Neylon, C.},
	date = {2010},
	langid = {english},
	keywords = {bigdata{\textasciitilde}},
}

@article{dunning_accurate_1993,
	title = {Accurate Methods for the Statistics of Surprise and Coincidence},
	volume = {19},
	url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.5962},
	abstract = {Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.},
	pages = {61--74},
	number = {1},
	journaltitle = {{COMPUTATIONAL} {LINGUISTICS}},
	author = {Dunning, Ted},
	date = {1993},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, t\_Stylometry},
}

@article{waltman_unified_2010,
	title = {A unified approach to mapping and clustering of bibliometric networks},
	volume = {4},
	issn = {17511577},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1751157710000660},
	doi = {10.1016/j.joi.2010.07.002},
	abstract = {In the analysis of bibliometric networks, researchers often use mapping and clustering techniques in a combined fashion. Typically, however, mapping and clustering techniques that are used together rely on very different ideas and assumptions. We propose a unified approach to mapping and clustering of bibliometric networks. We show that the {VOS} mapping technique and a weighted and parameterized variant of modularity-based clustering can both be derived from the same underlying principle. We illustrate our proposed approach by producing a combined mapping and clustering of the most frequently cited publications that appeared in the field of information science in the period 1999-2008. (C) 2010 Elsevier Ltd. All rights reserved.},
	pages = {629--635},
	number = {4},
	journaltitle = {Journal of Informetrics},
	author = {Waltman, Ludo and van Eck, Nees Jan and Noyons, Ed C.M.},
	urldate = {2013-04-05},
	date = {2010-10},
	langid = {english},
	keywords = {act\_Visualizing, bigdata},
}

@book{mcgann_new_2014,
	location = {Boston, {MA}},
	title = {A New Republic of Letters. Memory and Scholarship in the Age of Digital Reproduction},
	url = {http://www.hup.harvard.edu/catalog.php?isbn=9780674728691},
	abstract = {Jerome {McGann}'s manifesto argues that the history of texts and how they are preserved and accessed for interpretation are the overriding subjects of humanist study in the digital age. Theory and philosophy no longer suffice as an intellectual framework. But philology -- out of fashion for decades -- models these concerns with surprising fidelity.},
	publisher = {Harvard Univ. Press},
	author = {{McGann}, Jerome},
	urldate = {2014-03-30},
	date = {2014},
	langid = {english},
	keywords = {bigdata{\textasciitilde}, meta\_Assessing, meta\_GiveOverview, obj\_DigitalHumanities, obj\_Text},
}

@book{bod_new_2013,
	location = {Oxford},
	title = {A new history of the humanities: the search for principles and patterns from Antiquity to the present},
	isbn = {9780199665211  0199665214},
	shorttitle = {A new history of the humanities},
	abstract = {Many histories of science have been written, but A New History of the Humanities offers the first overarching history of the humanities from Antiquity to the present. There are already historical studies of musicology, logic, art history, linguistics, and historiography, but this volume gathers these, and many other humanities disciplines, into a single coherent account.

Its central theme is the way in which scholars throughout the ages and in virtually all civilizations have sought to identify patterns in texts, art, music, languages, literature, and the past. What rules can we apply if we wish to determine whether a tale about the past is trustworthy? By what criteria are we to distinguish consonant from dissonant musical intervals? What rules jointly describe all possible grammatical sentences in a language? How can modern digital methods enhance pattern-seeking in the humanities? Rens Bod contends that the hallowed opposition between the sciences (mathematical, experimental, dominated by universal laws) and the humanities (allegedly concerned with unique events and hermeneutic methods) is a mistake born of a myopic failure to appreciate the pattern-seeking that lies at the heart of this inquiry. A New History of the Humanities amounts to a persuasive plea to give Panini, Valla, Bopp, and countless other often overlooked intellectual giants their rightful place next to the likes of Galileo, Newton, and Einstein.},
	publisher = {Oxford Univ. Press},
	author = {Bod, Rens},
	date = {2013},
	langid = {english},
	keywords = {bigdata{\textasciitilde}, meta\_Assessing, obj\_DigitalHumanities},
}

@thesis{simonis_framework_2004,
	title = {A framework for processing and presenting parallel text corpora},
	url = {https://publikationen.uni-tuebingen.de/xmlui/handle/10900/48620},
	abstract = {Diese Arbeit stellt ein erweiterbares System für die Bearbeitung und Präsentation von multi-modalen, parallelen Textkorpora vor. Es kann dazu verwendet werden um digitale Dokumente in vielerlei Formaten wie zum Beispiel einfache Textdateien, {XML}-Dateien oder Graphiken zu bearbeiten wobei bearbeiten in diesem Zusammenhang vor allem strukturieren und verlinken bedeutet. Diese Strukturierung nach einem neu entwickelten Kodierungschema kann zum Beispiel auf formalen, linguistischen, semantischen, historischen oder auch vielen anderen Gesichtspunkten beruhen. Die Dokumente können gleichzeitig mit beliebig vielen parallelen und sich möglicherweise auch überlappenden Strukturen versehen werden und bezüglich jeder dieser Strukturen auch miteinander verknüpft werden. Die unterschiedlichen Strukturen können je nach Art entweder automatisch oder halbautomatisch erzeugt werden oder sie können vom Benutzer manuell spezifiziert werden. Als Grundlage des vorgestellten Systems dient {XTE}, ein einfaches aber zugleich mächtiges, externe Kodierungsschema das sowohl als eine {XML} {DTD} als auch als ein {XML} Schema verwirklicht wurde. {XTE} ist besonders zum Kodieren von vielen, sich gegenseitig überlappenden Hierarchien in multi-modalen Dokumenten und zum Verknüpfen dieser Strukturen über mehrere Dokumente hinweg, geeignet. Zusammen mit {XTE} wurden zwei ausgereifte Anwendungen zum Betrachten und Bearbeiten von {XTE}-kodierten Dokumenten sowie zum komfortablen Arbeiten mit den so erstellten Ergebnisdokumenten geschaffen. Diese Anwendungen wurden als anpassbares und erweiterbares System konzipiert, das möglichst einfach für andere Einsatzgebiete und an neue Benutzerwünsche angepasst werden können soll. Die Kombination einer klassischen Synopse zusammen mit den vorhandenen Erweiterungsmöglichkeiten mittels Wörterbüchern, Lexika und Multi-Media Elementen die das System bietet, machen es zu einem Werkzeug das auf vielen Gebieten, angefangen von der Text-Analyse und dem Sprachenlernen über die Erstellung textkritischer Editionen bis hin zum elektronischen Publizieren, einsetzbar ist. Neben diesem System sind als weitere Ergebnisse dieser Arbeit verschiedene Werkzeuge für die Softwaredokumentation entstanden und zur Dokumentation des Systems eingesetzt worden. Weiterhin wurde eine neuartige, mehrsprachige, graphische Benutzeroberfläche entwickelt, die unter anderem in dem hier beschriebenen System eingesetz wurde.},
	institution = {Tübingen},
	type = {phdthesis},
	author = {Simonis, Volker},
	editora = {Güntzer, Ulrich and Loos, Rüdiger},
	editoratype = {collaborator},
	date = {2004},
	langid = {english},
	note = {Online-Ressource
Tübingen, Univ., Diss, 2004},
	keywords = {{AnalyzeStatistically}, bigdata{\textasciitilde}, goal\_Dissemination, t\_XML},
}

@book{buttner_handbuch_2011,
	location = {Bad Honnef},
	title = {Handbuch Forschungsdatenmanagement},
	isbn = {9783883472836},
	url = {http://opus4.kobv.de/opus4-fhpotsdam/files/208/HandbuchForschungsdatenmanagement.pdf},
	abstract = {Das Handbuch Forschungsdatenmanagement behandelt disziplinübergreifend zentrale Aspekte des Forschungsdatenmanagements aus informationswissenschaftlicher und  anwendungsbezogener Perspektive.  Zahlreiche ausgewiesene Expertinnen und Experten haben mit Beiträgen zu ihren Spezialgebieten mitgewirkt. Das Handbuch Forschungsdatenmanagement ist konzipiert als Leitfaden für das Selbststudium sowie zur Unterstützung der Aus- und Weiterbildung auf dem aktuellen Stand der Diskussion. Es richtet sich insbesondere an Einsteiger im Forschungsdatenmanagement, aber gleichermaßen auch an wissenschaftliche Datenkuratoren, {IT}-Administratoren und Informationswissenschaftler, die ihre Aufgaben im Forschungsdatenmanagement nicht mehr nur einzelfall- oder disziplinorientiert, sondern in Hinblick auf die Arbeit in und an Forschungsdateninfrastrukturen wahrnehmen wollen.},
	publisher = {Bock   Herchen},
	author = {Büttner, Stephan and Hobohm, Hans-Christoph and Müller, Lars},
	date = {2011},
	langid = {german},
	keywords = {goal\_Storage, meta\_ProjectManagement, obj\_Data},
}

@article{bohannon_google_2011,
	title = {Google Books, Wikipedia, and the Future of Culturomics},
	volume = {331},
	url = {http://www.sciencemag.org/content/331/6014/135.short},
	doi = {10.1126/science.331.6014.135},
	abstract = {As a follow-up to the quantitative analysis of data obtained from Google Books published online in Science on 16 December 2010 and in this week's issue on page 176, one of the study's authors has been using Wikipedia to analyze the fame of scientists whose names appear in books over the centuries. But his effort has been hampered by the online encyclopedia's shortcomings, from the reliability of its information to the organization of its content. Several efforts are under way to improve Wikipedia as a teaching and research tool, including one by the Association for Psychological Science that seeks to create a more complete and accurate representation of its field.},
	pages = {135},
	number = {6014},
	journaltitle = {Science},
	author = {Bohannon, John},
	urldate = {2011-07-19},
	date = {2011-01-14},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview},
}

@report{bruch_open-access-publikationsfonds._2014,
	title = {Open-Access-Publikationsfonds. Eine Handreichung},
	url = {http://doi.org/10.2312/allianzoa.006},
	abstract = {Damit wissenschaftliche Veröffentlichungen frei zugänglich werden, müssen Autoren häufig Publikationsgebühren an die Rechteinhaber zahlen. Für finanzielle Unterstützung sorgen Publikationsfonds. Wie sich solche Fonds planen und aufbauen lassen, erläutert eine neue „Handreichung“ der Allianz der deutschen Wissenschaftsorganisationen.

Zunehmend gehen Wissenschaftler und Wissenschaftlerinnen d ...},
	pages = {43},
	institution = {Arbeitsgruppe Open Access der Allianz der Schwerpunktinitiative  der deutschen Wissenschaftsorganisationen Digitale Information},
	author = {Bruch, Christoph and Fournier, Johannes and Pampel, Heinz},
	urldate = {2014-10-22},
	date = {2014},
	langid = {german},
	keywords = {act\_Publishing, goal\_Dissemination, obj\_ResearchResults},
}

@collection{cohen_hacking_2012,
	location = {Ann Arbor},
	edition = {Digital Edition},
	title = {Hacking the Academy. New Approaches to Scholarship and Teaching from Digital Humanities},
	series = {{DigitalCultureBooks}},
	publisher = {University of Michigan Press},
	editor = {Cohen, Dan and Scheinfeldt, Tom},
	date = {2012},
	langid = {english},
	keywords = {meta\_Assessing, meta\_GiveOverview, meta\_Teaching, obj\_DigitalHumanities},
}

@article{schafer_gutenberg_2007,
	title = {Gutenberg Galaxy Revis(it)ed: A Brief History of Combinatory, Hypertextual and Collaborative Literature from the Baroque Period to the Present},
	abstract = {Literature in computer-based media cannot be contemplated without a long literary tradition.
This article aims at substantiating this assumption with numerous examples of combinatory,
hypertextual and collaborative texts from German literary history since baroque
times. Therewith it provides us with a historical basis in order to work out the common
features and differences that with computers have entered literary texts.},
	pages = {121--160},
	journaltitle = {Gendolla/Schäfer 2007},
	author = {Schäfer, Jörgen},
	date = {2007},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_Literature},
}

@book{moretti_graphs_2005,
	title = {Graphs, maps, trees: abstract models for a literary history},
	isbn = {9781844670260},
	shorttitle = {Graphs, maps, trees},
	abstract = {Professor Franco Moretti argues heretically that literature scholars should stop reading books and start counting, graphing, and mapping them instead. He insists that such a move could bring new luster to a tired field, one that in some respects is among "the most backwards disciplines in the academy." Literary study, he argues, has been random and unsystematic. For any given period scholars focus on a select group of a mere few hundred texts: the canon. As a result, they have allowed a narrow distorting slice of history to pass for the total picture.Moretti offers bar charts, maps, and time lines instead, developing the idea of "distant reading," set forth in his path-breaking essay "Conjectures on World Literature," into a full-blown experiment in literary historiography, where the canon disappears into the larger literary system. Charting entire genres'the epistolary, the gothic, and the historical novel'as well as the literary output of countries such as Japan, Italy, Spain, and Nigeria, he shows how literary history looks significantly different from what is commonly supposed and how the concept of aesthetic form can be radically redefined.},
	pagetotal = {140},
	publisher = {Verso},
	author = {Moretti, Franco},
	date = {2005},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Literature},
}

@article{fagan_government_1976,
	title = {Government freezes administrators' salaries},
	volume = {53},
	issn = {0317-7645},
	pages = {10--11},
	number = {1},
	journaltitle = {Dimensions in health service},
	shortjournal = {Dimens Health Serv},
	author = {Fagan, A},
	date = {1976-01},
	langid = {english},
	pmid = {1309},
}

@article{simon-ritz_google_2011,
	title = {Google Books und die Folgen: Von Nutzen und Nachteil der Digitalisierung},
	url = {http://www.otz.de/startseite/detail/-/specific/Google-Books-und-die-Folgen-Von-Nutzen-und-Nachteil-der-Digitalisierung-864409694#},
	journaltitle = {{OTZ}},
	author = {Simon-Ritz, Frank},
	date = {2011-11-05},
	langid = {german},
	note = {Zur Person: Der promovierte Historiker Frank Simon-Ritz leitet seit 1999 als Direktor die Bibliothek der Bauhaus-Universität zu Weimar. 2003 bis 2009 war er Vorsitzender des Thüringer Bibliotheksverbands, seit 2010 gehört er dem Vorstand des Deutschen Bibliotheksverbands an. Diesen Montag nimmt er an einem Expertengespräch der Enquete-Kommission "Internet und digitale Gesellschaft" des Deutschen Bundestags teil und referiert über Probleme der Digitalisierung.},
	keywords = {goal\_Capture},
}

@article{cohen_giving_2011,
	title = {Giving Literature Virtual Life - Humanities 2.0},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2011/03/22/books/digital-humanities-boots-up-on-some-campuses.html?_r=1&ref=humanities20},
	abstract = {Digital Humanities Boots Up on Some Campuses},
	journaltitle = {The New York Times},
	author = {Cohen, Patricia},
	urldate = {2011-05-20},
	date = {2011-03-21},
	langid = {english},
	keywords = {meta\_GiveOverview, meta\_Teaching, meta\_Teaching/Learning, obj\_DigitalHumanities},
}

@incollection{crane_give_2011,
	location = {Houston, {TX}},
	title = {Give us editors! Re-inventing the edition and re-thinking the humanities},
	url = {http://cnx.org/contents/5df82a16-bb60-4ab2-8277-a61894c801ab@2/Give_us_editors!_Re-inventing_},
	abstract = {This paper offers a response to Roger Bagnall’s contribution on Digital Papyrology, but a proper response to this particular topic requires addressing the broader topic behind this workshop: the reinvention of editing in a digital age. More than a decade ago, at a conference at {MIT}, Jerome {McGann} remarked in passing that we were entering a great age of editing. These words were not among his prepared remarks—when this programmatic remark was called to his attention several years later, he had forgotten the words but warmly endorsed the sentiment. The papers in this workshop suggest the impending truth of that prophetic remark. Scanning books and generating transcriptions is the incunabular phase of digital publication.1 We need to rethink the goals of editing in the light of the possibilities and challenges of emergent digital media.2 We are not entering—we have already entered and will never leave—a new intellectual space, where the speed and the distance between question and answer is qualitatively different from that for which we were trained.

In a digital world where we can publish video and sound and where we can annotate space, we need to extend our vision of editing beyond linguistic sources. In his paper for this collection, Ken Price talks about “topic-based editing,” of which his own Civil War Washington3 provides one example.4HyperCities5 illustrates the opportunities of annotating coordinates in space and time, allowing us to trace such events as the turmoil in Tehran after the 2009 Iranian elections and a tumultuous succession of public buildings over the past century in Berlin. Alison Muri’s Grub Street Project6 sets out to bring an entire moment in history to life. If we are to publish documents—especially documents as enmeshed with their material and cultural context as tweets from Tehran or newspapers from eighteenth century London or nineteenth century Washington, we need to embed them within rich cultural databases and to imagine our textual annotations as links into geographic, visual, quantitative, and textual data.

Within this essay, I restrict myself to the editing of textual sources, but within that field I understand editing in a very broad sense as making our primary textual sources usable for scholarly work. If we take this as an intellectual model, then a wide range of document-centric publications is relevant. These include not only facsimile, diplomatic, and critical editions but also translations, commentaries, and even specialized lexica and indices—documents that are hypertextual in nature, largely composed of individual annotations and expositions upon named portions of a primary source.7 The boundary between editing in this sense and other categories of publication is, in this case (as in almost any classification task), fuzzy. Essays in expository prose that largely follow the structure of a document to elicit an interpretation should probably be considered as well. At least some such studies would be better served if published as hypertextual guides through a document, directing a reader’s focus to one passage after another and using chunks of argumentation to draw out various features of the primary source and comparanda.8 The instinct for such publications is deep, and early forms of such hypertextual publications have appeared in various guises ({PerseusPaths}, Walden’s Paths,9 etc.).

I advance two basic hypotheses:

1) We need editors—lots of them. We have before us a new model of intellectual life in general and especially within the humanities. We have valued scholarship that is difficult to produce and almost as difficult to understand. When a 2009 tenure track job listing asked for candidates who can support contributions and original research by undergraduates as well as {MA} students within the field of Classics, almost none of nearly two hundred applicants had been trained to think about what {MA}-level students, much less undergraduates, could contribute to the field or about what meaningful research they might be able to conduct.10 A few had creative ideas and had even experimented in their teaching but they had done so outside of—and in some measure in spite of—their formal training. Most of those with whom we spoke shifted uncomfortably in their chairs as we pressed them on this point.

We have vast amounts of work before us—far more than a relative handful of salaried academics can accomplish and plenty accessible to our students and to those who love a given subject but maintain a day job doing something else.11 We need to edit the entire record of humanity. Brute digitization provides physical access to digital representations that are qualitatively more useful than anything possible in print—print publication constitutes only a small dimensional reduction of the space in which we now move. At least as important, we have at our disposal a growing set of analytical tools that can make these sources intellectually as well as physically accessible.12 At one end, we can detect not only words and phrases but also ideas in vast collections of data—the bigger the better, in fact.13 The same currents that flatten individual human analysts provide the lift on which many of our algorithms can soar, allowing us to find within vast collections patterns that yield themselves to deep contemplation—and indeed, to the most traditional of intensive reading.14 At the other end, we can now automatically generate background information—a workable commentary—with which to contextualize what we see. And we have begun to attack the greatest of all logistical barriers in intellectual life—the heretofore impenetrable barrier of language. In print culture, we could do nothing with documents in languages that we had not studied. Already today, if we combine machine translation of individual words as well as passages, morphological and syntactic analysis, dictionary lookup, and text mining we can begin to work with sources that were once inaccessible.15

Vast collections and clever services provide a starting point for human analysis. Consider one particular example from my own field. Most Latin literature was produced after, and much builds upon, the tiny surviving corpus of Classical Latin.16 We have an endless supply of intellectually accessible and eminently useful undergraduate and {MA}-level projects, with our students building upon their training in Classics, analyzing the results of automated systems, and producing introductions, commentaries, and annotated translations of individual documents. We can then publish these as components of increasingly sophisticated digital libraries that can parse their structure and mine the machine-actionable information within them: the scholarly labor applied to each edited document becomes training data that then improves the automated results for the rest of the document in question, as well as the corpus of digital Latin.17 If we move towards community-driven models of updating and preserving such editions, preserving the original contributions within a versioning system but allowing the documents to evolve as their authors pursue their careers, these editions can serve as starting points rather than as fixed and obsolescent snapshots.

If we understand editing as the process of enabling others to think about an object from the past, then the editorial process applies as much to spaces (e.g., the development of the Unter den Linden in Berlin), buildings (e.g., the Brandenburg Gate), and objects (e.g., the Quadriga atop the Brandenburg Gate, a chariot drawn by four horses driven by Victoria, the Roman goddess of victory), as to topics (e.g., the development of Prussian nationalism of which Unter den Linden and the Brandenburg Gate are expressions).

Editors can be as much artists as scholars, for the editor who contextualizes an object directs the reader, listener or viewer to a finite set of data that align themselves into meaningful patterns. Editors in this sense differ from authors insofar as they direct their audiences’ gaze away from themselves and towards the object of contemplation. The point is not to vanish—to vanish is to deceive and to imply a transparency that simply does not exist. Rather, editors must provide the clearest possible account of their own biases.18 In this, they resemble their colleagues in the sciences, who explain how the data was collected and how they conducted their experiments so that others can draw their own conclusions about the strengths and limitations of their work.

2) In a digital age, philologists need to treat our editions as components of larger, well-defined corpora rather than as the raw material for printed page layouts.19 Many may take this as obvious but few have pursued the implications of this general idea. The addition of punctuation, the use of upper case to mark proper names, specialized glossaries, the addition of name and place indices, and even translations prefigure major classes of machine-actionable annotation—interpretations of morphological and syntax analyses, lexical entries, word senses, co-reference, named entities are only a subset of the features we may choose to include as new practices of editing emerge. Even when we turn to the most heavily studied classical Greek and Latin texts, a radically new world is taking shape. We have returned to an age of the editio princeps—not literally the first edition, but the first edition in a medium so distinct from that which preceded it that it constitutes a new beginning. We see before us a great age—indeed, a heroic age, one filled with triumphs and false starts, messy, destabilized and destabilizing, and, above all, dynamic.

The remainder of this paper will focus primarily upon the new forms of editing and their consequences. But before exploring those consequences, we will first outline some basic goals based upon the changing possibilities within the digital culture.},
	pages = {81--97},
	booktitle = {Online Humanities Scholarship: The Shape of Things to Come},
	publisher = {Rice University},
	author = {Crane, Gregory and {McGann}, Jerome},
	date = {2011},
	langid = {english},
	keywords = {goal\_Enrichment, meta\_GiveOverview},
}

@collection{burckhardt_geschichte_2005,
	title = {Geschichte und Neue Medien in Forschung, Archiven, Bibliotheken und Museen. Tagungsband .hist 2003},
	volume = {2},
	url = {http://edoc.hu-berlin.de/histfor/7_I/},
	publisher = {Clio-Online},
	editor = {Burckhardt, Daniel and Hohls, Rüdiger and Ziegeldorf, Vera},
	date = {2005},
	langid = {german},
	keywords = {X-{CHECK}, act\_Discovering, meta\_GiveOverview},
}

@article{baasner_geschichte_1998,
	title = {Geschichte der deutschen Literatur des 18. Jahrhunderts als Computeranwendung. Ein Werkstattbericht},
	volume = {22},
	issn = {978-3-89244-305-6},
	shorttitle = {http://www.wallstein-verlag.de/9783892443056-das-achtzehnte-jahrhundert-22-2.html},
	pages = {165--171},
	number = {2},
	journaltitle = {Das achtzehnte Jahrhundert},
	author = {Baasner, Rainer},
	date = {1998},
	langid = {german},
	note = {Baasner, Rainer: Geschichte der deutschen Literatur des 18. Jahrhunderts als Computeranwendung. Ein Werkstattbericht. In: Das achtzehnte Jahrhundert 22/2 (1998), S. 165-171.},
	keywords = {X-{CHECK}},
}

@article{cohen_geographic_2011,
	title = {Geographic Information Systems Help Scholars See History},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2011/07/27/arts/geographic-information-systems-help-scholars-see-history.html?_r=1&hp},
	journaltitle = {The New York Times},
	author = {Cohen, Patricia},
	urldate = {2011-07-26},
	date = {2011-07-26},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Documents},
}

@book{conolly_geographical_2008,
	location = {Cambridge},
	edition = {3rd printing.},
	title = {Geographical Information Systems in archaeology},
	isbn = {9780521793308},
	url = {http://ebooks.cambridge.org/ebook.jsf?bid=CBO9780511807459},
	abstract = {Geographical Information Systems has moved from the domain of the computer specialist into the wider archaeological community, providing it with an exciting new research method. This clearly written but rigorous book provides a comprehensive guide to that use. Topics covered include: the theoretical context and the basics of {GIS}; data acquisition including database design; interpolation of elevation models; exploratory data analysis including spatial queries; statistical spatial analysis; map algebra; spatial operations including the calculation of slope and aspect, filtering and erosion modeling; methods for analysing regions; visibility analysis; network analysis including hydrological modeling; the production of high quality output for paper and electronic publication; and the use and production of metadata. Offering an extensive range of archaeological examples, it is an invaluable source of practical information for all archaeologists, whether engaged in cultural resource management or academic research. This is essential reading for both the novice and the advanced user.},
	publisher = {Cambridge University Press},
	author = {Conolly, James},
	date = {2008},
	langid = {english},
	doi = {http://dx.doi.org/10.1017/CBO9780511807459},
	keywords = {act\_Visualizing, meta\_GiveOverview, obj\_Architecture, obj\_Artefacts},
}

@article{olsen_gender_1991,
	title = {Gender representation and histoire des mentalités : Language and Power in the Trésor de la langue française},
	volume = {6},
	issn = {0982-1783},
	url = {http://www.persee.fr/web/revues/home/prescript/article/hism_0982-1783_1991_num_6_3_1401},
	doi = {10.3406/hism.1991.1401},
	shorttitle = {Gender representation and histoire des mentalités},
	pages = {349--373},
	number = {3},
	journaltitle = {Histoire \& Mesure},
	author = {Olsen, Mark},
	urldate = {2013-10-01},
	date = {1991},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@article{argamon_gender_2009,
	title = {Gender, Race, and Nationality in Black Drama, 1950-2006: Mining Differences in Language Use in Authors and their Characters},
	volume = {3},
	url = {http://www.digitalhumanities.org/dhq/vol/3/2/000043/000043.html},
	shorttitle = {Gender, Race, and Nationality in Black Drama, 1950-2006},
	abstract = {Machine learning and text mining offer new models for text analysis in the humanities by searching for meaningful patterns across many hundreds or thousands of documents. In this study, we apply comparative text mining to a large database of 20th century Black Drama in an effort to examine linguistic distinctiveness of gender, race, and nationality. We first run tests on the plays of American versus non-American playwrights using a variety of learning techniques to classify these works, identifying those which are incorrectly classified and the features which distinguish the plays. We achieve a significant degree of performance in this cross-classification task and find features that may provide interpretative insights. Turning our attention to the question of gendered writing, we classify plays by male and female authors as well as the male and female characters depicted in these works. We again achieve significant results which provide a variety of feature lists clearly distinguishing the lexical choices made by male and female playwrights. While classification tasks such as these are successful and may be illuminating, they also raise several critical issues. The most successful classifications for author and character genders were accomplished by normalizing the data in various ways. Doing so creates a kind of distance from the text as originally composed, which may limit the interpretive utility of classification tools. By framing the classification tasks as binary oppositions (male/female, etc), the possibility arises of stereotypical or "lowest common denominator" results which may gloss over important critical elements, and may also reflect the experimental design. Text mining opens new avenues of textual and literary research by looking for patterns in large collections of documents, but should be employed with close attention to its methodological and critical limitations.},
	number = {2},
	author = {Argamon, Shlomo and Cooney, Charles and Horton, Russell and Olsen, Mark and Stein, Sterling and Voyer, Robert},
	urldate = {2012-04-25},
	date = {2009},
	langid = {english},
	keywords = {bigdata, t\_Stylometry},
}
@article{bamman_gender_2012,
	title = {Gender identity and lexical variation in social media},
	url = {http://arxiv.org/abs/1210.4567},
	doi = {10.1111/josl.12080},
	abstract = {We present a study of the relationship between gender, linguistic style, and social networks, using a novel corpus of 14,000 Twitter users. Prior quantitative work on gender often treats this social variable as a female/male binary; we argue for a more nuanced approach. By clustering Twitter users, we find a natural decomposition of the dataset into various styles and topical interests. Many clusters have strong gender orientations, but their use of linguistic resources sometimes directly conflicts with the population-level language statistics. We view these clusters as a more accurate reflection of the multifaceted nature of gendered language styles. Previous corpus-based work has also had little to say about individuals whose linguistic styles defy population-level gender patterns. To identify such individuals, we train a statistical classifier, and measure the classifier confidence for each individual in the dataset. Examining individuals whose language does not match the classifier's model for their gender, we find that they have social networks that include significantly fewer same-gender social connections and that, in general, social network homophily is correlated with the use of same-gender language markers. Pairing computational methods and social theory thus offers a new perspective on how gender emerges as individuals position themselves relative to audiences, topics, and mainstream gender norms.},
	journaltitle = {Journal of Sociolinguistic},
	author = {Bamman, David and Eisenstein, Jacob and Schnoebelen, Tyler},
	date = {2012},
	langid = {german},
	keywords = {bigdata},
}

@article{schanze_geisteswissenschaften_1994,
	title = {Geisteswissenschaften und Neue Medien. Ziele, Funktionen, Tendenzen},
	pages = {25--32},
	journaltitle = {Barckow/Delabar 1994},
	author = {Schanze, Helmut},
	date = {1994},
	langid = {german},
	note = {Schanze, Helmut: Geisteswissenschaften und Neue Medien. Ziele, Funktionen, Tendenzen. In: Barckow/Delabar 1994, S. 25-32.},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, obj\_NewMedia},
}

@incollection{craiger_future_2007,
	location = {New York, {NY}},
	title = {Future Trends in Authorship Attribution},
	volume = {242},
	isbn = {978-0-387-73741-6},
	url = {http://www.springerlink.com/content/p4382170ju6v1517/},
	abstract = {Authorship attribution, the science of inferring characteristics of an author from the characteristics of documents written by that author, is a problem with a long history and a wide range of application. This paper surveys the history and present state of the discipline — essentially a collection of ad hoc methods with little formal data available to select among them. It also makes some predictions about the needs of the discipline and discusses how these needs might be met.},
	pages = {119--132},
	booktitle = {Advances in Digital Forensics {III}},
	publisher = {Springer New York},
	author = {Juola, Patrick},
	editor = {Craiger, Philip and Shenoi, Sujeet},
	urldate = {2011-12-08},
	date = {2007},
	langid = {english},
	keywords = {{AnalyzeStatistically}, X-{CHECK}, meta\_GiveOverview, t\_Stylometry},
}

@article{garcia_function_2007,
	title = {Function Words in Authorship Attribution Studies},
	volume = {22},
	url = {http://llc.oxfordjournals.org/content/22/1/49.abstract},
	doi = {10.1093/llc/fql048},
	abstract = {The search for a reliable expression to measure an author's lexical richness has constituted many statisticians' holy grail over the last decades in their attempt to solve some controversial authorship attributions. The greatest effort has been devoted to find a formula grounded on the computation of tokens, word-types, most-frequent-word(s), hapax legomena, hapax dislegomena, etc., such that it would characterize a text successfully, independent of its length. In this line, Yule's K and Zipf 's Z seem to be generally accepted by scholars as reliable measures of lexical repetition and lexical richness by computing content and function words altogether.1 Given the latter's higher frequency, they prove to be more reliable identifiers when isolatedly computed in p.c.a. and Delta-based attribution studies, and their rate to the former also measures the functional density of a text. In this paper, we aim to show that each constant serves to measure a specific feature and, as such, they are thought to complement one another since a supposedly rich text (in terms of its lemmas) does necessarily have to characterize by its low functional density, and vice versa. For this purpose, an annotated corpus of the West Saxon Gospels ({WSG}) and Apollonius of Tyre ({AoT}) has been used along with a huge raw corpus.},
	pages = {49--66},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {{LLC}},
	author = {Garcia, A M and Martin, J C},
	date = {2007},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, t\_Stylometry},
}

@article{milojkovic_is_2013,
	title = {Is corpus stylistics bent on self-improvement? The role of reference corpora 20 years after the advent of semantic prosody},
	volume = {42},
	issn = {1613-3838, 0341-7638},
	url = {http://www.degruyter.com/view/j/jlse.2013.42.issue-1/jls-2013-0002/jls-2013-0002.xml},
	doi = {10.1515/jls-2013-0002},
	shorttitle = {Is corpus stylistics bent on self-improvement?},
	abstract = {This paper (based on a presentation delivered at the 2012 {PALA} conference in Malta) aims to explain and illustrate the main principles of Contextual Prosodic Theory ({CPT}), developed by Bill Louw, the originator of the notion of semantic prosody ({McEnery} and Hardy, 2012: 135). No less importantly, it aims to free the theory from unwarranted criticism it has attracted over the past few years. I deal with the theoretical objections to Louw's stylistics in {McEnery} and Hardy (2012) and in Hunston (2007). In doing so, certain basic assumptions of Louw's stylistics are restated, such as the roles of intuition, authorial intention and the individual reader's perception. Is Louw justified in assuming that a text may be interpreted by what is, prima facie, {NOT} in it? After showing how reference corpora can reasonably be taken to influence the act of reading, I give an illustration of a new development in {CPT}: logical semantic prosody – subtext (Louw 2010a, 2010b). The reference corpora I use include the {BNC} and {COCA}, available on Mark Davies' site, and Tim Johns' corpus of the 1995 edition of the Times newspaper, containing 44.5 million words.},
	pages = {59--78},
	number = {1},
	journaltitle = {Journal of Literary Semantics},
	author = {Milojkovic, Marija},
	urldate = {2013-07-03},
	date = {2013-01-14},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@incollection{louw_irony_1993,
	location = {Amsterdam},
	title = {Irony in the text or insincerity in the writer? The diagnostic potential of semantic prosodies},
	url = {http://www.academia.edu/844204/Irony_in_the_text_or_insincerity_in_the_writer_The_diagnostic_potential_of_semantic_prosodies},
	abstract = {"Text and Technology" focuses on three major areas of modern linguistics: discourse analysis, corpus-driven analysis of language, and computational linguistics. The volume starts off with a description of the various British traditions in text analysis by Michael Stubbs. The first section Spoken and Written Discourse contains contributions by Martin Warren, Mohd Dahan Hazadiah., Amy B.M. Tsui, Anna Mauranen and Susan Hunston. The next section on corpus-driven analysis Corpus Studies: Theory and Practice contains contributions by Gill Francis, Bill Louw, Allan Partington, Elena Tognini-Bonelli. The contributions in this section by Kirsten Malmkjaer and Mona Baker deal specifically with translated text. The final third section Text and Technology: Computational Tools has contributions by David Coniam, Jeremy Clear, Junsaku Nakamura, Geoff Barnbrook and Margaret Allen. In spite of the specialised nature of the topics discussed and the level of sophistication with which these topcis are handled, the papers are written in a clear and accessible style and will therefore be of interest to seasoned scholars and students alike. An extensive index further enhances the value of this collection as a reference point for many of the issues that currently lie at the heart of modern linguistics enquiry},
	pages = {157--176},
	booktitle = {Text and technology},
	publisher = {John Benjamins},
	author = {Louw, W.},
	editor = {Baker, M. and Francis, G. and Tognini-Bonelli, E.},
	date = {1993},
	langid = {english},
	note = {[Reprinted in Sampson, G. and {McCarthy}, D. (eds.) (2004). Corpus linguistics: readings in a widening discipline. London: Continuum. 229-241.]},
	keywords = {{AnalyzeQualitatively}, act\_StylisticAnalysis},
}

@article{schanze_introduction_1989,
	title = {Introduction to the special section on full text retrieval systems},
	volume = {4},
	url = {http://llc.oxfordjournals.org/content/4/2/106.full.pdf},
	doi = {10.1093/llc/4.2.106},
	pages = {106--145},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {{LLC}},
	author = {Schanze, Helmut},
	date = {1989},
	langid = {english},
	note = {Schanze, Helmut: Full Text Retrieval Systems. In: Literary and Linguistic Computing 4/2 (1989), S. 106-145.},
	keywords = {{AnalyzeStatistically}, act\_Query/Retrieve, meta\_GiveOverview},
}

@article{upchurch_full-text_2012,
	title = {Full-Text Databases and Historical Research: Cautionary Results from a Ten-Year Study},
	issn = {0022-4529, 1527-1897},
	url = {http://jsh.oxfordjournals.org/cgi/doi/10.1093/jsh/shs035},
	doi = {10.1093/jsh/shs035},
	shorttitle = {Full-Text Databases and Historical Research},
	abstract = {Full-text electronic searching of books, periodicals, and government documents is providing historians with the ability to survey unprecedented amounts of historical material for references traditionally not associated with those sources. Yet because the research conducted with these new tools is often not possible or practical using other methods, the shortcomings of the results are not always evident. This article highlights some of those shortcomings through a detailed discussion of a ten-year study that spanned the development of some of the main databases now used in nineteenth-century British history. The study was designed to uncover the number of newspaper reports discussing sex between men in three major London newspapers between 1820 and 1870, ultimately detecting over one thousand such reports. The study was designed in 1999 using the Palmer's Index to the Times on {CD} {ROM} in conjunction with archival sources, and those results were contrasted in later years to the results from the Times Digital Archive, the Palmer's Index to the Times within C19: The Nineteenth Century Index, and the British Library Newspapers 1800-1900 databases. The inability of newer tools to detect large amounts known material sparked a systematic crosschecking of the results of these databases to map the extent of the discrepancies. The article discusses some major shortcomings of these new tools, demonstrates methodologies for working around the limitations, and highlights some of the new findings related to the public discussion of sex between men obtained through this study.},
	journaltitle = {Journal of Social History},
	author = {Upchurch, C.},
	urldate = {2012-07-05},
	date = {2012-06-29},
	langid = {english},
	keywords = {bigdata},
}

@book{committee_on_the_analysis_of_massive_data;_committee_on_applied_and_theoretical_statistics;_board_on_mathematical_sciences_and_their_applications;_division_on_engineering_and_physical_sciences;_national_research_council_frontiers_2013,
	location = {Washington},
	title = {Frontiers in Massive Data Analysis},
	url = {http://www.nap.edu/catalog.php?record_id=18374},
	abstract = {Data mining of massive data sets is transforming the way we think about crisis response, marketing, entertainment, cybersecurity and national intelligence. Collections of documents, images, videos, and networks are being thought of not merely as bit strings to be stored, indexed, and retrieved, but as potential sources of discovery and knowledge, requiring sophisticated analysis techniques that go far beyond classical indexing and keyword counting, aiming to find relational and semantic interpretations of the phenomena underlying the data.

Frontiers in Massive Data Analysis examines the frontier of analyzing massive amounts of data, whether in a static database or streaming through a system. Data at that scale--terabytes and petabytes--is increasingly common in science (e.g., particle physics, remote sensing, genomics), Internet commerce, business analytics, national security, communications, and elsewhere. The tools that work to infer knowledge from data at smaller scales do not necessarily work, or work well, at such massive scale. New tools, skills, and approaches are necessary, and this report identifies many of them, plus promising research directions to explore. Frontiers in Massive Data Analysis discusses pitfalls in trying to infer knowledge from massive data, and it characterizes seven major classes of computation that are common in the analysis of massive data. Overall, this report illustrates the cross-disciplinary knowledge--from computer science, statistics, machine learning, and application disciplines--that must be brought to bear to make useful inferences from massive data},
	publisher = {The National Academies Press},
	author = {{Committee on the Analysis of Massive Data; Committee on Applied and Theoretical Statistics; Board on Mathematical Sciences and Their Applications; Division on Engineering and Physical Sciences; National Research Council}},
	urldate = {2013-09-12},
	date = {2013},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@article{hughes_cover:_2012,
	title = {From the Cover: Quantitative patterns of stylistic influence in the evolution of literature},
	volume = {109},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1115407109},
	doi = {10.1073/pnas.1115407109},
	shorttitle = {From the Cover},
	abstract = {Literature is a form of expression whose temporal structure, both in content and style, provides a historical record of the evolution of culture. In this work we take on a quantitative analysis of literary style and conduct the first large-scale temporal stylometric study of literature by using the vast holdings in the Project Gutenberg Digital Library corpus. We find temporal stylistic localization among authors through the analysis of the similarity structure in feature vectors derived from content-free word usage, nonhomogeneous decay rates of stylistic influence, and an accelerating rate of decay of influence among modern authors. Within a given time period we also find evidence for stylistic coherence with a given literary topic, such that writers in different fields adopt different literary styles. This study gives quantitative support to the notion of a literary “style of a time” with a strong trend toward increasingly contemporaneous stylistic influence.},
	pages = {7682--7686},
	number = {20},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Hughes, J. M. and Foti, N. J. and Krakauer, D. C. and Rockmore, D. N.},
	urldate = {2012-07-11},
	date = {2012-04-30},
	langid = {english},
	keywords = {X-{CHECK}},
}

@book{lessig_free_2004,
	location = {New York},
	title = {Free Culture},
	url = {http://www.free-culture.cc/},
	publisher = {Penuin},
	author = {Lessig, Lawrence},
	date = {2004},
	langid = {english},
}

@article{rieger_framing_2010,
	title = {Framing Digital Humanities: The Role of New Media in Humanities Scholarship},
	volume = {15},
	url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3198/2628},
	abstract = {The phrase “digital humanities” refers to a range of new media applications that converge at the intersection of technology and humanities scholarship. It is an evolving notion and conveys the role of information technologies in humanities scholarship. Based on a qualitative case study approach, this paper interprets the concept by eliciting the diverse perspectives — which nevertheless express several discernible themes — of a group of humanities scholars. It synthesizes the wide range of opinions and assumptions about information and communication technologies ({ICTs}) held by these humanists by using Bijker’s (1995) notion of a technological frame. The digital humanities domain is interpreted through three lenses: digital media as facilitator of scholarly communication; digital media as a platform for creative expression and artistic endeavors; and, digital media as context for critical studies of digital culture. The article concludes that, while technologies are being positioned as driving forces behind academic innovation, it is more important than ever to understand the cultural, social, and political implications of new media and how they are perceived and used by humanities scholars.},
	number = {10},
	journaltitle = {First Monday},
	author = {Rieger, Oya Y.},
	urldate = {2011-06-16},
	date = {2010-10-04},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{henrich_forschungsinfrastrukturen_2011,
	title = {Forschungsinfrastrukturen - Auf dem (langen) Weg zu den e-Humanities},
	volume = {uni.vers 11 Medieninformatik Auf dem (langen) Weg zu den e-Humanities Es ist eine verlockende Vision: Wenn die in zahlreichen kultur- und geisteswissenschaft- lichen Einzelprojekten gesammelten For- schungsdaten miteinander verknüpft wer- den, könnte dies fachübergreifende Analysen erheblich vereinfachen. Wie sinnvoll der wissenschaftliche Austausch zwischen ganz unterschiedlichen Disziplinen in der Praxis sein kann, zeigt ein Beispiel: Um universelle Trends in der Literatur und Architektur des Mittelalters ableiten zu können, müssen Spezialisten aus mehreren Bereichen zusam- menarbeiten – nämlich Bauhistoriker und Philologen. Aber auch innerhalb der Einzel- disziplinen ist ein umfassender Erkenntnis- austausch durchaus vielversprechend. So lassen sich beispielsweise aus mehreren, unabhängig voneinander entstandenen Ar- beiten über die Portraitmalerei verschiedener Epochen ggf. neue Erkenntnisse über das Bildverständnis im Zeitverlauf gewinnen. Die Forschungsinitiative {DARIAH}-{DE} (Digital Re- search Infrastructure for the Arts and Huma- nities) leistet einen wichtigen Beitrag dazu, diese Vision schon bald Realität werden zu lassen. Im Rahmen des umfangreichen Pro- jekts befassen sich Bamberger Informatiker mit der Datenintegration und der Etablierung von übergreifenden Suchdiensten},
	url = {http://www.uni-bamberg.de/fileadmin/uni/verwaltung/presse/Publikationen/uni.vers/univers_forschung_2011/04_Forschungsinfrastrukur.pdf},
	pages = {11--15},
	issue = {Mai 2011},
	journaltitle = {uni.vers, Magazin der Otto-Friedrich-Universität Bamberg},
	author = {Henrich, Andreas},
	date = {2011},
	langid = {german},
	keywords = {act\_Communicating, obj\_Infrastructures},
}

@article{makar_formate_1975,
	title = {Formate assay in body fluids: application in methanol poisoning},
	volume = {13},
	issn = {0006-2944},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/1},
	shorttitle = {Formate assay in body fluids},
	pages = {117--126},
	number = {2},
	journaltitle = {Biochemical medicine},
	shortjournal = {Biochem Med},
	author = {Makar, A B and {McMartin}, K E and Palese, M and Tephly, T R},
	date = {1975-06},
	langid = {english},
	pmid = {1},
}

@article{holmes_forensic_1995,
	title = {Forensic Stylometry: A Review of the Cusum Controversy of Liege},
	url = {http://promethee.philo.ulg.ac.be/RISSHpdf/Annee1995/Articles/DHolmesetc.pdf},
	abstract = {Les
graphes
de
sommes
cumulatives
des
différentes
classes
de
mots
ont
récemment
été
utilisés
pour
mettre
en
doute,
lors
de
procès,
les
aveux
de
l'accusé.
Cet
article
se
propose
d'examiner
d'une
part,
la
validité
de
cette
technique,
et
d'autre
part,
l'hypothèse
sur
laquelle
cette
technique
est
développée.
Cet
article
examinera
les
travaux
de
recherche
relatifs
à
l'ap'
plication
des
graphes
de
sommes
cumulatives.
L'efficacité
d'une
autre
procédure
reposant
sur
la
pondération
des
sommes
cumulatives
est
également
explorée.
Notre
conclusion
remet
en
cause
la
crédibilité
des
indicateurs
de
l'identité
de
l'auteur.},
	journaltitle = {Revue Informatique et Statistique dans les Sciences Humaines},
	author = {Holmes, David I. and Tweedie, F. J.},
	date = {1995},
	langid = {english},
	note = {Belgium: Univ. of Liège},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@article{jacomy_forceatlas2_2014,
	title = {{ForceAtlas}2, a Continuous Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi Software},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pone.0098679},
	doi = {10.1371/journal.pone.0098679},
	abstract = {Gephi is a network visualization software used in various disciplines (social network analysis, biology, genomics…). One of its key features is the ability to display the spatialization process, aiming at transforming the network into a map, and {ForceAtlas}2 is its default layout algorithm. The latter is developed by the Gephi team as an all-around solution to Gephi users’ typical networks (scale-free, 10 to 10,000 nodes). We present here for the first time its functioning and settings. {ForceAtlas}2 is a force-directed layout close to other algorithms used for network spatialization. We do not claim a theoretical advance but an attempt to integrate different techniques such as the Barnes Hut simulation, degree-dependent repulsive force, and local and global adaptive temperatures. It is designed for the Gephi user experience (it is a continuous algorithm), and we explain which constraints it implies. The algorithm benefits from much feedback and is developed in order to provide many possibilities through its settings. We lay out its complete functioning for the users who need a precise understanding of its behaviour, from the formulas to graphic illustration of the result. We propose a benchmark for our compromise between performance and quality. We also explain why we integrated its various features and discuss our design choices.},
	pages = {98679},
	number = {6},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Jacomy, Mathieu and Venturini, Tommaso and Heymann, Sebastien and Bastian, Mathieu},
	urldate = {2014-06-18},
	date = {2014-06-10},
	langid = {english},
	keywords = {act\_Visualizing, goal\_Interpretation, obj\_Data},
}

@thesis{bethard_finding_2007,
	location = {United States -- Colorado},
	title = {Finding event, temporal and causal structure in text: A machine learning approach},
	rights = {Copyright {ProQuest}, {UMI} Dissertations Publishing 2007},
	url = {http://search.proquest.com/docview/304860129/citation?accountid=15156},
	shorttitle = {Finding event, temporal and causal structure in text},
	pagetotal = {132},
	institution = {University of Colorado at Boulder},
	type = {phdthesis},
	author = {Bethard, Steven John},
	urldate = {2013-03-19},
	date = {2007},
	langid = {english},
	keywords = {t\_MachineLearning},
}

@article{forsyth_feature-finding_1996,
	title = {Feature-finding for text classification},
	url = {http://richardsandesforsyth.net/pubs/fftc.pdf},
	abstract = {Stylometrists have proposed and used a wide variety of textual features or markers, but
until recen
tly very little attention has been focused on the question: where do
textual features come from? In many text
-
categorization tasks the choice of textual
features is a crucial determinant of success, yet is typically left to the intuition of
the analyst. We
argue that it would be desirable, at least in some cases, if this part
of the process were less dependent on subjective judgement. Accordingly, this
paper compares five different methods of textual feature finding that do not need
background knowledge ext
ernal to the texts being analyzed (three proposed by
previous stylometers, two devised for this study). As these methods do not rely on
parsing or semantic analysis, they are not tied to the English language only.
Results of a benchmark test on 10 represen
tative text
-
classification problems
suggest that the technique here designated
Monte
-
Carlo Feature
-
Finding
has
certain advantages that deserve consideration by future workers in this area.},
	pages = {163--174},
	number = {11},
	journaltitle = {Literary and Linguistic Computing},
	author = {Forsyth, R. S. and Holmes, David I.},
	date = {1996},
	langid = {english},
	keywords = {t\_Stylometry},
}

@book{dusa_facing_2014,
	location = {Berlin},
	title = {Facing the future: European research infrastructures for the humanities and social sciences},
	isbn = {9783944417035 3944417038},
	url = {http://www.akademienunion.de/BMBF_Projekt/FACING_THE_FUTURE.pdf},
	shorttitle = {Facing the future},
	abstract = {This book documents the results of the conference
Facing the Future: European
Research Infrastructure for Humanities and Social Sciences
(November 21/22
2013, Berlin), initiated by the Social and Cultural Innovation Strategy Working Group of {ESFRI} ({SCI}-{SWG}) and the German Federal Ministry of Education
and Research ({BMBF}), and hosted by the European Federation of Academies of
Sciences and Humanities ({ALLEA}) and the German Data Forum ({RatSWD})},
	pagetotal = {210},
	publisher = {Scivero},
	author = {Duşa, Adrian and Nelle, Dietrich and Stock, Günter and Wagner, Gert G},
	date = {2014},
	langid = {english},
}

@incollection{mounier_faire_2012,
	title = {Faire des humanités numériques},
	rights = {© {OpenEdition} Press, 2012 Conditions d'utilisation : http://www.openedition.org/6540},
	url = {http://books.openedition.org/oep/226},
	abstract = {Qu’est-ce que les humanités numériques ? Apparue en 2006, l’expression connaît depuis un véritable succès. Mais au-delà du slogan à la mode, quelle est la réalité des pratiques qu’il désigne ? Si tout le monde s’accorde sur une définition minimale à l’intersection des technologies numériques et des sciences humaines et sociales, les vues divergent lorsqu’on entre dans le vif du sujet. Les humanités numériques représentent-elles une véritable révolution des pratiques de recherche et des paradigmes intellectuels qui les fondent ou, plus simplement, une optimisation des méthodes existantes ? Constituent-elles un champ suffisamment structuré pour justifier une réforme des modes de financement de la recherche, des cursus de formation, des critères d’évaluation ? L’archive numérique offre-t-elle à la recherche suffisamment de garanties ? Quelle place la recherche « dirigée par les données » laisse-t-elle à l’interprétation ? Telles sont quelques-unes des questions abordées par ce deuxième opus de la collection « Read/Write Book ». Ces dix-huit textes essentiels, rédigés ou traduits en français par des chercheurs de différentes nationalités, proposent une introduction aux humanités numériques accessible à tous ceux qui souhaitent en savoir plus sur ce domaine de recherche en constante évolution.},
	booktitle = {Read/Write Book 2 : Une introduction aux humanités numériques},
	publisher = {{OpenEdition} Press},
	author = {Berra, Aurélien},
	editor = {Mounier, Pierre},
	urldate = {2013-10-19},
	date = {2012},
	langid = {french},
}

@article{correll_exploring_2011,
	title = {Exploring Collections of Tagged Text for Literary Scholarship},
	volume = {30},
	issn = {01677055},
	url = {http://doi.wiley.com/10.1111/j.1467-8659.2011.01922.x},
	doi = {10.1111/j.1467-8659.2011.01922.x},
	abstract = {Modern literary scholars must combine access to vast collections of text with the traditional close analysis of their field. In this paper, we discuss the design and development of tools to support this work. Based on analysis of the needs of literary scholars, we constructed a suite of visualization tools for the analysis of large collections of tagged text (i.e. text where one or more words have been annotated as belonging to a specific category). These tools unite the aspects of the scholars’ work: large scale overview tools help to identify corpus-wide statistical patterns while fine scale analysis tools assist in finding specific details that support these observations. We designed visual tools that support and integrate these levels of analysis. The result is the first tool suite that can support the multilevel text analysis performed by scholars, combining standard visual elements with novel methods for selecting individual texts and identifying represenative passages in them.},
	pages = {731--740},
	number = {3},
	journaltitle = {Computer Graphics Forum},
	author = {Correll, M. and Witmore, M. and Gleicher, M.},
	urldate = {2011-07-22},
	date = {2011-06},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, act\_Discovering, obj\_Text, obj\_Tools},
}

@book{tukey_exploratory_1977,
	location = {Reading, Mass.},
	title = {Exploratory data analysis},
	isbn = {0201076160 9780201076165},
	publisher = {Addison-Wesley Pub. Co.},
	author = {Tukey, John W},
	date = {1977},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview},
}

@article{lavrentiev_exploration_2013,
	title = {Exploration textométrique du corpus des dossiers de Bouvard et Pécuchet},
	url = {http://flaubert.univ-rouen.fr/revue/article.php?id=113},
	abstract = {This paper presents an experience of creating and analyzing a corpus of Bouvard et Pécuchet files in the methodological and technological framework of textometry, which is completely different from that of the project where these files were produced. It shows the advantages of using an open and interdisciplinary encoding system that is provided by the {XML} standard and the guidelines of the Text Encoding Initiative Consortium ({TEI}), but also points out the limits due to the extreme variability of {TEI} encoding practices and to the difficulty of reconciling a very precise documentary representation of the primary sources encoding practice with identifying the semantic structures relevant for textometric analysis.},
	number = {13},
	journaltitle = {Revue Flaubert},
	author = {Lavrentiev, Alexei and Heiden, Serge},
	editor = {Dord-Crouslé, Stéphanie},
	date = {2013},
	langid = {french},
	note = {Actes du colloque de Lyon, 7-9 mars 2012: « Les dossiers documentaires de Bouvard et Pécuchet » : l’édition numérique du creuset flaubertien.},
	keywords = {act\_StructuralAnalysis, goal\_Analysis, obj\_Text},
}

@article{casella_explaining_1992,
	title = {Explaining the Gibbs Sampler},
	volume = {46},
	issn = {00031305},
	url = {https://ariddell.org/simple-topic-model.html},
	doi = {10.2307/2685208},
	abstract = {This is an extended version of the appendix to my paper exploring trends in German Studies in the {US} between 1928 and 2006. In that paper I used a topic model (Latent Dirichlet Allocation). This tutorial is intended to help readers in the humanities and social sciences start to understand how {LDA} works. It is, however, an extremely poor substitute for an introductory class in Bayesian statistics.},
	pages = {167},
	number = {3},
	journaltitle = {The American Statistician},
	author = {Casella, George and George, Edward I.},
	urldate = {2014-03-29},
	date = {1992-08},
	langid = {english},
	keywords = {bigdata},
}

@article{labbe_experiments_2007,
	title = {Experiments on authorship attribution by intertextual distance in english*},
	volume = {14},
	issn = {0929-6174},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09296170600850601},
	doi = {10.1080/09296170600850601},
	abstract = {How can it be said that texts are ?near to? or ?distant from? one another? Are different texts by a single author more similar than texts by different authors? To answer these questions, a method is proposed by calculating intertextual distance. A blind test and some additional experiments show that this calculation offers an interesting tool for non-traditional authorship attribution.},
	pages = {33--80},
	number = {1},
	journaltitle = {Journal of Quantitative Linguistics},
	author = {Labbé, Dominique},
	date = {2007},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}
@article{cameron_evolution_2011,
	title = {Evolution, science and the study of literature: A critical response},
	volume = {20},
	issn = {0963-9470, 1461-7293},
	url = {http://lal.sagepub.com/cgi/doi/10.1177/0963947010391126},
	doi = {10.1177/0963947010391126},
	shorttitle = {Evolution, science and the study of literature},
	abstract = {This article offers a critical response to the proposal made by Jonathan Gottschall (2008), and discussed sympathetically in a review article for this journal (Francis 2010), for a more ‘scientific’ approach to the study of literature. A critical examination of evolutionary psychology, the particular scientific approach which underpins Gottschall’s own work on folk tales, is followed by a broader consideration of what ‘scientific’ might mean in relation to literature, asking how far it is either possible or desirable to apply the methods and evaluative metrics of science in other areas of scholarly endeavour. It is argued that there are good reasons for linguists and literary scholars to maintain the theoretical and methodological pluralism that has characterized their fields in the past},
	pages = {59--72},
	number = {1},
	journaltitle = {Language and Literature},
	author = {Cameron, Deborah},
	urldate = {2013-07-03},
	date = {2011-02-18},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@book{neuroth_evolution_2013,
	location = {Glückstadt},
	title = {Evolution der Informationsinfrastruktur Forschung und Entwicklung als Kooperation von Bibliothek und Fachwissenschaft},
	isbn = {9783864880438 3864880432},
	url = {http://webdoc.sub.gwdg.de/univerlag/2013/Neuroth_Festschrift.pdf},
	publisher = {Hülsbusch, W},
	author = {Neuroth, Lossau, Norbert, Rapp, Andrea, Heike},
	date = {2013},
	langid = {german},
	keywords = {goal\_Collaboration, obj\_VREs},
}

@report{schlunzen_evaluation_2011,
	title = {Evaluation und Dokumentation existierender Architekturkonzepte},
	url = {http://www.wissgrid.de/publikationen/deliverables/wp2/Gridarchitekturen-im-Ueberblick-V2.0.pdf},
	abstract = {Die Dokumentation und Evaluiering existierender Architektur-Konzepte im D-Grid dient als Grund-
lage fur die Erstellung von Blaupausen architektonischer Konzep
te fur Grid-Infrastrukturen. Daraus
ensteht eine Sammlung generischer Modelle, die neuen Communiti
es durch das Expertenteam zur
Verfugung gestellt werden konnen.},
	institution = {Wissgrid},
	author = {Schlünzen, E. F and Agapov, I.},
	date = {2011},
	langid = {german},
	keywords = {meta\_Assessing, obj\_VREs},
}

@inproceedings{kestemont_evaluating_2012,
	location = {Hamburg},
	title = {Evaluating Unmasking for Cross-Genre Authorship Verification},
	url = {http://www.dh2012.uni-hamburg.de/conference/programme/abstracts/evaluating-unmasking-for-cross-genre-authorship-verification/},
	abstract = {In this paper we will stress-test a recently proposed technique for computational authorship verification, ‘unmasking’ (Koppel et al. 2004, 2007), which has been well received in the literature (Stein et al. 2010). The technique envisages an experimental set-up commonly referred to as ‘authorship verification’, a task generally deemed more difficult than so-called ‘authorship attribution’ (Koppel et al. 2007). We will apply the technique to authorship verification across genres, an extremely complex text categorization problem that so far has remained unexplored (Stamatatos 2009). We focus on five representative contemporary English-language authors. For each of them, the corpus under scrutiny contains several texts in two genres (literary prose and theatre plays).},
	eventtitle = {{DH}2012},
	booktitle = {{DH}2012},
	author = {Kestemont, Mike},
	date = {2012-07},
	langid = {english},
}

@inproceedings{warwick_evaluating_2007,
	location = {Vienna},
	title = {Evaluating Digital Humanities Resources: The {LAIRAH} Project Checklist and the Internet Shakespeare Editions Project},
	url = {http://eprints.ucl.ac.uk/4806/},
	pages = {297--306},
	booktitle = {{ELPUB}2007. Openness in Digital Publishing: Awareness, Discovery and Access - Proceedings of the 11th International Conference on Electronic Publishing},
	publisher = {{OEKK} Editions},
	author = {Warwick, Claire and Terras, Melissa and Galina, I. and Huntington, P. and Pappa, N. Eva and Martens, B.},
	editor = {Chan, L.},
	date = {2007},
	langid = {english},
}

@article{bergstrom_evaluating_2014,
	title = {Evaluating big deal journal bundles},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/early/2014/06/11/1403006111},
	doi = {10.1073/pnas.1403006111},
	abstract = {Large commercial publishers sell bundled online subscriptions to their entire list of academic journals at prices significantly lower than the sum of their á la carte prices. Bundle prices differ drastically between institutions, but they are not publicly posted. The data that we have collected enable us to compare the bundle prices charged by commercial publishers with those of nonprofit societies and to examine the types of price discrimination practiced by commercial and nonprofit journal publishers. This information is of interest to economists who study monopolist pricing, librarians interested in making efficient use of library budgets, and scholars who are interested in the availability of the work that they publish.},
	pages = {201403006},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Bergstrom, Theodore C. and Courant, Paul N. and {McAfee}, R. Preston and Williams, Michael A.},
	urldate = {2014-06-17},
	date = {2014-06-16},
	langid = {english},
	keywords = {act\_Publishing, goal\_Dissemination, obj\_ResearchResults},
}

@book{hughes_evaluating_2011,
	location = {[S.l.]},
	title = {Evaluating and measuring the value, use and impactof digital collections},
	isbn = {9781856047203},
	url = {http://www.facetpublishing.co.uk/title.php?id=7203},
	abstract = {This book brings together a group of international experts to consider the following key issues:

    What is the role of digital resources in the research life cycle?
    Do the arts and humanities face a ‘data deluge’?
    How are digital collections to be sustained over the long term?
    How is use and impact to be assessed?
    What is the role of digital collections in the ‘digital economy’?
    How is public engagement with digital cultural heritage materials to be assessed and supported?},
	publisher = {Facet Publishing},
	author = {Hughes, Lorna},
	date = {2011},
	langid = {english},
	keywords = {bigdata{\textasciitilde}},
}

@incollection{agosti_escidoc_2009,
	title = {{eSciDoc} Infrastructure: A Fedora-Based e-Research Framework},
	isbn = {9783642043451},
	url = {https://smartech.gatech.edu/handle/1853/28474?show=full},
	abstract = {{eSciDoc} is the open-source e-Research environment jointly created by the German Max Planck Society and {FIZ} Karlsruhe. It consists of a generic set of basic services ("{eSciDoc} Infrastructure") and various applications built on top of this infrastructure ("{eSciDoc} Solutions"). This presentation will focus on the {eSciDoc} Infrastructure, highlight the differences to the underlying Fedora repository, and demonstrate its powerful und application-centric programming model. In the end of 2008, we released version 1.0 of the {eSciDoc} Infrastructure. Digital Repositories undergo yet again a substantial change of paradigm. While they started several years ago with a library perspective, mainly focusing on publications, they are now becoming more and more a commodity tool for the workaday life of researchers. Quite often the repository itself is just a background service, providing storage, persistent identification, preservation, and discovery of the content. It is hidden from the end-user by means of specialized applications or services. Fedora's approach of providing a repository architecture rather than an end-user tool accommodates well to this evolution. {eSciDoc}, from the start of the project nearly five years ago, has emphasized this design pattern by separating backend services ({eSciDoc} Infrastructure) and front-end applications ({eSciDoc} Solutions).},
	pages = {227--238},
	booktitle = {Research and Advanced Technology for Digital Libraries: 13th European Conference. {ECDL} 2009, Corfu, Greece, September 27 - October 2, 2009, Proceedings},
	publisher = {Springer},
	author = {Razum, Matthias and Schwichtenberg, Frank and Wagner, Steffen and Hoppe, Michael},
	editor = {Agosti, Maristella and Borbinha, José and Kapidakis, Sarantos},
	date = {2009-10-01},
	langid = {english},
	keywords = {goal\_Collaboration, obj\_Infrastructures, obj\_VREs},
}

@book{hollender_erfolgreich_2012,
	location = {München},
	title = {Erfolgreich recherchieren - Romanistik},
	isbn = {978-3-11-027104-1},
	url = {http://www.degruyter.com/view/product/179936?format=B},
	series = {Erfolgreich recherchieren},
	abstract = {Der Band Erfolgreich recherchieren – Romanistik bietet einen umfassenden Überblick zu allen Teilgebieten der Romanistik. Im Vordergrund stehen elektronische Angebote, berücksichtigt werden aber auch die gedruckten Informationsmittel, die für die Romanistik nach wie vor unverzichtbar sind. In einem ›Basic‹-Kapitel lernen Sie von Grund auf, wie Sie Bücher und Aufsätze zur romanistischen Lingustik und Literaturwissenschaft suchen und finden und wie Sie diese Werke in elektronischer und gedruckter Form beschaffen. Hier erfahren Sie auch, wie Bibliotheken effektiv \&\#8211; also zeitsparend und nachhaltig \&\#8211; zu nutzen sind. Ein ›Advanced‹-Kapitel führt die Recherchetipps dann sowohl methodisch als auch am praktischen Einzelfall orientiert weiter aus, so dass Sie sich auf dem weiten Feld der Möglichkeiten bald professionell bewegen können. Ein eigener Teil des Buches widmet sich schließlich der Weiterverarbeitung der gefundenen Informationen, d.h. der Dokumentenlieferung, der Verwendung von Literaturverwaltungsprogrammen und dem richtigen Zitieren. Ob also für das erste Referat oder die Abschluss- oder Doktorarbeit \&\#8211; hier bekommen Sie einen kompetenten Leitfaden für die erfolgreiche romanistische Recherche an die Hand. Die Reihe Nur wer heutzutage effektiv mit Informationsressourcen umgehen kann, ist in der Lage, das Studium erfolgreich zu meistern. Die Bände der Reihe Erfolgreich recherchieren bieten einen ebenso schnellen wie kompetenten Überblick für den Benutzer zu jedem Zeitpunkt des Studiums. Sie stammen von renommierten „Informationsprofis” aus den Universitäts- und Fachbibliotheken. Regelmäßige Neuauflagen sind geplant. === Inhaltsverzeichnis Einleitung: Googlest Du noch oder recherchierst Du schon? 1 1 Basics 6 1.1 Literatur finden: Basics 6 1.1.1 Wikipedia 6 1.1.2 Der erste Gang in die Bibliothek 7 1.1.3 {MLA} International Bibliography ({MLA}) 11 1.1.4 Online Contents ({OLC}) 13 1.1.5 Internationale Bibliographie der geistes- und sozialwissenschaftlichen Zeitschriftenliteratur ({IBZ}) 14 1.1.6 Periodicals Index Online ({PIO}) 17 1.1.7 Die französische Datenbank zu den Geistes- und Sozialwissenschaften {FRANCIS} 20 1.1.8 Bases de datos bibliográficas del {CSIC} 21 1.1.9 Die Datenbank zu spanischer wissenschaftlicher Literatur {DIALNET} 22 1.1.10 Handbook of Latin American Studies ({HLAS}) 22 1.1.11 Articoli italiani di periodici accademici/Bibliographie der italienischen Zeitschriftenliteratur ({AIDA} online) 23 1.1.12 Riviste di italianistica nel mondo (Italinemo) 24 1.2 Literatur beschaffen: Basics 25 1.2.1 Der Katalog Ihrer Bibliothek ({OPAC}) 25 1.2.2 Der „Deutschlandkatalog“ (und mehr): der Karlsruher Virtuelle Katalog ({KVK}) 30 1.2.3 {WorldCat} 31 1.2.4 Die Zeitschriftendatenbank ({ZDB}) 32 1.2.5 Die Elektronische Zeitschriftenbibliothek ({EZB}) 33 1.2.6 Das Datenbank-Infosystem ({DBIS}) 34 2 Advanced 38 2.1 Literatur finden: Advanced 38 2.1.1 Kindlers Literatur Lexikon 38 2.1.2 Kritische Lexika zur Gegenwartsliteratur 42 2.1.3 Literature Resource Center 44 2.1.4 Romanische Bibliographie 46 2.1.5 Bibliographie der französischen Literaturwissenschaft (Klapp) 56 2.1.6 Bibliographie de la littérature française ({XVIe}–{XXe} siècles)(Rancoeur) 58 2.1.7 French 17 61 2.1.8 French {XX} bibliography 61 2.1.9 Bibliografia Generale della Lingua e della Letteratura Italiana ({BiGLI}) 62 2.1.10 Letteratura Italiana. Aggiornamento Bibliografico ({LIAB}) 66 2.1.11 Letteratura Italiana. Repertorio Automatizzato ({LIRA}) 68 2.1.12 Bibliography of Linguistic Literature Database ({BLLDB}) 69 2.1.13 Linguistics and Language Behavior Abstracts ({LLBA}) 70 2.1.14 Linguistic Bibliography Online 73 2.1.15 Web of Knowledge/Arts and Humanities Citation Index 74 2.2 Literatur beschaffen: Advanced 76 2.2.1 Portale für Retrodigitalisate und andere Volltexte 76 2.2.1.1 Google Books 76 2.2.1.2 Gallica 77 2.2.1.3 Europeana 81 2.2.1.4 Hispana 82 2.2.1.5 Internetculturale/Biblioteca digitale 82 2.2.1.6 Google Scholar 83 2.2.1.7 {BASE} 84 2.2.1.8 {OAIster} 85 2.2.1.9 Internet Archive 85 2.2.1.10 {HathiTrust} 86 2.2.2 Ausländische Bibliothekskataloge 87 2.2.2.1 Catalogue de la Bibliothèque nationale de France 87 2.2.2.2 Verbundkatalog der französischen Universitätsbibliotheken({SUDOC}) 88 2.2.2.3 Catálogo de la Biblioteca Nacional de España 89 2.2.2.4 Verbundkatalog der spanischen Universitätsbibliotheken({REBIUN}) 89 2.2.2.5 Internetculturale/Servizio bibliotecario nazionale 89 2.2.3 Virtuelle Fachbibliotheken und Linksammlungen 90 2.2.3.1 {ViFaRom} 90 2.2.3.2 Cibera 90 2.2.3.3 Les signets de la {BnF} 91 2.2.4 Zeitschriftenvolltextarchive 92 2.2.4.1 {JSTOR} 93 2.2.4.2 Project {MUSE} 94 2.2.4.3 Periodicals Archive Online ({PAO}) 95 2.2.4.4 Digizeitschriften – Das deutsche digitale Zeitschriftenarchiv 97 2.2.4.5 Persée 98 2.2.4.6 Torrossa/Editoria Italiana Online ({EIO}) – Periodici 99 3 Informationen weiterverarbeiten 100 3.1 Treffer bewerten, exportieren und verwalten 100 3.2 Dokumentlieferung 103 3.2.1 Fernleihe 103 3.2.2 Subito 104 3.2.3 {RefDoc} 105 3.2.4 {eBooks} on Demand ({EOD}) 105 3.3 Literatur richtig zitieren 105 Anstelle eines Glossars 113 Verzeichnis der Informationsressourcen 114 Noch mehr Literaturhinweise, Ratgeber, Datenbanken, Bibliographien und Quellen 118 Sachregister 125 Abbildungsnachweis 127},
	publisher = {De Gruyter Saur},
	author = {Hollender, Ulrike},
	urldate = {2012-07-08},
	date = {2012},
	langid = {german},
	note = {Broschiert: 19,95 € Auch als E-Book erhältlich.},
	keywords = {act\_Discovering, obj\_Research},
}

@book{jankowski_e-research:_2009,
	location = {New York},
	title = {e-Research: transformation in scholarly practice},
	isbn = {9780415990288},
	url = {http://scholarly-transformations.virtualknowledgestudio.nl/},
	series = {Routledge advances in research methods},
	shorttitle = {e-Research},
	pagetotal = {349},
	number = {1},
	publisher = {Routledge},
	editora = {Jankowski, Nick},
	editoratype = {collaborator},
	date = {2009},
	langid = {english},
	note = {This site is a companion to e-Research: Transformation in Scholarly Practice (Routledge, 2009) and includes material both supplementing and expanding what is contained in the book. The site contains background information, new research and resources, visualizations, and opportunity to post comments and contact contributors. It is a dynamic site in the sense that additions and revisions are frequently made. There is a list of blogs related to the topics of the book. The basic purpose of the site is to contribute to understanding changes ongoing in scholarly practice and to facilitate new research ventures in exploring these transformations. We welcome your queries and suggestions for scholarly collaboration.},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{dalen-oskam_epistolary_2014,
	title = {Epistolary voices. The case of Elisabeth Wolff and Agatha Deken},
	volume = {29},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/29/3/443},
	doi = {10.1093/llc/fqu023},
	abstract = {The article focuses on two related issues: authorship distinction and the analysis of characters’ voices in fiction. It deals with the case of Elisabeth Wolff and Agatha Deken, two women writers from the Netherlands who collaboratively published several epistolary novels at the end of the 18th century. First, the task division between the two authors will be analysed based on their usage of words and their frequencies. Next, any stylistic differences between the characters (letter writers) will be dealt with. The focus lies on Wolff’s and Deken’s first joint novel, Sara Burgerhart (1782). As to the authorship, nothing clearly showed a clear task division, which implies that Deken’s and Wolff’s writing styles are very much alike. This confirms findings of other scholars, who found that collaborating authors jointly produce a style that is distinguishable from both authors’ personal styles. As to stylistic differences in the voices of the characters in Sara Burgerhart, it was found that only a couple of the letter writers are clearly distinguishable compared with the main characters in the novel. I experimented with two possible tools to zoom in on the exact differences between those characters, but the methods are still too subjective to my taste. In the follow-up research, I will look further than words and their frequencies as building stones of literary style.},
	pages = {443--451},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Dalen-Oskam, Karina van},
	urldate = {2014-09-22},
	date = {2014-09-01},
	langid = {english},
	keywords = {act\_StylisticAnalysis, goal\_Analysis, obj\_Letters, obj\_Text},
}

@collection{ajouri_empirie_2013,
	location = {Münster},
	title = {Empirie in der Literaturwissenschaft},
	isbn = {9783897854581 3897854589},
	url = {http://www.ndl-medien.uni-kiel.de/personal/mitarbeiter/assistenten/christoph_rauen/empirie-in-der-literaturwissenschaft/image_view_fullscreen},
	abstract = {Literaturwissenschaftler wie Geisteswissenschaftler überhaupt grenzen sich häufig von den sogenannten »empirischen Wissenschaften« ab. Dem liegt aber ein verengtes Verständnis von Empirie zu Grunde, hinter das der Sammelband zurückzusetzen versucht. Ausgangspunkt der Beiträge ist eine Auffassung von Literaturwissenschaft als Realwissenschaft: Literarische Texte wie auch ihr historisches Bedingungsgefüge, die sogenannten Kontexte, gelten als empirisch beobachtbare Sachverhalte. Und auch wenn die eigentümliche ästhetische Erfahrung im Mittelpunkt steht, geht es nicht um Metaphysik, sondern um soziale und psychologische Realitäten, die anhand von Quellen rekonstruierbar und deren Gesetzmäßigkeiten interdisziplinär erforschbar sind. ›Empirisch‹ bezeichnet dabei nicht einen bestimmten Satz an Methoden, sondern viel grundsätzlicher den Versuch, Aussagen beobachtungssprachlich zu formulieren und auf diese Weise kritisierbar zu machen – eine Herangehensweise, die Literaturwissenschaftler mit Vertretern anderer Disziplinen teilen und für die es in der Geschichte des Fachs viele Beispiele gibt. Diese Traditionen methodologisch zu reflektieren und fortzusetzen ist das Ziel der hier versammelten Beiträge},
	publisher = {Mentis},
	editor = {Ajouri, Philip and Mellmann, Katja and Rauen, Christoph},
	date = {2013},
	langid = {german},
	keywords = {goal\_Analysis, goal\_Enrichment, meta\_Assessing},
}

@book{lucia_megias_elogio_2012,
	location = {Madrid},
	title = {Elogio del text digital. Claves para interpretar el cambio de paradigma},
	isbn = {978-84-15174-33-2},
	url = {http://revistacaracteres.net/revista/vol1n1mayo2012/resena-elogio-del-texto-digital-de-jose-manuel-lucia-megias/},
	publisher = {Fórcola Ediciones},
	author = {Lucía Megías, José Manuel},
	editora = {Celaya, Javier},
	editoratype = {collaborator},
	date = {2012},
	langid = {spanish},
	keywords = {meta\_Theorizing},
}

@online{suarez_humanista_2012,
	title = {El humanista digital},
	url = {http://www.cultureplex.ca/publications/el-humanista-digital/},
	titleaddon = {{CulturePlex}},
	author = {Suárez, Juan Luis},
	date = {2012},
	langid = {spanish},
	keywords = {obj\_DigitalHumanities},
}

@book{hockey_electronic_2000,
	location = {Oxford},
	title = {Electronic Texts in the Humanities},
	isbn = {0198711948},
	url = {http://ukcatalogue.oup.com/product/9780198711940.do},
	abstract = {The first overview of electronic texts in the humanities
    Shows Internet users what tools and techniques can be used to analyse and manipulate electronic texts

With word processing and the Internet, computing is much more part and parcel of the everyday life of the humanities scholar, but computers can do much more than assist with writing or Internet searching. This book introduces a range of tools and techniques for manipulating and analysing electronic texts in the humanities. It shows how electronic texts can be used for the literary analysis, linguistic analysis, authorship attribution, and the preparation and publication of electronic scholarly editions. It assesses the ways in which research in corpus and computational linguistics can feed into better electronic tools for humanities research. The tools and techniques discussed in this book will feed into better Internet tools and pave the way for the electronic scholar of the twenty-first century.},
	publisher = {Oxford University Press},
	author = {Hockey, Susan},
	date = {2000},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, X-{CHECK}, meta\_GiveOverview},
}

@thesis{schrader_xml-datenformat_2007,
	location = {Tübingen},
	title = {Ein {XML}-Datenformat zur Repräsentation kritischer Musikedition unter besonderer Berücksichtigung von Neumennotation},
	url = {http://www.dimused.uni-tuebingen.de/downloads/studienarbeit.pdf},
	pagetotal = {64},
	institution = {Tübingen},
	type = {{MA} level},
	author = {Schräder, Gregor},
	editora = {Morent, Stefan},
	editoratype = {collaborator},
	date = {2007},
	langid = {german},
	keywords = {obj\_Music, t\_Encoding, t\_XML},
}

@book{jansen_einfuhrung_2006,
	location = {Wiesbaden},
	title = {Einführung in die Netzwerkanalyse: Grundlagen, Methoden, Forschungsbeispiele},
	isbn = {3531150545 9783531150543},
	url = {http://www.springer.com/springer+vs/soziologie/book/978-3-531-15054-3},
	shorttitle = {Einführung in die Netzwerkanalyse},
	abstract = {Netzwerkanalyse, soziale Strukturen und soziales Kapital - Geschichte der Netzwerkanalyse - Merkmalsträger, Merkmale und Analyseebenen - Erhebung von Netzwerkdaten - Einfache Analyseverfahren - Zentralität und Prestige in Netzwerken - Macht, Einfluss und Autonomie in Netzwerken - Teilgruppen in Netzwerken - Forschungsfelder der Netzwerkanalyse - Perspektiven der Analyse sozialer Netzwerke - Anhang: Software zur Netzwerkanalyse Dr. Dorothea Jansen ist Professorin für Soziologie der Organisation an der Deutschen Hochschule für Verwaltungswissenschaften in Speyer.},
	publisher = {{VS}, Verl. für Sozialwiss.},
	author = {Jansen, Dorothea},
	date = {2006},
	langid = {german},
	keywords = {*****},
}

@article{neuroth_e-humanities_2007,
	title = {e-Humanities - eine virtuelle Forschungsumgebung für die Geistes-, Kultur- und Sozialwissenschaften},
	url = {https://www.b2i.de/fileadmin/dokumente/BFP_Bestand_2007/Jg_31-Nr_3/Jg_31-Nr_3_Aufsaetze/Jg_31-2007-Nr_3-S_272-279.pdf},
	abstract = {Digital technologies increasingly permeate research and education in the arts, humanities, and social sciences. Their
digital environment expands continuously, and the next evolutionary step is imminent as the field embraces the op-
portunities of modern network technologies and social software. Accessibility of primary data; embedded in a web of
supplementary material; availability of generic tools for analysis and supporting the scientific process; collaboration
between scholars across disciplines – all these features are part and parcel of the emerging „e-Humanities“. Virtual
research environments are composed of three layers: research, research infrastructure, and the basic infrastructure.
This article aims to specify these layers and define potential roles and responsibilities within them, with a special view
at a research infrastructure for the e-Humanities},
	pages = {272--279},
	journaltitle = {Bibliothek},
	author = {Neuroth, Heike and Aschenbrenner, Andreas and Lohmeier, Felix},
	date = {2007},
	langid = {german},
	keywords = {obj\_VREs},
}

@inproceedings{gleim_ehumanities_2009,
	title = {{eHumanities} Desktop - An Online System for Corpus Management and Analysis in Support of Computing in the Humanities},
	url = {http://www.hucompute.org/data/pdf/gleim_waltinger_ernst_mehler_esch_feith_2009.pdf},
	abstract = {This paper introduces
{eHumanities} Desk-
top
- an online system for corpus manage-
ment and analysis in support of Comput-
ing in the Humanities. Design issues and
the overall architecture are described as
well as an initial set of applications which
are offered by the system},
	eventtitle = {{EACL} Conference 2009},
	booktitle = {Proceedings of the Demonstrations Session of the 12th Conference of the European Chapter of the Association for Computational Linguistics ({EACL})},
	author = {Gleim, Rüdiger and Waltinger, Ulli and Ernst, Alexandra and Mehler, Alexander and Feith, Tobias and Esch, Dietmar},
	date = {2009},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, goal\_Collaboration, obj\_Infrastructures, obj\_VREs},
}

@article{jones_e-carrel:_2010,
	title = {E-Carrel: An Environment for Collaborative Textual Scholarship},
	volume = {1},
	url = {https://letterpress.uchicago.edu/index.php/jdhcs/article/view/54},
	shorttitle = {E-Carrel},
	abstract = {The E-Carrel project aims to address the preservation of, access to, and re-uses of humanities electronic text files. It enables dynamic, growing resource projects as repositories for new knowledge. It provides for on-line distributed data and tools that are open to new scholarly enhancement through a user friendly tagging tool, sophisticated use of stand-off markup and annotation (leveraging {RDF} capabilities), and a browsing system anyone can use. It creates a secure system of text preparation and dissemination that encourages collaboration and participation by anyone interested in the texts. To insure the endurance of authenticated texts, multiple copies are distributed on the Internet. Foundation texts anchor a system for maintaining and growing project usefulness beyond the originators’ interest and the functions they imagined. Increasing access to humanities texts as useful, adaptable, reliable source materials that can be re-purposed will increase interest in continued maintenance, which are critical for long-term preservation and access.},
	number = {2},
	journaltitle = {Journal of the Chicago Colloquium on Digital Humanities and Computer Science},
	shortjournal = {{JDHCS}},
	author = {Jones, Steven E. and Shillingsburg, Peter and Thiruvathukal, George K.},
	date = {2010-06-16},
	langid = {english},
	keywords = {act\_Publishing, bigdata{\textasciitilde}, goal\_Collaboration, goal\_Enrichment, obj\_Infrastructures, obj\_VREs},
}

@report{kircz_e-based_2004,
	location = {Amsterdam},
	title = {E-based Humanities and E-humanities on a {SURF} plat form},
	url = {http://hdl.handle.net/11245/2.26364},
	abstract = {As of 2003, {SURF} enables three platforms: {ICT} and Research, Education, and Organisation. Within these programmes, {SURF} has funds available to promote {ICT} innovations. Innovation is not an easy notion to explain. Too often we encounter new wine in old bottles and changes in vocabulary frequently cover the continuation of existingprogrammes in a new framework.This explorative study was commissioned to review the underlaying issues and to suggest new directions for research that will exploit {ICT} in the Humanities to our best advantage. Thus, the purpose of this study is to provide an insight in the complicated relationship between so-called E-techniques and the humanities or better E-humanities. Based on thisgeneral insight, suggestions for a {SURF} policy in the E-humanities are offered.},
	institution = {{KRA} for {SURF}-{DARE}},
	author = {Kircz, Joost},
	date = {2004},
	langid = {english},
	keywords = {X-{CHECK}},
}

@incollection{burnard_du_2012,
	title = {Du literary and linguistic computing aux digital humanities : retour sur 40 ans de relations entre sciences humaines et informatique},
	rights = {Licence Creative Commons Attribution - Pas d’Utilisation Commerciale - Pas de Modification 3.0 France},
	isbn = {978-2-8218-1324-3},
	url = {http://press.openedition.org/242},
	shorttitle = {Du literary and linguistic computing aux digital humanities},
	abstract = {{IntroductionJe} me suis fixé le but ambitieux de proposer un point de vue sur quarante ans d’histoire, sous la forme d’une synthèse, plutôt que de vous présenter des faits les uns à la suite des autres. J’ai essayé d’extraire de mon expérience quelques principes généraux.Je commencerai en témoignant du fait que nous vivons de façon irréversible un âge numérique. J’expliquerai cela en présentant les trois périodes successives qui ont marqué le développement des relations entre sciences humaines et informatique, ainsi que les principes qui les ont portées. Enfin, pour parler de façon concrète, je présenterai le cas spécifique de mon expérience à Oxford, ce qui permettra d’approfondir l’état courant des recherches dans les sciences humaines et sociales ({SHS}) informatisées.Il y a toujours eu une vraie tension entre les deux méthodes opposées dont on peut se servir dans les sciences humaines et sociales et l’informatique ; une confrontation entre le texte et les données. Certains diront qu’on peut se servir d’un ordinateur pour faire du data processing, pour gérer des données : des chiffres, des faits, des observations, des objets, des tendances statistiques. Le texte, en revanche, est composé d’autres choses : de mots, d’une langue, de paroles, qui ont une existence tout à fait indépendante de leur représentation dans un format numérique. Les données numériques, elles, n’existent que dans leur expression informatisée. C’est précisément là que réside la tension entre les deux. Pour [...]},
	pages = {45--58},
	booktitle = {{OpenEdition} Press},
	publisher = {{OpenEdition} Press},
	author = {Burnard, Lou},
	urldate = {2012-09-24},
	date = {2012-09-18},
	langid = {french},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{fitzpatrick_risky_2011,
	title = {Do 'the Risky Thing' in Digital Humanities},
	issn = {0009-5982},
	url = {http://chronicle.com/article/Do-the-Risky-Thing-in/129132/},
	journaltitle = {The Chronicle of Higher Education},
	author = {Fitzpatrick, Kathleen},
	urldate = {2011-10-04},
	date = {2011-09-25},
	langid = {english},
	keywords = {meta\_Assessing, meta\_GiveOverview, obj\_DigitalHumanities},
}

@incollection{mair_-support_nodate,
	location = {Oxford},
	title = {Do-support with got (to) in contemporary American English – from opportunistic to systematic use of the Web as corpus},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199922765.001.0001/oxfordhb-9780199922765-e-23},
	abstract = {The chapter argues that the best way to profit from the rich corpus-linguistic working environment available to the student of the history of English is to use traditional (and sometimes small) linguistic corpora together with larger textual databases and digital archives, including the World-Wide Web, in a coordinated way. Linguistic corpora ({ARCHER}, Brown family, {BNC}, {COCA}, {COHA}) are sufficient to document the successive waves of grammaticalisation which have added have to, have got to and, more recently, want to or need to to the older form must, producing the complex layered system of present-day English modal markers of obligation and necessity. Using do-support with modal got (to)/gotta as an illustration, the paper shows that, in spite of its known deficiencies as a linguistic corpus, the World-Wide Web can help fill in the language-historical picture in useful ways where even the biggest available corpora fail to produce sufficient evidence.},
	booktitle = {Handbook on the History of English: Rethinking Approaches to the History of English},
	publisher = {Oxford Univ. Press},
	author = {Mair, Christian},
	editor = {Nevalainen, Terttu and Traugott, Elizabeth Closs},
	langid = {english},
	doi = {10.1093/oxfordhb/9780199922765.013.0023},
	keywords = {{AnalyzeStatistically}, bigdata, obj\_Web},
}

@inproceedings{eder_does_2010,
	location = {London},
	title = {Does Size Matter? Authorship Attribution, Small Samples, Big Problem.},
	url = {http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/html/ab-744.html},
	abstract = {The aim of this study is to find a minimal size of text samples for authorship attribution that would provide stable results independent of random noise. A few controlled tests for different sample lengths, languages and genres are discussed and compared. Although I focus on Delta methodology, the results are valid for many other multidimensional methods relying on word frequencies and "nearest neighbor" classifications.

In the field of stylometry, and especially in authorship attribution, the reliability of the obtained results becomes even more essential than the results themselves: failed attribution is much better than false attribution (cf. Love, 2002). However, while dozens of outstanding papers deal with increasing the effectiveness of current stylometric methods, the problem of their reliability remains somehow underestimated. Especially, the simple yet fundamental question of the shortest acceptable sample length for reliable attribution has not been discussed convincingly.

In many attribution studies based on short samples, despite their well-established hypotheses, convincing choice of style-markers, advanced statistics applied and brilliant results presented, one cannot avoid a very simple yet uneasy question: whether those impressive results could be obtained by chance, or at least positively affected by randomness? This question can be also formulated in a different way: if a cross-checking experiment with numerous short samples were available, would the results be just as satisfying},
	pages = {132--134},
	booktitle = {Digital Humanities 2010: Conference Abstracts},
	publisher = {King’s College London},
	author = {Eder, Maciej},
	urldate = {2011-12-14},
	date = {2010-07},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, t\_Stylometry},
}
@article{collins_docuburst:_2009,
	title = {Docuburst: Visualizing Document Content Using Language Structure},
	volume = {28},
	url = {http://vialab.science.uoit.ca/portfolio/docuburst-visualizing-document-content-using-language-structure},
	abstract = {{DocuBurst} is the first visualization of document content which takes advantage of the human-created structure in lexical databases. We use an accepted design paradigm to generate visualizations which improve the usability and utility of {WordNet} as the backbone for document content visualization. A radial, space-filling layout of hyponymy ({IS}-A relation) is presented with interactive techniques of zoom, filter, and details-on-demand for the task of document visualization. The techniques can be generalized to multiple documents.},
	pages = {1039--1046},
	number = {2},
	journaltitle = {Computer Graphics Forum},
	author = {Collins, C. and Carpendale, S. and Penn, G.},
	date = {2009},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_Visualizing, obj\_Tools},
}

@article{eder_birds_2013,
	title = {Do birds of a feather really flock together, or how to choose training samples for authorship attribution},
	volume = {28},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/28/2/229},
	doi = {10.1093/llc/fqs036},
	abstract = {This study investigates the problem of appropriate choice of texts for the training set in machine-learning classification techniques. Although intuition suggests picking the most typical texts (whatever ‘typical’ means) by the authors studied, any arbitrary choice might substantially affect the final results. Thus, to eschew cherry picking, we introduce a method of verification of the choice of ‘typical’ samples, inspired by k-fold cross-validation procedures. Namely, we use a bootstrap-like approach to choose randomly, in 500 iterations, the samples for the training and the test sets. Next, we examine the obtained 500 attribution accuracy scores: if the density function shows widespread results, the corpus is assumed to be very sensitive to the permutations of the training set. To test this methodology empirically, we have selected roughly similar corpora in five languages: English, French, German, Italian, and Polish. The results show considerable resistance of the English corpus to permutations, while the other corpora turned out to be more dependent on the choice of the samples; the Polish corpus produces both accuracy and consistency below any acceptable standards.},
	pages = {229--236},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Eder, Maciej and Rybicki, Jan},
	urldate = {2013-06-12},
	date = {2013-06-01},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, t\_MachineLearning},
}

@article{davis_frost_divided_2011,
	title = {Divided and Conquered. How Multivarious Isolation is Suppressing Digital Humanities Scholarship},
	journaltitle = {{NITLE}},
	author = {Davis Frost, Rebecca and Dombrowski, Quinn},
	date = {2011-04},
	langid = {english},
	keywords = {meta\_Advocating, obj\_DigitalHumanities},
}

@book{moretti_distant_2013,
	location = {London},
	title = {Distant reading},
	isbn = {9781781680841},
	url = {http://www.amazon.de/Distant-Reading-Franco-Moretti/dp/1781680841},
	abstract = {How does a literary historian end up thinking in terms of z-scores, principal component analysis, and clustering coefficient?

In the ten essays collected in this volume, Franco Moretti reconstructs the intellectual trajectory of his philosophy of ‘distant reading’. From the evolutionary model of ‘Modern European Literature’, through the geo-cultural dominant of ‘Conjectures on World Literature’ and ‘Planet Hollywood’ to the quantitative findings of ‘Style, inc.’ and the abstract patterns of ‘Network Theory, Plot Analysis’, the book follows two decades of critical explorations that have come to define – well beyond the wildest expectations of its author – a growing field of unorthodox literary studies.},
	pagetotal = {224},
	publisher = {Verso},
	author = {Moretti, Franco},
	date = {2013},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@article{huh_discriminative_2012,
	title = {Discriminative Topic Modeling Based on Manifold Learning},
	volume = {5},
	issn = {1556-4681},
	url = {http://doi.acm.org/10.1145/2086737.2086740},
	doi = {10.1145/2086737.2086740},
	abstract = {Topic modeling has become a popular method used for data analysis in various domains including text documents. Previous topic model approaches, such as probabilistic Latent Semantic Analysis ({pLSA}) and Latent Dirichlet Allocation ({LDA}), have shown impressive success in discovering low-rank hidden structures for modeling text documents. These approaches, however do not take into account the manifold structure of the data, which is generally informative for nonlinear dimensionality reduction mapping. More recent topic model approaches, Laplacian {PLSI} ({LapPLSI}) and Locally-consistent Topic Model ({LTM}), have incorporated the local manifold structure into topic models and have shown resulting benefits. But they fall short of achieving full discriminating power of manifold learning as they only enhance the proximity between the low-rank representations of neighboring pairs without any consideration for non-neighboring pairs. In this article, we propose a new approach, Discriminative Topic Model ({DTM}), which separates non-neighboring pairs from each other in addition to bringing neighboring pairs closer together, thereby preserving the global manifold structure as well as improving local consistency. We also present a novel model-fitting algorithm based on the generalized {EM} algorithm and the concept of Pareto improvement. We empirically demonstrate the success of {DTM} in terms of unsupervised clustering and semisupervised classification accuracies on text corpora and robustness to parameters compared to state-of-the-art techniques.},
	pages = {20:1--20:25},
	number = {4},
	journaltitle = {{ACM} Trans. Knowl. Discov. Data},
	author = {Huh, Seungil and Fienberg, Stephen E.},
	urldate = {2012-04-19},
	date = {2012-02},
	langid = {english},
}

@article{schmoldt_digitoxin_1975,
	title = {Digitoxin metabolism by rat liver microsomes},
	volume = {24},
	issn = {0006-2952},
	url = {http://www.researchgate.net/publication/23132237_Digitoxin_metabolism_by_rat_liver_microsomes},
	pages = {1639--1641},
	number = {17},
	journaltitle = {Biochemical pharmacology},
	shortjournal = {Biochem. Pharmacol.},
	author = {Schmoldt, A and Benthe, H F and Haberland, G},
	date = {1975-09-01},
	langid = {english},
}

@online{liu_digital_2012,
	title = {Digital Toychest for Humanists},
	url = {http://toychest.pbworks.com/},
	abstract = {"Toy Chest" collects online or downloadable software tools and thinking toys that humanities students and others without programming skills (but with basic computer and Internet literacy) can use to create interesting projects. Most of the tools gathered here are free or relatively inexpensive (exceptions: items that are expensive but can be used on a free trial basis). Also on this site are "paradigms"--books, essays, digital projects, etc.--that illustrate the kinds of humanities projects that these thinking tools/toys might help create.},
	author = {Liu, Alan},
	date = {2012},
	langid = {english},
	keywords = {obj\_Tools},
}

@book{stiegler_digital_2014,
	location = {Limoges},
	title = {Digital studies : Organologie des savoirs et technologies de la connaissance},
	isbn = {9782364051089},
	shorttitle = {Digital studies},
	abstract = {Le numérique bouleverse les savoirs, et depuis quelques années a émergé le concept de digital humanities (humanités numériques), paradigme à travers lequel les sciences de l homme et de la société prennent acte de ce devenir. Cet ouvrage, qui s inscrit évidemment dans cette dynamique, pose cependant en principe que les digital humanities ne sont qu une dimension de ce qu il faut appréhender plus largement comme les digital studies, lesquelles concernent toutes les formes de savoirs, théoriques aussi bien que pratiques. Il soutient autrement dit que le numérique constitue une mutation globale des savoirs sous toutes leurs formes (scientifiques, artistiques, politiques, sociaux au sens le plus large, pratiques dans tous les domaines) qui pose des questions épistémologiques fondamentales et radicalement nouvelles. Le contexte géopolitique de cette réflexion qui a donné lieu à la constitution d un réseau international par l Institut de recherche et d innovation (cf. digital-studies.org) est l émergence d une industrie planétaire des savoirs pour laquelle l Amérique du Nord tente de constituer ce que l on appelle désormais un smart power.},
	pagetotal = {192},
	publisher = {{FYP} {EDITIONS}},
	author = {Stiegler, Bernard},
	date = {2014-03-10},
	langid = {french},
}

@incollection{price_digital_nodate,
	title = {Digital Scholarly Editing},
	isbn = {978-1-60329-145-3},
	url = {http://dlsanthology.commons.mla.org/digital-scholarly-editing/},
	booktitle = {Literary Studies in the Digital Age},
	publisher = {Modern Language Association of America},
	author = {Schreibman, Susan},
	editor = {Price, Kenneth M. and Siemens, Ray},
	urldate = {2014-03-05},
	langid = {english},
	keywords = {goal\_Enrichment, obj\_Text},
}

@book{bodard_digital_2010,
	location = {Farnham, Surrey, England},
	title = {Digital Research in the Study of Classical Antiquity},
	isbn = {9780754677734},
	url = {http://www.ashgate.com/isbn/9780754677734},
	series = {Digital research in the arts and humanities},
	abstract = {This book explores the challenges and opportunities presented to Classical scholarship by digital practice and resources. Drawing on the expertise of a community of scholars who use innovative methods and technologies, it shows that traditionally rigorous scholarship is as central to digital research as it is to mainstream Classical Studies. The chapters in this edited collection cover many subjects, including text and data markup, data management, network analysis, pedagogical theory and the Social and Semantic Web, illustrating the range of methods that enrich the many facets of the study of the ancient world. This volume exemplifies the collaborative and interdisciplinary nature that is at the heart of Classical Studies.},
	pagetotal = {210},
	publisher = {Ashgate},
	editora = {Bodard, Gabriel and Mahony, Simon},
	editoratype = {collaborator},
	date = {2010},
	langid = {english},
	keywords = {X-{CHECK}, meta\_GiveOverview, obj\_AnyObject, obj\_Artefacts},
}

@book{haber_digital_2011,
	location = {München},
	title = {Digital Past. Geschichtswissenschaften im digitalen Zeitalter},
	isbn = {978-3-486-70704-5},
	url = {http://www.amazon.de/Digital-Past-Geschichtswissenschaft-digitalen-Zeitalter/dp/3486707043},
	abstract = {Digitale Ressourcen, multimediale Methoden und das Werkzeug Computer bestimmen scheinbar selbstverständlich die tägliche Arbeit des Historikers, der Historikerin. Peter Haber geht der Frage nach, wie die Digitalisierung der Welt die Wissenschaft von der Vergangenheit verändert. Wie wird Geschichte im 21. Jahrhundert geschrieben? Wie wird unser Wissen neu geordnet? Welche Macht hat Google? Eines ist offensichtlich: die Medienrevolutionen des Web und des Web 2.0 wirken massiv auf das Fach Geschichte und auf die Geschichtswissenschaft ein. Haber macht die Veränderungen der letzten Jahrzehnte greifbar und wagt einen Blick auf neue Perspektiven für die Wissenschaft von der Vergangenheit. "Peter Habers 'Digital Past' sollte Pflichtlektüre für alle sein, die sich für die digitale Geschichtswissenschaft in der Vergangenheit, Gegenwart und Zukunft interessieren." Mills Kelly, Assoziierter Direktor des Center for History and New Media an der George Mason University},
	publisher = {Oldenbourg Wissenschaftsverlag},
	author = {Haber, Peter},
	date = {2011},
	langid = {german},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@book{rogers_digital_2013,
	location = {Cambridge, {MA}},
	title = {Digital methods},
	isbn = {9780262018838 0262018837},
	url = {http://www.amazon.com/Digital-Methods-Richard-Rogers/dp/0262018837},
	abstract = {In Digital Methods, Richard Rogers proposes a methodological outlook for social and cultural scholarly research on the Web that seeks to move Internet research beyond the study of online culture. It is not a toolkit for Internet research, or operating instructions for a software package; it deals with broader questions. How can we study social media to learn something about society rather than about social media use? How can hyperlinks reveal not just the value of a Web site but the politics of association? Rogers proposes repurposing Web-native techniques for research into cultural change and societal conditions. We can learn to reapply such "methods of the medium" as crawling and crowd sourcing, {PageRank} and similar algorithms, tag clouds and other visualizations; we can learn how they handle hits, likes, tags, date stamps, and other Web-native objects. By "thinking along" with devices and the objects they handle, digital research methods can follow the evolving methods of the medium. Rogers uses this new methodological outlook to examine the findings of inquiries into 9/11 search results, the recognition of climate change skeptics by climate-change-related Web sites, the events surrounding the Srebrenica massacre according to Dutch, Serbian, Bosnian, and Croatian Wikipedias, presidential candidates' social media "friends," and the censorship of the Iranian Web. With Digital Methods, Rogers introduces a new vision and method for Internet research and at the same time applies them to the Web's objects of study, from tiny particles (hyperlinks) to large masses (social media).},
	pagetotal = {280},
	publisher = {{MIT} Press},
	author = {Rogers, Richard},
	date = {2013},
	langid = {english},
	keywords = {bigdata{\textasciitilde}},
}

@article{schoch_big?_2013,
	title = {Big? Smart? Clean? Messy? Data in the Humanities},
	volume = {2},
	url = {http://journalofdigitalhumanities.org/2-3/big-smart-clean-messy-data-in-the-humanities/},
	shorttitle = {Data in the Humanities},
	number = {3},
	journaltitle = {Journal of Digital Humanities},
	author = {Schöch, Christof},
	urldate = {2014-09-23},
	date = {2013},
}

@article{sula_citations_2014,
	title = {Citations, contexts, and humanistic discourse: Toward automatic extraction and classification},
	volume = {29},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/29/3/452},
	doi = {10.1093/llc/fqu019},
	shorttitle = {Citations, contexts, and humanistic discourse},
	abstract = {This paper examines prospects and limitations of citation studies in the humanities. We begin by presenting an overview of bibliometric analysis, noting several barriers to applying this method in the humanities. Following that, we present an experimental tool for extracting and classifying citation contexts in humanities journal articles. This tool reports the bibliographic information about each reference, as well as three features about its context(s): frequency, location-in-document, and polarity. We found that extraction was highly successful (above 85\%) for three of the four journals, and statistics for the three citation figures were broadly consistent with previous research. We conclude by noting several limitations of the sentiment classifier and suggesting future areas for refinement.},
	pages = {452--464},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Sula, Chris Alen and Miller, Matthew},
	urldate = {2014-09-22},
	date = {2014-09-01},
	langid = {english},
}

@article{ganascia_automatic_2014,
	title = {Automatic detection of reuses and citations in literary texts},
	volume = {29},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/29/3/412},
	doi = {10.1093/llc/fqu020},
	abstract = {For more than 40 years now, modern theories of literature insist on the role of paraphrases, rewritings, citations, reciprocal borrowings, and mutual contributions of many kinds. The notions of ‘intertextuality’, ‘transtextuality’, and ‘hypertextuality/hypotextuality’ were introduced in the seventies and eighties to approach these phenomena. Through the Phœbus project, computer scientists from the computer science laboratory of the University Pierre and Marie Curie collaborate with the literary teams of Paris-Sorbonne University to develop efficient tools for literary studies that take advantage of modern computer science techniques to detect borrowings of huge masses of texts and to help put them in context. In this context, we have developed a piece of software that automatically detects and explores networks of textual reuses in classical literature. This article describes the principles on which our program is based, the significant results that have already been obtained and the prospective for the near future. It is divided into four parts. The first part recalls the distinction between various types of borrowings like plagiarism, pastiches, citations, etc. The second enumerates the criteria that are retained to characterize reuses and citations on which we are focusing here. The third part describes the implementation and shows its efficiency by comparison with manual detection. Finally, we show some of the results that have already been obtained with the Phœbus program.},
	pages = {412--421},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Ganascia, Jean-Gabriel and Glaudes, Peirre and Lungo, Andrea Del},
	urldate = {2014-09-22},
	date = {2014-09-01},
	langid = {english},
	keywords = {act\_NetworkAnalysis, act\_RelationalAnalysis, goal\_Analysis, obj\_Text},
}

@online{miller_uncovering_2014,
	title = {Uncovering Hidden Text on a 500-Year-Old Map That Guided Columbus},
	url = {http://www.wired.com/2014/09/martellus-map/},
	abstract = {A team of researchers is using a technique called multispectral imaging to uncover the hidden text on a 500-year old map used by Christopher Columbus to plan his first voyage across the Atlantic.},
	titleaddon = {{WIRED}},
	author = {Miller, Greg},
	urldate = {2014-09-21},
	date = {2014-09-15},
	keywords = {act\_DataRecognition, goal\_Enrichment, obj\_Maps},
}

@online{european_society_for_textual_scholarship_lexicon_2012,
	title = {Lexicon of Scholarly Editing [Bibliography]},
	url = {https://www.zotero.org/groups/lexicon_of_scholarly_editing},
	author = {{European Society for Textual Scholarship}},
	date = {2012},
}

@article{romary_sustainable_2014,
	title = {Sustainable data for sustainable infrastructures},
	url = {http://hal.inria.fr/hal-00992220},
	abstract = {{DARIAH}, the Digital Research Infrastructure for the Arts and Humanities, is committed to advancing the digital revolution that has captured the arts and humanities. As more legacy primary and secondary sources become digital, more digital content is being produced and more digital tools are being deployed, we see a next generation of digitally aware scholars in the humanities emerge. {DARIAH} aims to connect these resources, tools and scholars, ensuring that the state-of-the-art in research is sustained and integrated across European countries. To do so, it is important to understand the actual role that proper data modelling and standards could play to make digital content sustainable. Even if it does not seem obvious at first sight that the arts and humanities would be fit for taking up the technological prerequisites of standardisation, we want to show in this paper that we can and should integrate standardisation issues at the core of our {DARIAH} infrastructural work. This analysis may lead us to a wider understanding of the role of scholars within a digital infrastructure and consequently on how {DARIAH} could better integrate a variety of research communities in the arts and humanities.},
	journaltitle = {Facing the Future: European Research Infrastructures for the Humanities and Social Sciences},
	author = {Romary, Laurent},
	urldate = {2014-09-17},
	date = {2014},
}

@article{benardou_conceptual_2010,
	title = {A conceptual model for scholarly research activity},
	pages = {26--32},
	journaltitle = {{iConference} 2010 Proceedings},
	author = {Benardou, Agiatis and Constantopoulos, Panos and Dallas, Costis and Gavrilis, Dimitris},
	date = {2010},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, obj\_Research},
}

@incollection{mccarty_humanities_2003,
	location = {New York},
	title = {Humanities computing},
	pages = {1224--1235},
	booktitle = {Encyclopedia of Library and Information Science},
	publisher = {Dekker},
	author = {{McCarty}, Willard},
	date = {2003},
	langid = {english},
}

@inproceedings{unsworth_scholarly_2000,
	location = {King's College London},
	title = {Scholarly Primitives: what methods do humanities researchers have in common, and how might our tools reflect this?},
	url = {http://people.brandeis.edu/~unsworth/Kings.5-00/primitives.html},
	abstract = {I’m using the term [scholarly] “primitives” in a self-consciously analogical way, to refer to some basic functions common to scholarly activity across disciplines, over time, and independent of theoretical orientation.  These “self-understood” functions form the basis for higher-level scholarly projects, arguments, statements, interpretations—in terms of our original, mathematical/philosophical analogy, axioms.},
	eventtitle = {Humanities Computing: formal methods, experimental practice},
	author = {Unsworth, John},
	urldate = {2011-05-03},
	date = {2000},
	langid = {english},
	keywords = {X-{CHECK}, act\_Conceptualizing, meta\_Advocating, meta\_Theorizing, obj\_DigitalHumanities, obj\_Methods},
}

@book{hayles_digital_2012,
	location = {Chicago},
	title = {Digital Media and contemporary technogenesis},
	isbn = {9780226321424},
	url = {http://press.uchicago.edu/ucp/books/book/chicago/H/bo5437533.html},
	abstract = {Hayles examines the evolution of the field from the traditional humanities and how the digital humanities are changing academic scholarship, research, teaching, and publication. She goes on to depict the neurological consequences of working in digital media, where skimming and scanning, or “hyper reading,” and analysis through machine algorithms are forms of reading as valid as close reading once was. Hayles contends that we must recognize all three types of reading and understand the limitations and possibilities of each.},
	pagetotal = {296},
	publisher = {University of Chicago Press},
	author = {Hayles, N. Katherine},
	date = {2012},
	langid = {english},
	keywords = {act\_Conceptualizing, goal\_Interpretation, meta\_Theorizing, obj\_DigitalHumanities, obj\_Humanities, obj\_Research},
}
@book{hoover_digital_2014,
	location = {London},
	title = {Digital Literary Studies: Corpus Approaches to Poetry, Prose, and Drama},
	isbn = {978-0415352307},
	url = {http://www.amazon.de/Digital-Literary-Studies-Approaches-Linguistics/dp/0415352304},
	abstract = {Digital Literary Studies presents a broad and varied picture of the promise and potential of methods and approaches that are crucially dependent upon the digital nature of the literary texts it studies and the texts and collections of texts with which they are compared. It focuses on style, diction, characterization, and interpretation of single works and across larger groups of texts, using both huge natural language corpora and smaller, more specialized collections of texts created for specific tasks, and applies statistical techniques used in the narrower confines of authorship attribution to broader stylistic questions. It addresses important issues in each of the three major literary genres, and intentionally applies different techniques and concepts to poetry, prose, and drama. It aims to present a provocative and suggestive sample intended to encourage the application of these and other methods to literary studies. Hoover, Culpeper, and O'Halloran push the methods, techniques, and concepts in new directions, apply them to new groups of texts or to new questions, modify their nature or method of application, and combine them in innovative ways},
	pagetotal = {202},
	publisher = {Routledge},
	author = {Hoover, David L. and Culpeper, Jonathan and O'Halloran, Kieran},
	date = {2014},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_Text},
}

@article{cohen_digital_2010,
	title = {Digital Keys for Unlocking the Humanities’ Riches - Humanities 2.0},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2010/11/17/arts/17digital.html},
	journaltitle = {The New York Times},
	author = {Cohen, Patricia},
	urldate = {2011-05-20},
	date = {2010-11-16},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@incollection{kamposiori_digital_2012,
	title = {Digital Infrastructure for Art Historical Research: thinking about user needs},
	url = {http://ewic.bcs.org/content/ConWebDoc/46142},
	abstract = {During the past years, the large technological advancements have provided research communities with applications and services never considered before. However, as the increased implication of the new technologies in the Arts \& Humanities have greatly affected the scholarly research process, the necessity to adapt digital tools and services to the needs of specific groups of researchers has considerably grown. The present paper aims to focus on the informational and methodological behaviour of art historians, so as to identify possible requirements for providing them with functional digital infrastructure. Hence, their research profile, their needs in terms of resources and the methodologies they employ should be examined. The emphasis, in particular, should be given in research activities with great value for art historical research, such as the information seeking and the collection of the required information objects. By supporting these first, stages of research with digital tools and services tailored to the needs of researchers would, actually, facilitate the whole research process in the field. Finally, this paper reports on research conducted for the author’s current {PhD} Thesis “Personal Research Collections: examining research practices and user needs in art historical research”, under the supervision of Prof. Claire Warwick and Mr Simon Mahony.},
	author = {Kamposiori, Christina},
	date = {2012},
	langid = {english},
	keywords = {act\_Discovering, meta\_Assessing, obj\_Infrastructures, obj\_Research},
}

@online{straumsheim_digital_2014,
	title = {Digital humanities won't save the humanities, digital humanists say},
	url = {http://www.insidehighered.com/news/2014/05/08/digital-humanities-wont-save-humanities-digital-humanists-say#sthash.FRHUVk0W.WbfPHQ2u.dpbs},
	titleaddon = {Inside Higher Ed},
	author = {Straumsheim, Carl},
	urldate = {2014-05-08},
	date = {2014-05},
	langid = {english},
}

@article{adams_digital_2012,
	title = {Digital Humanities - Where to Start},
	volume = {73},
	issn = {0099-0086, 2150-6698},
	url = {http://crln.acrl.org/content/73/9/536},
	pages = {536--569},
	number = {9},
	journaltitle = {College \& Research Libraries News},
	shortjournal = {Coll. res. libr. news},
	author = {Adams, Jennifer L. and Gunn, Kevin B.},
	urldate = {2012-10-03},
	date = {2012-10-01},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{gippert_digital_2010,
	title = {Digital Humanities. Was kommt ans Licht, wenn Texte und Bilder digital analysiert werden? Digital Humanities - die empirische Wende in den Geisteswissenschaften},
	url = {http://www.bdsl-onlihttp://www.forschung-frankfurt.uni-frankfurt.de/36050732/05Gippert.pdfne.de/bdsldb/suche/result.xml?Skript=mylistExportDC&contenttype=text/xml&step=1&anzeige=100&vid=4BC421A0-54B3-4D07-869D-A48DDB06232E&Suchtyp=1},
	abstract = {Dass Ludwig Börne seine langjährige vertraute Freundin Jeanette Wohl in einem seiner Briefe scherzhaft mit
»Moppel« ansprach, brachte erst eine Spezialaufnahme ans Licht, denn die Stelle war eigentlich für die Nach-
welt geschwärzt geworden. Nur ein kleines Beispiel, was die digitale Bearbeitung literarischer Originaltexte zu-
tage fördern kann. Immer mehr Dokumente stehen auch in mehrschichtigen Aufnahmen für die wissenschaftli-
che Analyse zur Verfügung. Bald werden die bekanntesten schriftlichen Quellen vom Altertum bis zur Gegenwart
nachhaltig erfasst und gespeichert und somit digital und online aufrufbar sein. Wie nutzen die Frankfurter Geis-
teswissenschaftler diese neuen fast grenzenlosen Chancen? Im {LOEWE}-Schwerpunkt »Digital Humanities« geht
es darum, Textcorpora mit digitalen Methoden auszuwerten, zu vergleichen und mit Bilddaten zu verknüpfen},
	pages = {21--25},
	number = {3},
	journaltitle = {Forschung Frankfurt},
	author = {Gippert, Jost},
	urldate = {2011-04-27},
	date = {2010},
	langid = {german},
	keywords = {{AnalyzeQualitatively}, meta\_GiveOverview, obj\_Images},
}

@online{dworak_digital_2012,
	title = {Digital Humanities Resource Guide},
	url = {http://hastac.org/blogs/melody-dworak/2012/04/23/digital-humanities-resource-guide},
	titleaddon = {{HASTAC}},
	author = {Dworak, Melody},
	date = {2012-04-23},
	langid = {english},
	keywords = {obj\_Research},
}

@book{warwick_digital_2012,
	location = {London},
	edition = {1.},
	title = {Digital Humanities in Practice},
	isbn = {978-1856047661},
	url = {http://www.amazon.de/Digital-Humanities-Practice-Claire-Warwick/dp/1856047660},
	abstract = {This title offers a cutting-edge and comprehensive introduction to this vibrant and increasingly important global field drawing together a broad spectrum of disciplines. Each chapter interweaves the expert commentary of leading academics, analysis of current research and practice and several exciting international case studies, exploring the possibilities and challenges that occur when culture and digital technologies intersect. It covers key topics that include: social media and crowd sourcing; digital images and digitisation; 3D scanning and museums; studying users and readers; electronic text and corpora; archaeology and {GIS}; open access and online teaching of digital humanities; and, books, texts and digital editing. This is an essential practical guide for academics, researchers, librarians and professionals involved in the digital humanities. It will also be core reading for all humanities students and those taking courses in the digital humanities in particular.},
	pagetotal = {192},
	publisher = {Facet Publishing},
	author = {Warwick, Claire and Terras, Melissa and Nyhan, Julianne},
	date = {2012},
	langid = {english},
	keywords = {X-{CHECK}, meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{thiel_digital_2012,
	title = {Digital Humanities Eine empirische Wende für die Geisteswissenschaften?},
	issn = {0174-4909},
	url = {http://www.faz.net/aktuell/feuilleton/forschung-und-lehre/digital-humanities-eine-empirische-wende-fuer-die-geisteswissenschaften-11830514.html},
	abstract = {24.07.2012 · Digitale Werkzeuge und Methoden werden immer wichtiger in den Geisteswissenschaften, sagt eine Empfehlung des Wissenschaftsrats. Läuten die Digital Humanities das Ende hermeneutischer Einzelforschung ein?},
	journaltitle = {{FAZ}.{NET}},
	author = {Thiel, Thomas},
	urldate = {2012-08-22},
	date = {2012-07-24},
	langid = {german},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@incollection{sagw-assh-assm_digital_2012,
	location = {Bern},
	title = {Digital Humanities - die anderen Geisteswissenschaften},
	volume = {1},
	url = {http://gerhardlauer.de/files/7013/3544/5343/lauer_DH.pdf},
	abstract = {Computer
machen
einen
{tJnterschied},
auch
in
den
Geistes-
w
is
s
en
schaften.
Sie
er
mö
glichen
einer
s
eit
s
neue
Metho
d.en
und
verfahren,
sind.
dber
andererseits
hostenintensiu
und
aufwandig.
Welche
Bedeutung
Computer
letztlich
für
die
Geisfesruissen
schaften
haben
werden,
beginnen
wir
gerade
erst
zu
entscheid.en},
	pages = {54--55},
	booktitle = {{SAGW}-Bulletin: Dossier "Digital Humanities und Web 2.0"},
	publisher = {{SAGW}-{ASSH}-{ASSM}},
	author = {Lauer, Gerhard},
	editor = {{SAGW-ASSH-ASSM}},
	date = {2012-02},
	langid = {german},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@online{fluharty_digital_2009,
	title = {Digital History [Bibliography]},
	url = {https://www.zotero.org/groups/digital_history},
	abstract = {Resources for learning, researching, and practicing digital history.  Bibliography of scholarship and other work in digital history. Exploration of current and future interdisciplinary intersections between digital history and humanities, social sciences, computer science, education, linguistics, information science, and cognitive science.},
	titleaddon = {Zotero.org},
	author = {Fluharty, Sterling},
	date = {2009},
	keywords = {obj\_DigitalHumanities},
}

@article{liu_digital_2009,
	title = {Digital Humanities and Academic Change},
	volume = {47},
	issn = {00138282},
	url = {http://search.ebscohost.com/login.aspx?direct=true&db=hlh&AN=43972909&site=ehost-live},
	doi = {Article},
	abstract = {In this article the author argues that the new digital technologies are adversely changing the humanities. These technologies are transmitting strange disciplinary attributes from other paradigms of knowledge. The author states that he will call this new vision the "global humanities." Discussed is the evolutionary explanation of the impact of the digital humanities. The author uses the metaphor of scientist Charles Darwin's origin of species. A list of projects that the author created or collaborated on at the University of California Santa Barbara is provided.},
	pages = {17--35},
	number = {1},
	journaltitle = {English Language Notes},
	author = {Liu, Alan},
	urldate = {2009-12-10},
	date = {2009},
	langid = {english},
	keywords = {obj\_DigitalHumanities},
}

@collection{burdick_digital_2012,
	location = {Cambridge, {MA}},
	title = {Digital Humanities},
	isbn = {9780262018470  0262018470},
	url = {http://www.amazon.com/Digital_Humanities-Anne-Burdick/dp/0262018470},
	abstract = {Digital\_Humanities is a compact, game-changing report on the state of contemporary knowledge production. Answering the question, "What is digital humanities?," it provides an in-depth examination of an emerging field. This collaboratively authored and visually compelling volume explores methodologies and techniques unfamiliar to traditional modes of humanistic inquiry--including geospatial analysis, data mining, corpus linguistics, visualization, and simulation--to show their relevance for contemporary culture.

Included are chapters on the basics, on emerging methods and genres, and on the social life of the digital humanities, along with "case studies," "provocations," and "advisories." These persuasively crafted interventions offer a descriptive toolkit for anyone involved in the design, production, oversight, and review of digital projects. The authors argue that the digital humanities offers a revitalization of the liberal arts tradition in the electronically inflected, design-driven, multimedia language of the twenty-first century.Written by five leading practitioner-theorists whose varied backgrounds embody the intellectual and creative diversity of the field, Digital\_Humanities is a vision statement for the future, an invitation to engage, and a critical tool for understanding the shape of new scholarship.},
	publisher = {{MIT} Press},
	editor = {Burdick, Anne and Lunenfeld, Peter and Drucker, Johanna and Presner, Todd and Schnapp, Jeffrey},
	date = {2012},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@book{cohen_digital_2005,
	location = {Philadelphia, {PA}},
	title = {Digital History: A Guide to Gathering, Preserving, and Presenting the Past on the Web},
	url = {http://chnm.gmu.edu/digitalhistory/},
	abstract = {his book provides a plainspoken and thorough introduction to the web for historians—teachers and students, archivists and museum curators, professors as well as amateur enthusiasts—who wish to produce online historical work, or to build upon and improve the projects they have already started in this important new medium. It begins with an overview of the different genres of history websites, surveying a range of digital history work that has been created since the beginning of the web. The book then takes the reader step-by-step through planning a project, understanding the technologies involved and how to choose the appropriate ones, designing a site that is both easy-to-use and scholarly, digitizing materials in a way that makes them web-friendly while preserving their historical integrity, and how to reach and respond to an intended audience effectively. It also explores the repercussions of copyright law and fair use for scholars in a digital age, and examines more cutting-edge web techniques involving interactivity, such as sites that use the medium to solicit and collect historical artifacts. Finally, the book provides basic guidance on insuring that the digital history the reader creates will not disappear in a few years.},
	publisher = {University of Pennsylvania Press},
	author = {Cohen, Daniel J. and Rosenzweig, Roy},
	urldate = {2009-11-19},
	date = {2005},
	langid = {english},
	keywords = {act\_Archiving, act\_Publishing, goal\_Capture, meta\_GiveOverview, obj\_Documents},
}

@article{hasse_digitale_2012,
	title = {Digitale Revolution der Wissenschaft},
	url = {http://www.welt.de/print/die_welt/hamburg/article108267495/Digitale-Revolution-der-Wissenschaft.html},
	abstract = {Weltweit größter Fachkongress befasst sich in Hamburg mit der Nutzung computergestützter Verfahren},
	journaltitle = {Welt Online},
	author = {Hasse, Edgar S.},
	urldate = {2012-07-14},
	date = {2012-12-07},
	langid = {german},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@online{kuczera_digitale_2014,
	title = {Digitale Perspektiven mediävistischer Quellenrecherche},
	url = {http://mittelalter.hypotheses.org/3492},
	titleaddon = {Mittelalter},
	author = {Kuczera, Andreas},
	urldate = {2014-05-08},
	date = {2014-04-18},
	langid = {german},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@book{schmale_digitale_2010,
	location = {Wien},
	title = {Digitale Geschichtswissenschaft},
	isbn = {978-3-205-78553-8},
	url = {http://www.boehlau-verlag.com/978-3-205-78553-8.html},
	abstract = {Digitale Geschichtswissenschaft ist mehr als nur der Einsatz digitaler Techniken und Medien, sie ist mehr als {EDV}, Web oder Internet. Sie eröffnet neue Forschungs- und Darstellungsmöglichkeiten, sie antwortet auf die liquiden Eigenschaften unserer gegenwärtigen Netzwerkzivilisation. Digitale Geschichtswissenschaft ersetzt dabei nicht die traditionelle monographische Geschichtswissenschaft, sondern verhilft dieser zu einer Renaissance im Feld der großen historischen Erzählung, die nicht obsolet ist. Digitale Geschichtswissenschaft gewinnt ihr Profil aus den vielfältigen Möglichkeiten der semantischen Techniken und der Forschungsvernetzung, sie stellt auch dem Nichthistoriker Techniken des Geschichtelernens von hoher Qualität zur Verfügung. Geschichte passiert auch im Netz - Geschichtswissenschaft wird wieder spannend.},
	pagetotal = {149},
	publisher = {Böhlau},
	author = {Schmale, Wolfgang},
	date = {2010},
	langid = {german},
	keywords = {X-{CHECK}, meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{thiel_digitale_2013,
	location = {Frankfurt},
	edition = {online},
	title = {Digitale Geschichtswissenschaft. Mittel auf der Suche nach einem Zweck},
	url = {http://www.faz.net/aktuell/feuilleton/forschung-und-lehre/digitale-geschichtswissenschaft-mittel-auf-der-suche-nach-einem-zweck-12059672.html},
	abstract = {Überall gewinnt man heute aus großen Datensätzen Muster menschlichen Verhaltens. Lässt sich diese Methode auch auf die Vergangenheit anwenden? Historiker bewegen sich langsam darauf zu.},
	journaltitle = {Frankfurter Allgemeine Zeitung ({FAZ})},
	author = {Thiel, Tomas},
	date = {2013-02-11},
	langid = {german},
	keywords = {goal\_Analysis, meta\_Assessing, obj\_Documents},
}

@online{puschmann_digitale_2009,
	title = {Digitale Geisteswissenschaften in Deutschland? {\textbar} wisspub.net},
	url = {http://wisspub.net/2009/04/08/digitale-geisteswissenschaften-in-deutschland/},
	titleaddon = {wisspub.net},
	author = {Puschmann, Cornelius},
	urldate = {2011-11-17},
	date = {2009-04-08},
	langid = {german},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@book{sahle_digitale_2013,
	location = {Norderstedt},
	title = {Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. Teil 2: Befunde, Theorie und Methodik.},
	volume = {8},
	isbn = {978-3-8482-5252-7},
	url = {http://kups.ub.uni-koeln.de/5352/},
	shorttitle = {Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. Teil 2},
	abstract = {Die neuen Technologien und Medien sind auch für die wissenschaftliche Edition eine Herausforderung. Die Entwicklung unterschiedlicher digitaler Ausgabeformen in den letzten Jahren kann zunächst als evolutionäre Abfolge verschiedener technischer Paradigmen beschrieben werden, die jeweils auch inhaltliche und methodische Konsequenzen hatten. Auf dieser Grundlage lassen sich Bausteine für eine neue verallgemeinernde Theorie der - nun digitalen - Edition umreißen, die ihren Kern u.a. im Konzept der Transmedialisierung findet. Damit ist nach der spezifischen Formung von Methode und Praxis in analogen und digitalen Medien eine Neufassung der Zielstellungen der Edition auf einer eher medienneutralen, konzeptionellen Ebene zu erreichen. Die veränderten Bedingungen und unsere zunehmend digitale Umwelt führen dazu, dass fast alle Bereiche der Edition einem Wandel unterworfen werden. Auch einige dieser Aspekte werden in diesem Band genauer beleuchtet.},
	publisher = {{BoD}},
	author = {Sahle, Patrick},
	urldate = {2014-02-09},
	date = {2013},
	langid = {german},
}

@book{kohle_digitale_2013,
	location = {Glückstadt},
	title = {Digitale Bildwissenschaft},
	rights = {info:eu-repo/semantics/{openAccess}},
	url = {http://archiv.ub.uni-heidelberg.de/artdok/2185/},
	abstract = {Das Digitale hat seinen Einzug in die Kulturwissenschaften gehalten. Aber was kann es zur Deutung von Bildern, gar von Kunstwerken beitragen? 
Eine Antwort darauf versucht das Buch „Digitale Bildwissenschaft“. Sein Autor behauptet nicht zuallererst, dass man mit dem Computer viel schneller wissenschaftliche Fragen abhandeln kann, sondern dass sich vor allem Methodiken des Analysierens, Publizierens und Bewertens ändern. 
Für klassische Geisteswissenschaftler sind solche Wandlungen nicht leicht zu verdauen. Wird eine digitale Bildgeschichte überhaupt noch Geschichtsschreibung sein?},
	publisher = {Hülsbusch},
	author = {Kohle, Hubertus},
	urldate = {2014-05-07},
	date = {2013},
	langid = {german},
	keywords = {meta\_GiveOverview},
}

@book{bailey_digital_2010,
	title = {Digital Curation and Preservation Bibliography 2010},
	url = {http://www.digital-scholarship.org/dcpb/dcpb2010.htm},
	abstract = {n a rapidly changing technological environment, the difficult task of ensuring long-term access to digital information is increasingly important. The Digital Curation and Preservation Bibliography 2010 presents over 500 English-language articles, books, and technical reports that are useful in understanding digital curation and preservation. This selective bibliography covers digital curation and preservation copyright issues, digital formats (e.g., data, media, and e-journals), metadata, models and policies, national and international efforts, projects and institutional implementations, research studies, services, strategies, and digital repository concerns. Most sources have been published from 2000 through 2010; however, a limited number of key sources published prior to 2000 are also included. The bibliography includes links to freely available versions of included works, such as e-prints and open access articles.},
	author = {Bailey, Charles W.},
	date = {2010},
	langid = {english},
	keywords = {act\_Organizing, act\_Publishing, goal\_Storage, meta\_GiveOverview},
}

@article{landes_schriften_2011,
	title = {Die Schriften der anderen. Rezensionskultur im Umbruch},
	volume = {62},
	pages = {669--672},
	number = {11},
	journaltitle = {Geschichte in Wissenschaft und Unterricht},
	author = {Landes, Lilian},
	date = {2011},
	langid = {german},
	note = {Dossier: "Internetressourcen zur Geschichte"},
}

@inproceedings{sperberg-mcqueen_hochzeit_2001,
	title = {Die Hochzeit der Philologie und des Merkur. Philologische Datenverarbeitung},
	url = {http://opac.regesta-imperii.de/lang_de/anzeige.php?buchbeitrag=Die+Hochzeit+der+Philologie+und+des+Merkur%3A+Philologische+Datenverarbeitung&pk=1108098},
	eventtitle = {Maschinelle Verarbeitung altdeutscher Texte. Beiträge zum Fünften Internationalen Symposium, Würzburg 4. bis 6. März 1997},
	pages = {3--22},
	booktitle = {Maschinelle Verarbeitung altdeutscher Texte. Beiträge zum Fünften Internationalen Symposium, Würzburg 4. bis 6. März 1997},
	author = {Sperberg-{McQueen}, Michael},
	date = {2001},
	langid = {german},
	note = {Sperberg-{McQueen}, Michael: Die Hochzeit der Philologie und des Merkur. Philologische Datenverarbeitung. In: Moser u.a. 2001, S. 3-22.},
	keywords = {goal\_Enrichment, t\_Encoding},
}

@book{piatti_geographie_2008,
	location = {Göttingen},
	title = {Die Geographie der Literatur : Schauplätze, Handlungsräume, Raumphantasien},
	isbn = {9783835303294},
	url = {http://www.wallstein-verlag.de/9783835303294-barbara-piatti-die-geographie-der-literatur.html},
	shorttitle = {Die Geographie der Literatur},
	abstract = {Konzepte einer künftigen Literaturgeographie, die literarische Schauplätze zum Ausgangspunkt der Textanalysen macht.

Wo spielt Literatur? Die vermeintlich simple Frage eröffnet ein erst in Ansätzen etabliertes Forschungsgebiet mit neuen methodischen Zugängen unter dem Stichwort »Literaturgeographie«.
Jede literarische Handlung ist irgendwo lokalisiert, wobei die Skala von gänzlich imaginären bis zu realistisch gezeichneten Schauplätzen mit hohem Wiedererkennungswert reicht. Die Literaturgeographie rückt die vielfältigen Bezugnahmen von Räumen der Fiktion auf den Realraum hin ins Zentrum der Aufmerksamkeit: Literatur weist eine spezifische Geographie auf, die ganz eigenen Regeln folgt. Denn fiktionale Räume sind niemals nur mimetische Abbilder der Realität, auch wenn sie sich auf existierende Landschaften und Städte beziehen. Vielmehr müssen die poetologischen Verfahren von Verfremdung, Überblendung, Neubenennung, die Kombinationsmöglichkeiten von realen Orten mit fiktiven Elementen in Visualisierungskonzepte und Deutungen der Textanalysen einfließen.
Diese Theorie findet zunächst Anwendung auf eine an literarischen Schauplätzen überreiche Modellregion: auf den Vierwaldstättersee und das Gotthardmassiv in der Zentralschweiz. Im Anschluss wird der methodische Horizont für einen Literaturatlas aufgespannt - und das Potenzial literatur­geographischer Konzepte im Hinblick auf eine vergleichende europäische Literaturgeschichte aufgezeigt.
17 beigefügte vierfarbige Faltkarten ermöglichen die differenzierte Gegenüberstellung von fiktionalen und realen Landschaften.},
	publisher = {Wallstein},
	author = {Piatti, Barbara},
	date = {2008},
	langid = {german},
	keywords = {act\_Visualizing, obj\_Literature},
}
@article{muhlberger_digitale_1999,
	title = {Die digitale Wissenschaft. Einige Bemerkungen über das Verhältnis von Internet und Geisteswissenschaft},
	url = {http://www.literature.at/viewer.alo?viewmode=overview&objid=21609&setLang=de},
	pages = {135--158},
	journaltitle = {Sprachkunst 30/1 (1999)},
	author = {Mühlberger, Günter},
	date = {1999},
	langid = {german},
	note = {Mühlberger, Günter: Die digitale Wissenschaft. Einige Bemerkungen über das Verhältnis von Internet und Geisteswissenschaft. In: Sprachkunst 30/1 (1999), S. 135-158.},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@article{herkommer_computergestutzte_2012,
	title = {Die computergestützte qualitative Inhaltsanalyse. Eine Möglichkeit zur Erweiterung des Methodenkanons der (zeit-)historischen Forschung},
	url = {http://universaar.uni-saarland.de/journals/index.php/zdg/article/view/295},
	abstract = {From its starting point as a scientific discipline, historiographical research has been primarily characterized through hermeneutic and source-critical analysis of texts. In the context of these analyses it is mainly shown, who speaks as well as what is said and why it is said. However, often the criteria for the selection of the analyzed sources as well as the exact categories on which the analysis is based remain unclear and are not further speci-fied.
Additionally and most notably in the field of contemporary historiographical research, research sources changed qualitatively and quantitatively. On the one hand and due to a changing understanding of historical research new forms of research sources like, for example, witness reports and biographical interviews need to be examined. On the other hand technical innovation like a digitalized news-coverage as well as audio, video and online data sources produce large amounts of widely distributed memory artifacts which call for new forms of research analyses. It therefore seems that additional methods for selecting and evaluating historiographical data are necessary.
One possible addition to current historiographical research methods could be, for example, the computer-assisted qualitative content analysis. Until now this method is mainly used in the field of social sciences. However it could be advantageous for historiographical research as well as it provides transparency in the analysis of large data sets and reduces complexity without leaving out qualitative and hermeneutic aspects of the analysis.},
	number = {1},
	journaltitle = {Zeitschrift für Digitale Geschichtswissenschaft},
	author = {Herkommer, Christina},
	date = {2012},
	langid = {german},
	keywords = {{AnalyzeQualitatively}, meta\_GiveOverview},
}

@article{thisted_did_1987,
	title = {Did Shakespeare Write a Newly-Discovered Poem?},
	volume = {74},
	issn = {0006-3444},
	url = {http://www.jstor.org/stable/2336684},
	doi = {10.2307/2336684},
	abstract = {The consistency of the word usage in a previously unknown nine-stanza poem attributed to Shakespeare with that of the Shakespearean canon is examined using a nonparametric empirical Bayes model. We consider also poems by Jonson, Marlowe and Donne, as well as four poems definitely attributed to Shakespeare. On balance, the poem is found to fit previous Shakespearean usage reasonably well.},
	pages = {445--455},
	number = {3},
	journaltitle = {Biometrika},
	author = {Thisted, Ronald and Efron, Bradley},
	urldate = {2011-12-14},
	date = {1987},
	langid = {english},
	note = {{ArticleType}: research-article / Full publication date: Sep., 1987 / Copyright © 1987 Biometrika Trust},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@article{mahlberg_dickens_2012,
	title = {Dickens, the suspended quotation and the corpus},
	volume = {21},
	issn = {0963-9470, 1461-7293},
	url = {http://lal.sagepub.com/cgi/doi/10.1177/0963947011432058},
	doi = {10.1177/0963947011432058},
	abstract = {This article presents a computer-assisted approach to the study of character discourse in Dickens. It focuses on the concept of the ‘suspended quotation’ – the interruption of a character’s speech by at least five words of narrator text. After an outline of the concept of the suspended quotation as introduced by Lambert (1981), the article compares manually derived counts for suspensions in Dickens with automatically generated figures. This comparison shows how corpus methods can help to increase the scale at which the phenomenon is studied. It highlights that quantitative information for selected sections of a novel does not necessarily represent the patterns that are found across the whole text. The article also includes a qualitative analysis of suspensions. With the help of the new tool {CLiC}, it investigates interruptions of the speech of Mrs Sparsit in Hard Times and illustrates how suspensions can be useful places for the presentation of character information. {CLiC} is further used to find patterns of the word pause that provide insights into how suspensions contribute to the representation of pauses in character speech.},
	pages = {51--65},
	number = {1},
	journaltitle = {Language and Literature},
	author = {Mahlberg, M. and Smith, C.},
	urldate = {2013-07-03},
	date = {2012-04-09},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@article{collins_detecting_2004,
	title = {Detecting Collaborations in Text Comparing the Authors' Rhetorical Language Choices in The Federalist Papers},
	volume = {38},
	issn = {0010-4817},
	url = {http://www.springerlink.com/content/v82jqux520153836/},
	doi = {10.1023/B:CHUM.0000009291.06947.52},
	abstract = {In author attribution studies function words or lexical measures areoften used to differentiate the authors' textual fingerprints. Thesestudies can be thought of as quantifying the texts, representing thetext with measured variables that stand for specific textual features.The resulting quantifications, while proven useful for statisticallydifferentiating among the texts, bear no resemblance to the understanding a human reader – even an astute one – would develop whilereading the texts. In this paper we present an attribution study that,instead, characterizes the texts according to the representationallanguage choices of the authors, similar to a way we believe close humanreaders come to know a text and distinguish its rhetorical purpose. Fromour automated quantification of The Federalist papers, it isclear why human readers find it impossible to distinguish the authorshipof the disputed papers. Our findings suggest that changes occur in theprocesses of rhetorical invention when undertaken in collaborativesituations. This points to a need to re-evaluate the premise ofautonomous authorship that has informed attribution studies of The Federalist case},
	pages = {15--36},
	number = {1},
	journaltitle = {Computers and the Humanities},
	author = {Collins, Jeff and Kaufer, David and Vlachos, Pantelis and Butler, Brian and Ishizaki, Suguru},
	urldate = {2011-12-08},
	date = {2004-02},
	langid = {english},
	keywords = {act\_StylisticAnalysis, t\_Stylometry},
}

@article{pearl_detecting_2012,
	title = {Detecting authorship deception: a supervised machine learning approach using author writeprints},
	volume = {27},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/cgi/doi/10.1093/llc/fqs003},
	doi = {10.1093/llc/fqs003},
	shorttitle = {Detecting authorship deception},
	abstract = {We describe a new supervised machine learning approach for detecting authorship deception, a specific type of authorship attribution task particularly relevant for cybercrime forensic investigations, and demonstrate its validity on two case studies drawn from realistic online data sets. The core of our approach involves identifying uncharacteristic behavior for an author, based on a writeprint extracted from unstructured text samples of the author's writing. The writeprints used here involve stylometric features and content features derived from topic models, an unsupervised approach for identifying relevant keywords that relate to the content areas of a document. One innovation of our approach is to transform the writeprint feature values into a representation that individually balances characteristic and uncharacteristic traits of an author, and we subsequently apply a Sparse Multinomial Logistic Regression classifier to this novel representation. Our method yields high accuracy for authorship deception detection on the two case studies, confirming its utility.},
	pages = {183--196},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Pearl, L. and Steyvers, M.},
	urldate = {2013-03-19},
	date = {2012-03-07},
	langid = {english},
	keywords = {t\_MachineLearning},
}

@article{ruecker_designing_2009,
	title = {Designing Data Mining Droplets: New Interface Objects for the Humanities Scholar},
	volume = {3},
	url = {http://digitalhumanities.org/dhq/vol/3/3/000067/000067.html},
	abstract = {In this paper, we describe the design of a number of alternative interface "droplets" that are intended for use by humanities scholars interested in applying data mining and information visualization tools to the task of hypothesis formulation. The trained droplets provide several functions. Their primary purpose is to encapsulate the results of the software training phase. They can be saved for future re-use against other collections or combinations of collections. They can be modified by having the user accept or reject features identified by the data mining software. Finally, they can also contain choices for how to display and organize items in the collection. The opportunity to develop a new interface object presents the designer with the challenge of effectively communicating what the tool is good for and how it is used. This paper outlines the design process we followed in creating the visual representations of these interface objects, describes the communicative strengths and weaknesses of a number of alternative designs, and discusses the importance of the study of new interface objects as the means of providing the user with new interface affordances},
	number = {3},
	journaltitle = {Digital Humanities Quarterly},
	author = {Ruecker, Stan and Radzikowska, Milena and Sinclair, Stéfan},
	urldate = {2011-09-23},
	date = {2009},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_Visualizing, bigdata{\textasciitilde}, meta\_GiveOverview},
}

@article{hecker_einsatz_1986,
	title = {Der Einsatz von Computern in den Geisteswissenschaften},
	pages = {180--189},
	journaltitle = {Forschungsmagazin der Universität Heidelberg 1986},
	author = {Hecker, Axel},
	date = {1986},
	langid = {german},
	note = {Hecker, Axel: Der Einsatz von Computern in den Geisteswissenschaften. In: Carola Ruperto (Hg.): Forschungsmagazin der Universität Heidelberg 1986, S. 180-189.},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{hoover_delta_2004,
	title = {Delta Prime?},
	volume = {19},
	url = {http://llc.oxfordjournals.org/content/19/4/477.abstract},
	doi = {10.1093/llc/19.4.477},
	shorttitle = {Delta Prime?},
	abstract = {John F. Burrows has proposed Delta, a simple new measure of textual difference, as a tool for authorship attribution, and has shown that it has great potential, especially in attribution problems where the possible authors are numerous and difficult to limit by traditional methods. In tests on prose, Delta has performed nearly as well as for Burrows's verse texts. A series of further tests using automated methods, however, shows that two modified methods of calculating Delta and three alternatives to or transformations of Delta produce results that are even more accurate. Four of these five new measures produce much better results than Delta both on a very diverse group of 104 novels and on a group of forty-four smaller contemporary literary critical texts. Although further testing is needed, Delta and its modifications should prove valuable and effective tools for authorship attribution.},
	pages = {477 --495},
	number = {4},
	journaltitle = {Literary and Linguistic Computing},
	author = {Hoover, David L.},
	urldate = {2011-07-26},
	date = {2004-11},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, t\_Stylometry},
}

@article{van_dalen-oskam_delta_2007,
	title = {Delta for Middle Dutch—Author and Copyist Distinction in Walewein},
	volume = {22},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/22/3/345},
	doi = {10.1093/llc/fqm012},
	abstract = {The Middle Dutch Arthurian romance Roman van Walewein (‘Romance of Gawain’) is attributed in the text itself to two authors, Penninc and Vostaert. Very little quantitative research into this dual authorship has been done. This article describes our progress in applying different non-traditional authorship attribution methods to the text of Walewein. After providing an introduction to the romance and an overview of earlier research, we evaluate previous statements on authorship and stylistics by applying both Yule's measure of lexical richness and Burrows's Delta. To find out whether these new methods would confirm or even enhance our present knowledge about the differences between the two authors, we applied an adapted version of John Burrows's Delta procedure. The adapted version seems to be able to distinguish the double authorship of the romance. It also helps us to confirm some and to reject other earlier statements about the position in the text where the second author started his work.},
	pages = {345--362},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {van Dalen-Oskam, Karina and van Zundert, Joris},
	urldate = {2012-02-21},
	date = {2007-09-01},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@article{burrows_delta:_2002,
	title = {‘Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship},
	volume = {17},
	url = {http://llc.oxfordjournals.org/content/17/3/267.abstract},
	doi = {10.1093/llc/17.3.267},
	shorttitle = {‘Delta’},
	abstract = {This paper is a companion to my ‘Questions of authorship: attribution and beyond’, in which I sketched a new way of using the relative frequencies of the very common words for comparing written texts and testing their likely authorship. The main emphasis of that paper was not on the new procedure but on the broader consequences of our increasing sophistication in making such comparisons and the increasing (although never absolute) reliability of our inferences about authorship. My present objects, accordingly, are to give a more complete account of the procedure itself; to report the outcome of an extensive set of trials; and to consider the strengths and limitations of the new procedure. The procedure offers a simple but comparatively accurate addition to our current methods of distinguishing the most likely author of texts exceeding about 1,500 words in length. It is of even greater value as a method of reducing the field of likely candidates for texts of as little as 100 words in length. Not unexpectedly, it works least well with texts of a genre uncharacteristic of their author and, in one case, with texts far separated in time across a long literary career. Its possible use for other classificatory tasks has not yet been investigated.},
	pages = {267 --287},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	author = {Burrows, John},
	urldate = {2011-07-26},
	date = {2002},
	langid = {english},
	keywords = {*****, X-{CHECK}, obj\_Methods, t\_Stylometry},
}

@article{samuels_deformance_1999,
	title = {Deformance and Interpretation},
	volume = {30},
	url = {http://www2.iath.virginia.edu/jjm2f/old/deform.html},
	abstract = {Works of imagination encourage interpreters, who respond in diverse and inventive ways. The variety of critical practices--indeed, the number of differing interpretations directed at the same works--can obscure the theoretical commonality that holds those practices together. We can draw an immediate distinction, however, between critical practices which do or do not aim to be interpretive: bibliographical studies and prosodic analysis, for example, typically discount their interpretive moves, if any are explicitly engaged.

The usual object of interpretation is "meaning," or some set of ideas that can be cast in thematic form. These meanings are sought in different ways: as though resident "in" the work, or evoked through "reader-response," or deconstructable through a process that would reinstall a structure of intelligibility at a higher, more critical level. The contemporary terminology will not obscure the long-standing character of such practices, which can be mixed in various ways. In all these cases, however, an essential relation is preserved between an artistic work and some structure of ideas, that is, some conceptual form that gets more or less fully articulated "for" the work. To understand a work of art, interpreters try to close with a structure of thought that represents its essential idea(s).

In this paper we want to propose--or recall--another way of engaging imaginative work. Perhaps as ancient as more normative practices, it has been less in vogue for some time. This alternative does not stand opposed to interpretive procedures as such, nor to the elaboration of conceptual equivalents for imaginative work. But it does try to set these modes of exegesis on a new footing. The alternative moves to break beyond conceptual analysis into the kinds of knowledge involved in performative operations--a practice of everyday imaginative life. We will argue that concept-based interpretation, reading along thematic lines, is itself best understood as a particular type of performative and rhetorical operation.},
	pages = {25--56},
	number = {1},
	journaltitle = {New Literary History},
	author = {Samuels, Lisa and {McGann}, Jerome},
	date = {1999},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, goal\_Interpretation, meta\_Theorizing},
}

@collection{terras_defining_2013,
	location = {Williston},
	title = {Defining Digital Humanities - A Reader},
	url = {http://www.ashgate.com/isbn/9781409469636},
	abstract = {Digital Humanities is becoming an increasingly popular focus of academic endeavour. There are now hundreds of Digital Humanities centres worldwide and the subject is taught at both postgraduate and undergraduate level. Yet the term ‘Digital Humanities’ is much debated. This reader brings together, for the first time, in one core volume the essential readings that have emerged in Digital Humanities. We provide a historical overview of how the term ‘Humanities Computing’ developed into the term ‘Digital Humanities’, and highlight core readings which explore the meaning, scope, and implementation of the field. To contextualize and frame each included reading, the editors and authors provide a commentary on the original piece. There is also an annotated bibliography of other material not included in the text to provide an essential list of reading in the discipline. This text will be required reading for scholars and students who want to discover the history of Digital Humanities through its core writings, and for those who wish to understand the many possibilities that exist when trying to define Digital Humanities.},
	publisher = {Ashgate},
	editor = {Terras, Melissa and Nyhan, Julianne and Vanhoutte, Edward},
	date = {2013},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@collection{gold_debates_2012,
	title = {Debates in the Digital Humanities},
	isbn = {0816677956},
	url = {http://dhdebates.gc.cuny.edu/},
	abstract = {Welcome to the open-access edition of Debates in the Digital Humanities, which brings together leading figures in the field to explore its theories, methods, and practices and to clarify its multiple possibilities and tensions.},
	pagetotal = {504},
	publisher = {Univ Of Minnesota Press},
	editor = {Gold, Matthew K.},
	date = {2012-01-21},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{golumbia_death_2014,
	title = {Death of a Discipline},
	volume = {25},
	issn = {1040-7391, 1527-1986},
	url = {http://differences.dukejournals.org/content/25/1/156},
	doi = {10.1215/10407391-2420033},
	abstract = {In 2003, Gayatri Chakravorty Spivak published Death of a Discipline, an exhortation to create “an inclusive comparative literature,” one that “takes the languages of the Southern Hemisphere as active cultural media rather than as objects of cultural study.” To many literary scholars such a development seemed welcome and even likely. Instead, ten years later, an entirely different transformation has taken place via the development of the digital humanities ({DH}), in which the close study of literature and the languages in which it is embedded have themselves been demoted in favor of “distant reading” and other forms of quantitative and large-scale analyses and whose language politics have regressed rather than progressed from the state Spivak described. {DH} advertises itself as an unexceptionable application of computational techniques to literary scholarship, yet its advent has accompanied an almost complete reorientation of literary studies as a field—a virtual death of the vision described by Spivak. The advent of {DH} is quite unlike the ones accompanying the introduction of computers into other disciplines, whose basic precepts have remained largely intact in the face of digitization. {DH}’s paradoxical use of the adjective “digital” to describe only a fraction of research methods that engage with digital technology creates a tension that must be resolved—either by the {DH} label being reabsorbed into literary studies or by literary research itself being fundamentally altered, a goal that {DH} has already in part achieved.},
	pages = {156--176},
	number = {1},
	journaltitle = {differences},
	shortjournal = {differences},
	author = {Golumbia, David},
	urldate = {2014-05-08},
	date = {2014-01-01},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{tilahun_dating_2013,
	title = {Dating medieval English charters},
	volume = {6},
	url = {http://arxiv.org/abs/1301.2405},
	doi = {10.1214/12-AOAS566},
	abstract = {Deeds, or charters, dealing with property rights, provide a continuous documentation which can be used by historians to study the evolution of social, economic and political changes. This study is concerned with charters (written in Latin) dating from the tenth through early fourteenth centuries in England. Of these, at least one million were left undated, largely due to administrative changes introduced by William the Conqueror in 1066. Correctly dating such charters is of vital importance in the study of English medieval history. This paper is concerned with computer-automated statistical methods for dating such document collections, with the goal of reducing the considerable efforts required to date them manually and of improving the accuracy of assigned dates. Proposed methods are based on such data as the variation over time of word and phrase usage, and on measures of distance between documents. The extensive (and dated) Documents of Early England Data Set ({DEEDS}) maintained at the University of Toronto was used for this purpose.},
	number = {4},
	journaltitle = {{arXiv}:1301.2405},
	author = {Tilahun, Gelila and Feuerverger, Andrey and Gervers, Michael},
	urldate = {2013-01-16},
	date = {2013-01-11},
	langid = {english},
	note = {Annals of Applied Statistics 2012, Vol. 6, No. 4, 1615-1640},
	keywords = {{AnalyzeStatistically}, bigdata, obj\_Manuscripts},
}

@book{witten_data_2011,
	location = {San Francisco},
	edition = {3rd edition},
	title = {Data Mining: Practical Machine Learning Tools and Techniques},
	url = {http://www.cs.waikato.ac.nz/ml/weka/book.html},
	abstract = {We have written a companion book for the Weka software, now into its third edition, that describes the machine learning techniques that it implements and how to use them. It is structured into three parts. The first part is an introduction to data mining using basic machine learning techniques, the second part describes more advanced machine learning methods, and the third part is a user guide for Weka.},
	publisher = {Morgan Kaufmann},
	author = {Witten, Ian and Frank, Eibe and Hall, Mark},
	date = {2011},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_MachineLearning},
}

@book{han_data_2011,
	location = {Burlington, {MA}},
	edition = {3rd ed},
	title = {Data mining: concepts and techniques},
	isbn = {9780123814791},
	url = {http://web.engr.illinois.edu/~hanj/bk2/toc.pdf},
	shorttitle = {Data mining},
	abstract = {The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of your data},
	pagetotal = {703},
	publisher = {Elsevier},
	author = {Han, Jiawei},
	editora = {Kamber, Micheline},
	editoratype = {collaborator},
	date = {2011},
	keywords = {bigdata, obj\_AnyObject, t\_MachineLearning},
}

@book{janert_data_2010,
	location = {Sebastopol, {CA}},
	title = {Data analysis with open source tools},
	isbn = {9780596802356  0596802358},
	url = {http://shop.oreilly.com/product/9780596802363.do},
	abstract = {Collecting data is relatively easy, but turning raw information into something useful requires that you know how to extract precisely what you need. With this insightful book, intermediate to experienced programmers interested in data analysis will learn techniques for working with data in a business environment. You'll learn how to look at data to discover what it contains, how to capture those ideas in conceptual models, and then feed your understanding back into the organization through business plans, metrics dashboards, and other applications.

Along the way, you'll experiment with concepts through hands-on workshops at the end of each chapter. Above all, you'll learn how to think about the results you want to achieve -- rather than rely on tools to think for you.

    Use graphics to describe data with one, two, or dozens of variables
    Develop conceptual models using back-of-the-envelope calculations, as well asscaling and probability arguments
    Mine data with computationally intensive methods such as simulation and clustering
    Make your conclusions understandable through reports, dashboards, and other metrics programs
    Understand financial calculations, including the time-value of money
    Use dimensionality reduction techniques or predictive analytics to conquer challenging data analysis situations
    Become familiar with different open source programming environments for data analysis},
	publisher = {O'Reilly},
	author = {Janert, Philipp K},
	date = {2010},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_Visualizing, obj\_AnyObject},
}

@article{rapp_projekt_2006,
	title = {Das Projekt {TextGrid}. Modulare Plattform für verteilte und kooperative wissenschaftliche Textdatenverarbeitung - ein Community-Grid für die Geisteswissenschaften. Chancen und Perspektiven für eine neue Wissenschaftskultur in den Geisteswissenschaften},
	url = {http://www.ahf-muenchen.de/Forschungsberichte/Jahrbuch2006/AHF_Jb2006_FB_B1_Rapp.pdf},
	pages = {61--68},
	journaltitle = {Jahrbuch der historischen Forschung in der Bundesrepublik Deutschland},
	author = {Rapp, Andrea},
	date = {2006},
	langid = {german},
	keywords = {act\_Collaborating, obj\_VREs},
}

@article{mirowski_digitale_2014,
	title = {Das Digitale denken ({III}) Die offene Wissenschaft und ihre Freunde},
	issn = {0174-4909},
	url = {http://www.faz.net/aktuell/feuilleton/geisteswissenschaften/das-digitale-denken-iii-die-offene-wissenschaft-und-ihre-freunde-12862246.html?printPagedArticle=true#pageIndex_2},
	abstract = {Open Access, Open Science, Science 2.0 - die Titel, unter denen vom Internet eine Beschleunigung des Erkenntnisgewinns erwartet wird, sind vielfältig. Doch wem nützen die entsprechenden Techniken? Hat die Forschung überhaupt Probleme, die sich so lösen lassen?},
	journaltitle = {{FAZ}.{NET}},
	author = {Mirowski, Philip},
	urldate = {2014-04-12},
	date = {2014-03-29},
	langid = {german},
}

@online{verhoeven_big_2014,
	title = {Big Data at the movies: the Kinomatics project},
	url = {http://theconversation.com/big-data-at-the-movies-the-kinomatics-project-29900},
	shorttitle = {Big Data at the movies},
	abstract = {The backlash against “big data” studies is well underway. And no more so than in the area of humanities and creative arts research. If I had a dollar for every person who has told me over the past year…},
	titleaddon = {The Conversation},
	author = {Verhoeven, Deb},
	urldate = {2014-09-09},
	date = {2014},
	langid = {english},
	keywords = {act\_StructuralAnalysis, goal\_Analysis, obj\_Video},
}

@book{forte_cyber-archaeology_2010,
	location = {Oxford, England},
	title = {Cyber-Archaeology},
	isbn = {1407307215},
	abstract = {The ontology of archaeological information, or the cybernetics of archaeology, refers to all the interconnective relationships which the datum produces, the code of transmission, and its transmittability. Because it depends on interrelationships, by its very nature information cannot be neutral with respect to how it is processed and perceived. It follows that the process of knowledge and communication have to be unified and represented by a single vector. 3D information is regarded as the core of the knowledge process, because it creates feedback, then cybernetic difference, among the interactor, the scientist and the ecosystem. It is argued that Virtual Reality (both offline and online) represents a possible ecosystem, which is able to host top-down and bottom-up processes of knowledge and communication. In these terms, the past is generated and coded by “a simulation process”. Thus, from the first phases of data acquisition in the field, the technical methodologies and technologies that we use, influence in a decisive way all the subsequent phases of interpretation and communication. Thus, the questions which we pose in a phase of bottom-up knowledge (for example, in an archaeological excavation) will influence the top-down phases of interpretation, or the mental patterns (for example, a comparative analysis and reconstruction of models). If we peremptorily separate knowledge and communication, we risk losing information along the way, reducing the relationships that are constructed between acquisition/input and transmission/output.

At the University of California, Merced, we are experimenting different forms of virtual embodiment and participatory learning. The innovation factor is mainly based on the principle of “enaction”: A principle employed by avatar-driven virtual (embodied) communities for perceiving and constructing information. This neuro-phenomenologic approach is based on the assumption that the cognitive activity is “embodied”, i. e. not separated from the body perception. In these terms it can be correctly identified and communicated only within its specific context. It seems evident that embodiment depends on the level of immersion in the cyberspace: if well implemented embodied communities should be able to learn and transmit more knowledge and in a shorter time than traditional text-based “chatting” communities. We distinguish different types of embodiment (A-B-C) according to levels and modalities of participatory learning and capacities of interaction; each embodiment level will focus on a different kind of interactive learning interface, content target and group of users.

Embodiment A: Virtual Participatory Museum. In this case the {VR} system is planned for interactive platforms (each one with a mono display) and a large {HD} stereo display serving for group display. The users/players interact in a common 3D space, the rest of the audience can watch by stereo vision. In this way it is possible to obtain a double participation: one active, between the user/players and one passive between a larger audience watching the environment in the large 3D screen. Target users are museum visitors.

Embodiment B: Simulation Participatory Environment. (Powerwall, http:// graphics.ucmerced.edu/ research.html). The primary purpose of this environment is to visualize datasets in a large and interactive high resolution display capable of achieving a very high degree of 3D immersion. Within the Powerwall environment it will be possible to integrate full-body 3D motion capture devices in order to implement novel interaction interfaces between immersed users, the environment, and avatars or autonomous characters simulated in the virtual environment. This environment will enable the team to develop new humanlike motion interfaces for collaborative virtual environments, in particular for achieving seamless human-computer interactions accessible to non computer experts for programming motions in the virtual environments. This will allow archeologists to precisely program the correct motions associated with the environment and will enable a novel concept of “Motion Heritage”. Target users are: archaeologists and scientific community.},
	pagetotal = {153},
	publisher = {Archaeopress},
	editora = {Forte, Maurizio},
	editoratype = {collaborator},
	date = {2010},
	langid = {english},
	keywords = {X-{CHECK}, meta\_GiveOverview, obj\_AnyObject, obj\_Artefacts},
}

@article{hand_culturomics:_2011,
	title = {Culturomics: Word play : Nature News},
	url = {http://www.nature.com/news/2011/110617/full/474436a.html},
	doi = {doi:10.1038/474436a},
	abstract = {By mining a database of the world's books, Erez Lieberman Aiden is attempting to automate much of humanities research. But is the field ready to be digitized?},
	pages = {436--440},
	number = {474},
	journaltitle = {Nature},
	author = {Hand, Eric},
	urldate = {2011-06-18},
	date = {2011-06-17},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, bigdata{\textasciitilde}, meta\_GiveOverview},
}
@article{morse-gagne_culturomics:_2011,
	title = {Culturomics: Statistical Traps Muddy the Data},
	volume = {332},
	url = {http://www.sciencemag.org/content/332/6025/35.2.short},
	doi = {10.1126/science.332.6025.35-b},
	shorttitle = {Culturomics},
	pages = {35},
	number = {6025},
	journaltitle = {Science},
	author = {Morse-Gagné, Elise E.},
	urldate = {2011-07-19},
	date = {2011-04-01},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata{\textasciitilde}},
}

@article{kestemont_cross-genre_2012,
	title = {Cross-Genre Authorship Verification Using Unmasking},
	volume = {93},
	issn = {0013-838X, 1744-4217},
	url = {http://www.tandfonline.com/doi/abs/10.1080/0013838X.2012.668793},
	doi = {10.1080/0013838X.2012.668793},
	abstract = {In this paper we will stress-test a recently proposed technique for computational authorship verification, ‘‘unmasking'', which has been well received in the literature. The technique envisages an experimental set-up commonly referred to as ‘‘authorship verification'', a task generally deemed more difficult than so-called ‘‘authorship attribution''. We will apply the technique to authorship verification across genres, an extremely complex text categorization problem that so far has remained unexplored. We focus on five representative contemporary English-language authors. For each of them, the corpus under scrutiny contains several texts in two genres (literary prose and theatre plays). Our research confirms that unmasking is an interesting technique for computational authorship verification, especially yielding reliable results within the genre of (larger) prose works in our corpus. Authorship verification, however, proves much more difficult in the theatrical part of the corpus.},
	pages = {340--356},
	number = {3},
	journaltitle = {English Studies},
	author = {Kestemont, Mike and Luyckx, Kim and Daelemans, Walter and Crombez, Thomas},
	urldate = {2012-11-25},
	date = {2012-05},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@article{berry_critical_2013,
	title = {Critical Digital Humanities},
	url = {http://stunlaw.blogspot.com/2013/01/critical-digital-humanities.html},
	series = {stunlaw-blog},
	abstract = {Critical Digital Humanities is an approach to the study and use of the digital which is attentive to questions of power, domination, myth and exploitation, what has been called the "The Dark Side of the Digital Humanities" (Chun 2013; Grusin 2013; Jagoda 2013; Raley 2013).},
	author = {Berry, David M.},
	date = {2013-11-01},
	langid = {english},
	keywords = {meta\_Assessing, meta\_Theorizing, obj\_DigitalHumanities},
}

@book{ho_corpus_2011,
	location = {London ; New York},
	title = {Corpus stylistics in principles and practice: a stylistic exploration of John Fowles' The Magus},
	isbn = {9780826426178},
	series = {Advances in stylistics},
	shorttitle = {Corpus stylistics in principles and practice},
	abstract = {n this book, Yufang Ho compares the text style difference between the two versions of John Fowles' The Magus, exemplifying the methodological principles and analytic practices of the corpus stylistic approach.

The Magus was first published in 1966 and was revised and republished by Fowles in 1977.  Fowles' own comment on the second edition was that it was 'rather more than a stylistic revision.' The book explores how the revised version is linguistically different from the original, especially in terms of point of view (re) representation.  The corpus stylistic approach adopted combines qualitative and quantitative comparison to confirm the overall text style difference.  The analysis demonstrates that computer assisted methods can identify significant linguistic features which literary critics have not noticed and provide a more detailed descriptive basis for literary interpretation of (either edition) of the novel.  This analysis of The Magus serves as a case study and exemplar of how corpus techniques may be used generally in the study of linguistics.},
	pagetotal = {254},
	publisher = {Continuum},
	author = {Ho, Yufang},
	date = {2011},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}},
}

@book{mahlberg_corpus_2012,
	location = {New York},
	edition = {1},
	title = {Corpus Stylistics and Dickens's Fiction},
	isbn = {0415800145},
	url = {http://www.routledge.com/books/details/9780415800143/},
	series = {Routledge advances in corpus linguistics},
	abstract = {This book presents an innovative approach to the language of one of the most popular English authors. It illustrates how corpus linguistic methods can be employed to study electronic versions of texts by Charles Dickens. With particular focus on Dickens’s novels, the book proposes a way into the Dickensian world that starts from linguistic patterns. The analysis begins with clusters, i.e. repeated sequences of words, as pointers to local textual functions. Combining quantitative findings with qualitative analyses, the book takes a fresh view on Dickens’s techniques of characterisation, the literary presentation of body language and speech in fiction. The approach brings together corpus linguistics, literary stylistics and Dickens criticism. It thus contributes to bridging the gap between linguistic and literary studies and will be a useful resource for both researchers and students of English language and literature.},
	pagetotal = {192},
	publisher = {Routledge},
	author = {Mahlberg, Michaela},
	date = {2012-07-01},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_StylisticAnalysis},
}

@book{fischer-starcke_corpus_2010,
	title = {Corpus Linguistics in Literary Analysis: Jane Austen and her Contemporaries},
	isbn = {1847064388},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20&path=ASIN/1847064388},
	shorttitle = {Corpus Linguistics in Literary Analysis},
	publisher = {Continuum},
	author = {Fischer-Starcke, Bettina},
	urldate = {2012-02-02},
	date = {2010-10-26},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, act\_StylisticAnalysis, obj\_Language, obj\_Literature},
}

@article{biber_corpus_2011,
	title = {Corpus linguistics and the study of literature: Back to the future?},
	volume = {1},
	doi = {10.1075/ssol.1.1.02bib},
	shorttitle = {Corpus linguistics and the study of literature},
	abstract = {The present paper introduces corpus-based analytical techniques and surveys some of the specific ways in which corpus analysis has been applied to the study of literature. In recent years, those research efforts have been mostly carried out under the umbrella of `corpus stylistics'. Most of these studies focus on the distribution of words (analyzing keywords, extended lexical phrases, or collocations) to identify textual features that are especially characteristic of an author or particular text. Corpus-based grammatical and pragmatic analyses of literary language are also briefly considered. Then, in the concluding part of the paper, I briefly survey earlier computational and statistical research on authorship attribution and literary style. While that research tradition is in some ways the precursor to more recent work in corpus stylistics, it is also complementary to recent research in its application of sophisticated statistical and computational methods.},
	pages = {15--23},
	number = {1},
	journaltitle = {Scientific Study of Literature},
	author = {Biber, Douglas},
	date = {2011},
	langid = {english},
	keywords = {{AnalyzeStatistically}, X-{CHECK}, act\_StylisticAnalysis, bigdata},
}

@article{suzuki_co-occurrence-based_2012,
	title = {Co-occurrence-based indicators for authorship analysis},
	volume = {27},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/cgi/doi/10.1093/llc/fqs011},
	doi = {10.1093/llc/fqs011},
	pages = {197--214},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Suzuki, T. and Kawamura, S. and Yoshikane, F. and Kageura, K. and Aizawa, A.},
	urldate = {2012-06-12},
	date = {2012-04-15},
	langid = {english},
}

@article{stubbs_conrad_2005,
	title = {Conrad in the computer: examples of quantitative stylistic methods},
	volume = {14},
	issn = {0963-9470},
	url = {http://lal.sagepub.com/cgi/doi/10.1177/0963947005048873},
	doi = {10.1177/0963947005048873},
	shorttitle = {Conrad in the computer},
	abstract = {A stylistic analysis of Joseph Conrad’s Heart of Darkness is used to illustrate the literary value of simple quantitative text and corpus data. Cultural and literary aspects of the book are briefly discussed. It is then shown that data on the frequencies and distributions of individual words and recurrent phraseology can not only provide a more detailed descriptive basis for widely accepted literary interpretations of the book, but also identify significant linguistic features which literary critics seem not to have noticed. The argument provides a response to scepticism of quantitative stylistics from both linguists and literary critics.},
	pages = {5--24},
	number = {1},
	journaltitle = {Language and Literature},
	shortjournal = {Language and Literature},
	author = {Stubbs, M.},
	urldate = {2011-04-26},
	date = {2005-02},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Theorizing, obj\_Literature},
}

@article{hitchcock_confronting_2013,
	title = {Confronting the Digital: Or How Academic History Writing Lost the Plot},
	volume = {10},
	doi = {10.2752/147800413X13515292098070},
	shorttitle = {Confronting the Digital},
	abstract = {This discussion piece argues that the design and structure of online historical resources and the process of search and discover embodied within them create a series of substantial problems for historians. Algorithm-driven discovery and misleading forms of search, poor {OCR}, and all the selection biases of a new edition of the Western print archive have changed how we research the past, and the underlying character of the object of study (inherited text). This piece argues that academic historians have largely failed to respond effectively to these challenges and suggests that while they have preserved the form of scholarly good practice, they have ignored important underlying principles.},
	pages = {9--23},
	number = {1},
	journaltitle = {Cultural and Social History},
	shortjournal = {Cultural and Social History},
	author = {Hitchcock, Tim},
	date = {2013-03-01},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@online{cover_conceptual_nodate,
	title = {Conceptual Modeling and Markup Languages},
	url = {http://xml.coverpages.org/conceptualModeling.html},
	abstract = {This document contains information relevant to 'Conceptual Modeling and Markup Languages' and is part of the Cover Pages resource. The Cover Pages is a comprehensive Web-accessible reference collection supporting the {SGML}/{XML} family of (meta) markup language standards and their application. The principal objective in this public access knowledgebase is to promote and enable the use of open, interoperable standards-based solutions which protect digital information and enhance the integrity of communication. Some of the standards covered include {SGML} (Standard Generalized Markup Language), {XML} (Extensible Markup Language), W3C {XML} Schema, Other {XML} Schema Languages ({RELAX} {NG}, Schematron, {DSDL}), {XSL} (Extensible Stylesheet Language), {XSLT} ({XSL} Transformations), {XPath} ({XML} Path Language), {XLink} ({XML} Linking), {XML} Query, {XHTML} (Extensible {HyperText} Markup Language), {DOM} (Document Object Model), {XPointer} ({XML} Pointer Language), {HyTime}, {RDF}, Topic Maps, {DSSSL}, {CSS} (Cascading Style Sheets), {SPDL}, {SVG} (Scalable Vector Graphics), {CGM}, {ISO}-{HTML}, etc. A secondary objective in The Cover Pages is to provide reference material on enabling technologies compatible with descriptive markup language standards and applications: object modeling, semantic nets, ontologies, authority lists, document production systems, and conceptual modeling. The reference collection contains over 9000 documents covering more than 700 topics on markup language technologies.},
	author = {Cover, Robin},
	urldate = {2011-11-01},
	langid = {english},
	keywords = {act\_Modeling, obj\_AnyObject},
}

@book{burton_computing_2002,
	location = {Urbana},
	title = {Computing in the Social Sciences and Humanities},
	isbn = {0252026853},
	url = {http://www.press.uillinois.edu/books/catalog/72exq4pd9780252026850.html},
	abstract = {A lively, hands-on introduction for teachers and scholars in the humanities and social sciences, this book-and-{CD} package will inspire even the faint-hearted to take the technological bull by the horns and make efficient, informed use of computer and Internet resources.

New technology is changing the very nature of research and teaching in the social sciences and humanities. From specialized online forums to Web-based teaching and distance learning, computers are being used to expand educational opportunities, promote cooperation and collaboration, stimulate creative thinking, and find answers to previously insoluble research problems. Combining interactive projects in a {CD}-{ROM} format with informative printed essays, this volume showcases innovations that are revolutionizing the craft of scholarship. More than that, it examines realistically how applicable the new technology is to learning. Contributors clarify some of the difficulties of using computers and address problems with the philosophy and culture of computers, including concerns about intellectual property protection and the potential for creating a technological underclass of electronically disadvantaged schools and universities.

The accompanying {CD} features multimedia entries such as an interactive project on owls that educates users about forest ecology; {RiverWeb}, an interactive archive of information on the history, culture, and science of the Mississippi River; and "Global Jukebox," which recreates the context in which the folklorist Alan Lomax made his pioneering field recordings. The {CD} includes links to many external sites on the World Wide Web. For those with limited Internet access, a collection of relevant sites is integrated into the {CD}. Minimum System Requirements 32 {MB} available {RAM}, {CD}-{ROM} drive (4x) Some articles and programs also require 256-color monitor and sound capability Macintosh®: System 8.5 Windows®: Intel Pentium® processor (or equivalent) running Windows® 95 or Windows {NT}® 4.0

Orville Vernon Burton is a professor of history and sociology at the University of Illinois at Urbana-Champaign and professor and senior research scientist at the National Center for Supercomputing Applications at the University of Illinois. He is the author of In My Father's House Are Many Mansions: Family and Community in Edgefield, South Carolina.},
	publisher = {University of Illinois Press},
	editora = {Burton, Orville Vernon},
	editoratype = {collaborator},
	date = {2002},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@book{kuckartz_computer_1988,
	location = {Frankfurt am Main u.a.},
	title = {Computer und verbale Daten: Chancen zur Innovation sozialwissenschaftlicher Forschungstechnik},
	volume = {173},
	isbn = {3-631-40769-6},
	url = {http://www.peterlang.de/download/datasheet/25284/datasheet_40769.pdf},
	series = {Europäische Hochschulschriften},
	abstract = {Qualitative Forschungsmethoden haben im letzten Jahrzehnt zunehmend an Attraktivität gewonnen; vor allem im Interview kommen bei
der Datenerhebung verstärkt offene Vorgehensweisen zum Einsatz. Die Techniken zur Auswertung verbaler Daten haben sich jedoch nicht
in gleichem Masse entwickelt; nach wie vor existiert kein Methodenkanon, der allgemeine Anerkennung findet. Der große Umfang und die
mangelnde Strukturiertheit sind die Charakteristika dieser Datenart. Kann der Computer hier effektiv eingesetzt werden? In dieser Arbeit werden
Methoden und Konzepte zur computergestützten Auswertung entwickelt, die auf methodischen und theoretischen Überlegungen im Rahmen der
verstehenden Soziologie in der Tradition Max Webers basieren. Die Verbindung von qualitativen und quantitativen Analysemethoden und ein
kontrolliertes Verfahren der Typenbildung stehen dabei im Mittelpunkt der Darstellung},
	pagetotal = {257},
	number = {22, Soziologie},
	publisher = {Lang},
	author = {Kuckartz, Udo},
	date = {1988},
	langid = {german},
	note = {Berlin, Techn. Univ., Diss., 1988},
	keywords = {{AnalyzeStatistically}, obj\_Methods},
}

@article{wisbey_computer_1991,
	title = {Computer und Philologie in Vergangenheit, Gegenwart und Zukunft},
	pages = {346--361},
	journaltitle = {Gärtner/Sappler/Trauth 1991},
	author = {Wisbey, Roy},
	date = {1991},
	langid = {german},
	note = {Wisbey, Roy: Computer und Philologie in Vergangenheit, Gegenwart und Zukunft. In: Gärtner/Sappler/Trauth 1991, S. 346-361.},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@incollection{culpeper_computers_2002,
	title = {Computers, language and characterisation: An Analysis of six characters in Romeo and Juliet},
	url = {http://www.lexically.net/wordsmith/corpus_linguistics_links/Keywords-Culpeper.pdf},
	pages = {11--30},
	booktitle = {Conversation in Life and in Literature: Papers from the {ASLA} Symposium, Association Suedoise de Linguistique Appliquee ({ASLA}), 15. Universitetstryckeriet: Uppsala},
	author = {Culpeper, Jonathan},
	editor = {Melander- Marttala, U. and Ostman, C. and Kyto, Merja},
	date = {2002},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}},
}

@article{herold_computerphilologie_2000,
	title = {Computerphilologie als Methode für die historisch-quellenkundliche Erschließung spätmittelalterlicher Briefe},
	pages = {129--148},
	journaltitle = {Hardmeier 2000},
	author = {Herold, Jürgen},
	date = {2000},
	langid = {german},
	note = {Herold, Jürgen: Computerphilologie als Methode für die historisch-quellenkundliche Erschließung spätmittelalterlicher Briefe. In: Hardmeier 2000, S. 129-148.},
	keywords = {goal\_Capture, goal\_Enrichment, meta\_GiveOverview, obj\_Letters},
}

@article{jannidis_computerphilologie_1998,
	title = {Computerphilologie},
	pages = {70--72},
	journaltitle = {Metzler Lexikon Literatur- und Kulturtheorie},
	author = {Jannidis, Fotis},
	date = {1998},
	langid = {german},
	note = {Jannidis, Fotis: Computerphilologie. In: Ansgar Nünning (Hg.): Metzler Lexikon Literatur- und Kulturtheorie. Stuttgart, Weimar 1998, S. 70-72.},
	keywords = {goal\_Analysis, goal\_Enrichment, meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{haller_computerlinguistik_2000,
	title = {Computerlinguistik und Computerphilologie: Kann der "einfältige Leser" dazulernen?},
	pages = {69--79},
	journaltitle = {Hardmeier 2000},
	author = {Haller, Johann},
	date = {2000},
	langid = {german},
	note = {Haller, Johann: Computerlinguistik und Computerphilologie: Kann der "einfältige Leser" dazulernen? In: Hardmeier 2000, S. 69-79.},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{schwanke_computer_1993,
	title = {Computer in der Germanistik},
	pages = {53--70},
	journaltitle = {Zeitschrift für Literaturwissenschaft und Linguistik 23 (1993)},
	author = {Schwanke, Martina},
	date = {1993},
	langid = {german},
	note = {Schwanke, Martina: Computer in der Germanistik. In: Zeitschrift für Literaturwissenschaft und Linguistik 23 (1993), S. 53-70.},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@collection{thaller_computer_1989,
	title = {Computer in den Geisteswissenschaften. Konzepte und Berichte},
	isbn = {978-3593338811},
	pagetotal = {336},
	publisher = {Frankfurt u.a.: Campus 1989},
	editor = {Thaller, Manfred},
	date = {1989},
	langid = {german},
	note = {Thaller, Manfred (Hg.): Computer in den Geisteswissenschaften. Konzepte und Berichte. Frankfurt u.a.: Campus 1989.},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@article{rauch_computer_1989,
	title = {Computer in den Geisteswissenschaften. Der Beitrag der Informationswissenschaft},
	pages = {93--100},
	journaltitle = {Schwob/Kranich-Hofbauer/Suntinger 1989},
	author = {Rauch, Wolf},
	date = {1989},
	langid = {german},
	note = {Rauch, Wolf: Computer in den Geisteswissenschaften. Der Beitrag der Informationswissenschaft. In: Schwob/Kranich-Hofbauer/Suntinger 1989, S. 93-100.},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@article{kornstadt_computergestutze_2005,
	title = {Computergestütze Suchverfahren in der Musikwissenschaft},
	url = {https://www.vifamusik.de/literatur/internetressourcen.html?tx_bsbsearch_pi1[navigation]=%2Bstaxclasss%3A^%22Nachbarfelder%20der%20Musikwissenschaft%2FMathematik%2C%20Informatik%22&tx_bsbsearch_pi1[offset]=80&tx_bsbsearch_pi1[id]=als87021vifamusik},
	abstract = {Gegensatz zu den recht etablierten computergestützten Suchverfahren in Texten sind solche Suchverfahren in der Musikwissenschaft noch weit von einer Standardisierung entfernt. Dies lässt sich darauf zurückführen, dass hinsichtlich dessen wonach gesucht wird, worin gesucht wird und hinsichtlich der elektronischen Repräsentation musikalischer Daten erhebliche Unterschiede zu herkömmlichen Texten bestehen. Im Folgenden soll aufgezeigt werden, worin diese Unterschiede bestehen und welche Perspektiven zu erkennen sind."},
	pages = {27--52},
	journaltitle = {Jahrbuch für Computerphilologie 7 (2005)},
	author = {Kornstädt, Andreas},
	date = {2005},
	langid = {german},
	note = {Kornstädt, Andreas: Computergestütze Suchverfahren in der Musikwissenschaft. In: Jahrbuch für Computerphilologie 7 (2005), S. 27-52.},
	keywords = {act\_Query/Retrieve, meta\_Theorizing, obj\_Music},
}

@article{sinclair_computerassisted_2003,
	title = {Computer‐Assisted Reading: Reconceiving Text Analysis},
	volume = {18},
	url = {http://llc.oxfordjournals.org/content/18/2/175.abstract},
	doi = {10.1093/llc/18.2.175},
	shorttitle = {Computer‐Assisted Reading},
	abstract = {The strengths of current text‐analysis tools lie in their ability to perform a variety of formal, enumerative, or statistical functions. These functions concord well with scientific perspectives of textual criticism. Much less evident is how current text‐analysis tools help read and experience literature. Design of new tools, it is argued, should give full space to how literary critics interact with texts, rather than simply focus on what computers can do well. Principles of reading, synthesis, and play are explored in relation to a prototype version of {HyperPo}: Text Analysis and Exploration Tools.},
	pages = {175 --184},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Sinclair, Stéfan},
	urldate = {2011-08-26},
	date = {2003-06},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata, obj\_Tools},
}

@article{shamir_computer_2012,
	title = {Computer Analysis Reveals Similarities between the Artistic Styles of Van Gogh and Pollock},
	volume = {45},
	issn = {0024-094X, 1530-9282},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/LEON_a_00281},
	doi = {10.1162/LEON_a_00281},
	abstract = {Recent advances in computer vision and image processing have enabled basic automatic analysis of visual art. The author uses computer analysis to extract thousands of low-level numerical image content descriptors from digitized paintings, which are then used to compare objective levels of similarity between the artistic styles of different painters. The analysis reveals that Vincent Van Gogh's and Jackson Pollock's artistic styles are far more similar to each other in terms of low-level image features than Pollock's work is to that of other painters'. This report also proposes that this methodology is useful in quantifying similarities between painters or artistic styles based on large sets of numerical image content descriptors and for detecting influential links not easily detected by the unaided eye.},
	pages = {149--154},
	number = {2},
	journaltitle = {Leonardo},
	author = {Shamir, Lior},
	urldate = {2012-07-10},
	date = {2012-04},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_RelationalAnalysis, bigdata, obj\_Images},
}

@book{burrows_computation_1987,
	location = {Oxford [Oxfordshire]  ;New York},
	title = {Computation into criticism : a study of Jane Austen's novels and an experiment in method},
	isbn = {9780198128564},
	url = {http://www.jstor.org/discover/10.2307/27710118?uid=3737864&uid=2&uid=4&sid=21104662946353},
	shorttitle = {Computation into criticism},
	pagetotal = {111-114},
	publisher = {Clarendon Press ;;Oxford University Press},
	author = {Burrows, John},
	date = {1987},
	langid = {english},
	keywords = {t\_Stylometry},
}
@article{suzuki_computational_2014,
	title = {Computational Stylistic Analysis of Popular Songs of Japanese Female Singer-songwriters},
	volume = {8},
	url = {http://www.digitalhumanities.org/dhq/vol/8/1/000170/000170.html},
	abstract = {This study analyzes popular songs composed by Japanese female singer-songwriters. Popular songs are a good representation of modern culture and society. Songs by female singer-songwriters account for a large portion of the current Japanese hit charts and particularly play an important role in understanding the Japanese language and communication style. In this study, we applied new methods of computational stylistics to the lyrics of the songs. The results clearly show differences in the characteristics of 10 female singer-songwriters, and we found that the "visualization of the lyrics" is a typical characteristic of current singer-songwriters. Our findings provide an important case study for computational stylistics and can also be useful for understanding Japanese cultural trends.},
	number = {1},
	journaltitle = {Digital Humanities Quarterly},
	author = {Suzuki, Takafumi and Hosoya, Mai},
	urldate = {2014-04-18},
	date = {2014},
	keywords = {act\_RelationalAnalysis, goal\_Analysis, obj\_Text, t\_PCA},
}

@article{meister_computational_2007,
	title = {Computational Narratology oder: Kann man das Erzählen berechenbar machen?},
	journaltitle = {Mediale Ordnungen. Erzählen, Archivieren, Beschreiben},
	author = {Meister, Jan Christoph},
	date = {2007},
	langid = {german},
	note = {Meister, Jan Christoph: Computational Narratology oder: Kann man das Erzählen berechenbar machen? In: Corinna Müller/Irina Scheidgen (Hg.): Mediale Ordnungen. Erzählen, Archivieren, Beschreiben. Marburg: Schüren Verlag 2007, 19-39.},
	keywords = {{AnalyzeQualitatively}, t\_Narratology},
}

@article{mani_computational_2012,
	title = {Computational Modeling of Narrative},
	volume = {5},
	issn = {1947-4040, 1947-4059},
	url = {http://www.morganclaypool.com/doi/abs/10.2200/S00459ED1V01Y201212HLT018},
	doi = {10.2200/S00459ED1V01Y201212HLT018},
	abstract = {The field of narrative (or story) understanding and generation is one of the oldest in natural language processing ({NLP}) and artificial intelligence ({AI}), which is hardly surprising, since storytelling is such a fundamental and familiar intellectual and social activity. In recent years, the demands of interactive entertainment and interest in the creation of engaging narratives with life-like characters have provided a fresh impetus to this field. This book provides an overview of the principal problems, approaches, and challenges faced today in modeling the narrative structure of stories. The book introduces classical narratological concepts from literary theory and their mapping to computational approaches. It demonstrates how research in {AI} and {NLP} has modeled character goals, causality, and time using formalisms from planning, case-based reasoning, and temporal reasoning, and discusses fundamental limitations in such approaches. It proposes new representations for embedded narratives and fictional entities, for assessing the pace of a narrative, and offers an empirical theory of audience response. These notions are incorporated into an annotation scheme called {NarrativeML}. The book identifies key issues that need to be addressed, including annotation methods for long literary narratives, the representation of modality and habituality, and characterizing the goals of narrators. It also suggests a future characterized by advanced text mining of narrative structure from large-scale corpora and the development of a variety of useful authoring aids.

This is the first book to provide a systematic foundation that integrates together narratology, {AI}, and computational linguistics. It can serve as a narratology primer for computer scientists and an elucidation of computational narratology for literary theorists. It is written in a highly accessible manner and is intended for use by a broad scientific audience that includes linguists (computational and formal semanticists), {AI} researchers, cognitive scientists, computer scientists, game developers, and narrative theorists.},
	pages = {1--142},
	number = {3},
	journaltitle = {Synthesis Lectures on Human Language Technologies},
	author = {Mani, Inderjeet},
	urldate = {2013-04-23},
	date = {2012-12-31},
	langid = {english},
	keywords = {act\_Modeling, goal\_Analysis, goal\_Interpretation},
}

@article{koppel_computational_2008,
	title = {Computational methods in authorship attribution},
	volume = {60},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full},
	abstract = {Statistical authorship attribution has a long history, culminating in the use of modern machine learning classification methods. Nevertheless, most of this work suffers from the limitation of assuming a small closed set of candidate authors and essentially unlimited training text for each. Real-life authorship attribution problems, however, typically fall short of this ideal. Thus, following detailed discussion of previous work, three scenarios are considered here for which solutions to the basic attribution problem are inadequate. In the first variant, the profiling problem, there is no candidate set at all; in this case, the challenge is to provide as much demographic or psychological information as possible about the author. In the second variant, the needle-in-a-haystack problem, there are many thousands of candidates for each of whom we might have a very limited writing sample. In the third variant, the verification problem, there is no closed candidate set but there is one suspect; in this case, the challenge is to determine if the suspect is or is not the author. For each variant, it is shown how machine learning methods can be adapted to handle the special challenges of that variant.},
	pages = {9--26},
	number = {1},
	journaltitle = {Journal of the American Society for information Science and Technology},
	author = {Koppel, M. and Schler, J. and Argamon, S.},
	urldate = {2012-12-22},
	date = {2008},
	langid = {english},
	keywords = {bigdata},
}

@article{meister_computational_2005,
	title = {Computational approaches to narrative},
	url = {http://www.amazon.de/Routledge-Encyclopedia-Narrative-Theory-Herman/dp/0415775124},
	pages = {78--80},
	journaltitle = {Routledge Encyclopedia of Narratology},
	author = {Meister, Jan Christoph},
	date = {2005},
	langid = {english},
	note = {Meister, Jan Christoph: Computational approaches to narrative. In: David Herman/Marie-Laure Ryan (Ed.): Routledge Encyclopedia of Narratology. London and New York 2005, S. 78-80.},
	keywords = {{AnalyzeQualitatively}, obj\_Literature},
}

@inproceedings{popescu_comparing_2009,
	title = {Comparing Statistical Similarity Measures for Stylistic Multivariate Analysis},
	url = {http://www.aclweb.org/anthology/R09-1063},
	abstract = {The goal of this paper is to compare a set of dis-
tance/similarity measures, some motivated sta-
tistically, others motivated stylistically, regard-
ing their ability to reflect stylistic similarity
between texts. To assess the ability of these
distance/similarity functions to capture stylistic
similarity between texts, we have tested them in
the two most frequently employed multivariate
statistical analysis settings: cluster analysis and
(kernel) principal components analysis.},
	eventtitle = {International Conference {RANLP}},
	pages = {349--354},
	booktitle = {Proceedings of the International Conference {RANLP}},
	author = {Popescu, Marisu and Dinu, Liviu P.},
	date = {2009},
	langid = {english},
	keywords = {bigdata},
}

@article{fortunato_community_2010,
	title = {Community detection in graphs},
	volume = {486},
	issn = {03701573},
	url = {http://arxiv.org/abs/0906.0612},
	doi = {10.1016/j.physrep.2009.11.002},
	abstract = {The modern science of networks has brought significant advances to our understanding of complex systems. One of the most relevant features of graphs representing real systems is community structure, or clustering, i. e. the organization of vertices in clusters, with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters. Such clusters, or communities, can be considered as fairly independent compartments of a graph, playing a similar role like, e. g., the tissues or the organs in the human body. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. This problem is very hard and not yet satisfactorily solved, despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years. We will attempt a thorough exposition of the topic, from the definition of the main elements of the problem, to the presentation of most methods developed, with a special focus on techniques designed by statistical physicists, from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other, to the description of applications to real networks.},
	pages = {75--174},
	number = {3},
	journaltitle = {Physics Reports},
	author = {Fortunato, Santo},
	urldate = {2014-04-24},
	date = {2010-02},
	langid = {english},
	keywords = {*****, act\_NetworkAnalysis, goal\_Analysis, meta\_GiveOverview, obj\_Data, t\_Clustering},
}

@article{walsh_comic_2012,
	title = {Comic Book Markup Language: An Introduction and Rationale},
	volume = {6},
	url = {http://www.digitalhumanities.org/dhq/vol/6/1/000117/000117.html},
	shorttitle = {Comic Book Markup Language},
	abstract = {Comics, comic books, and graphic novels are increasingly the target of seriously scholarly attention in the humanities. Moreover, comic books are exceptionally complex documents, with intricate relationships between pictorial and textual elements and a wide variety of content types within a single comic book publication. The complexity of these documents, their combination of textual and pictorial elements, and the collaborative nature of their production shares much in common with other complex documents studied by humanists—illuminated manuscripts, artists’ books, illustrated poems like those of William Blake, letterpress productions like those of the Kelmscott Press, illustrated children’s books, and even Web pages and other born-digital media. Comic Book Markup Language, or {CBML}, is a {TEI}-based {XML} vocabulary for encoding and analyzing comic books, comics, graphic novels, and related documents. This article discusses the goals and motivations for developing {CBML}, reviews the various content types found in comic book publications, provides an overview and examples of the key features of the {CBML} {XML} vocabulary, explores some of the problems and challenges in the encoding and digital representation of comic books, and outlines plans for future work. The structural, textual, visual, and bibliographic complexity of comic books make them an excellent subject for the general study of complex documents, especially documents combining pictorial and textual elements.},
	number = {1},
	author = {Walsh, John A.},
	urldate = {2014-03-04},
	date = {2012},
	langid = {english},
	keywords = {bigdata, goal\_Enrichment, obj\_Comics, t\_Encoding},
}

@article{barker_colloquium:_2011,
	title = {Colloquium: Digital technologies: Help or hindrance for the humanities?},
	issn = {1474-0222, 1741-265X},
	url = {http://ahh.sagepub.com/content/early/2011/12/01/1474022211428311},
	doi = {10.1177/1474022211428311},
	shorttitle = {Colloquium},
	abstract = {This article offers reflections arising from a recent colloquium at the Open University on the implications of the development of digital humanities for research in arts disciplines, and also for their interactions with computing and technology. Particular issues explored include the ways in which the digital turn in humanities research is also a spatial/visual one; the tension between analysis based on the extensive ‘hard’ data generated by digital methodologies and the more subtle evaluations of traditional humanities research; the advantages and disadvantages of online resources that distance the researcher from the actual archive, book, artefact or archaeological site under investigation; and the unrealized potential for applying to the humanities software tools designed for science and technology. Constructive responses to such challenges and opportunities require the full rigour of the critical thinking that is the essence of arts and humanities research.},
	journaltitle = {Arts and Humanities in Higher Education},
	shortjournal = {Arts and Humanities in Higher Education},
	author = {Barker, Elton and Bissell, Chris and Hardwick, Lorna and Jones, Allan and Ridge, Mia and Wolffe, John},
	urldate = {2013-05-03},
	date = {2011-12-02},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@book{deegan_collaborative_2012,
	location = {Farnham, England; Burlington, {VT}},
	title = {Collaborative research in the digital humanities a volume in honour of Harold Short, on the occasion of his 65th birthday and his retirement, September 2010},
	isbn = {9781409410690  1409410692},
	url = {http://site.ebrary.com/id/10541037},
	publisher = {Ashgate},
	author = {Deegan, Marilyn and {McCarty}, Willard and Short, Harold},
	urldate = {2012-07-19},
	date = {2012},
	langid = {english},
	keywords = {goal\_Collaboration, obj\_DigitalHumanities, obj\_Research},
}

@collection{deegan_collaborative_2011,
	location = {Farnham, Surrey, England ; Burlington, {VT}},
	title = {Collaborative research in the digital humanities},
	isbn = {9781409410683},
	url = {http://www.ashgate.com/isbn/9781409410683},
	abstract = {Collaboration within digital humanities is both a pertinent and a pressing topic as the traditional mode of the humanist, working alone in his or her study, is supplemented by explicitly co-operative, interdependent and collaborative research. This is particularly true where computational methods are employed in large-scale digital humanities projects. This book, which celebrates the contributions of Harold Short to this field, presents fourteen essays by leading authors in the digital humanities. It addresses several issues of collaboration, from the multiple perspectives of institutions, projects and individual researchers},
	pagetotal = {248},
	publisher = {Ashgate Pub},
	editor = {Deegan, Marilyn and {McCarty}, Willard},
	date = {2011},
	langid = {english},
	keywords = {act\_Collaborating, goal\_Collaboration, obj\_AnyObject},
}

@incollection{fink_classification_2009,
	location = {Berlin, Heidelberg},
	title = {Classification of Text Processing Components: The Tesla Role System},
	isbn = {978-3-642-01043-9, 978-3-642-01044-6},
	url = {http://www.springerlink.com/index/10.1007/978-3-642-01044-6_26},
	shorttitle = {Classification of Text Processing Components},
	abstract = {The modeling of component interactions represents a major challenge in designing component systems. In most cases, the components in such systems interact via the results they produce. This approach results in two conflicting requirements that have to be satisfied. On the one hand, the interfaces between the components are subject to exact specifications. On the other hand, however, the component interfaces should not be excessively restricted as this might require the data produced by the components to be converted into the system’s data format. This might pose certain difficulties if complex data types (e.g., graphs or matrices) have to be stored as they often require non-trivial access methods that are not supported by a general data format.

The approach introduced in this paper tries to overcome this dilemma by meeting both demands: A role system is a generic way that enables text processing components to produce highly specific results. The role concept described in this paper has been adopted by the Tesla (Text Engineering Software Laboratory) framework.},
	pages = {285--294},
	booktitle = {Advances in Data Analysis, Data Handling and Business Intelligence},
	publisher = {Springer Berlin Heidelberg},
	author = {Hermes, Jürgen and Schwiebert, Stephan},
	editor = {Fink, Andreas and Lausen, Berthold and Seidel, Wilfried and Ultsch, Alfred},
	urldate = {2011-11-08},
	date = {2009},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Language, obj\_Tools},
}

@article{angelis_classical_2011,
	title = {Classical philology, text-based and Holocaust studies facing digital research infrastructures: from practice to requirements},
	url = {http://www.ehri-project.eu/classical-philology-text-based-and-holocaust-studies-facing-digital-research-infrastructures-practic},
	journaltitle = {{SDH} 2011 Supporting Digital Humanities},
	author = {Angelis, Stavros and Benardou, Agiatis and Constantopoulos, Panos and Dallas, Costis},
	date = {2011},
	langid = {english},
	keywords = {act\_Conceptualizing, meta\_GiveOverview, obj\_Infrastructures},
}

@book{finke_citizen_2014,
	location = {München},
	title = {Citizen Science: Das unterschätzte Wissen der Laien},
	isbn = {9783865814661},
	shorttitle = {Citizen Science},
	abstract = {Charles Darwin und Gregor Mendel gelten zu Recht als herausragende Figuren der Wissenschaftsgeschichte. Sie waren auf ihren Gebieten Amateure, keine Berufsforscher im heutigen Sinne. Was sie antrieb, war eine unstillbare Neugier, die auch heute noch vielen Laien zu Eigen ist und in leidenschaftlich gepflegten Hobbys und ehrenamtlicher Forschung in vielen Problemfeldern der Zivilgesellschaft ihren Ausdruck findet. Doch Wissenschaft und Forschung gelten mittlerweile als Privileg der Profis, das oftmals lebensnähere Wirken der Laien als zweitklassig. Dabei sind ihre Leistungen bedeutsamer denn je: das Jahrhundertprojekt Wikipedia wäre ohne Citizen Science undenkbar und auch  erfolgreiches bürgerschaftliches Engagement kommt ohne fundierte Sachkenntnisse nicht aus. Peter Finke legt die erste Einführung in die Ideenwelt von Citizen Science vor und lädt ein, die unterschätzte Welt der Wissensbürger zu entdecken. Sein Fazit: Wenn wir wirklich eine Wissensgesellschaft werden wollen, müssen wir unsere akademischen und politischen Maßstäbe neu justieren.},
	pagetotal = {240},
	publisher = {oekom verlag},
	author = {Finke, Peter and Laszlo, Ervin},
	date = {2014},
	langid = {german},
	keywords = {act\_Crowdsourcing, goal\_Dissemination, meta\_GiveOverview},
}

@collection{schier_cid._1995,
	title = {{CID}. Computergestützte Interpretationen von Detektivromanen},
	url = {http://www.gbv.de/dms/goettingen/184540100.pdf},
	abstract = {Das Forschungsprojekt {CID} widmet sich dreierlei Aufgaben: Fundiert durch Webers Theorie der analytischen Erzählung wird {CID} zum einen die Bauform und das erzähltechnische Regelwerk des Detektivromans und seiner «Untergattungen» untersuchen. Zum anderen geht es um die Entwicklung und Erprobung eines leistungsfähigen und komfortablen Instrumentariums zur computergestützten empirischen Analyse epischer Texte, das den Qualitätsanforderungen der sozialwissenschaftlichen Inhaltsanalyse entspricht. Mit Hilfe dieser Instrumente sollen schließlich wissenschaftlich relevante und repräsentative Ergebnisse für die Literaturwissenschaft ermittelt werden: Die Arbeitsfelder, in die {CID} aufgeteilt ist, haben verschiedene konstitutive Elemente epischer Texte im allgemeinen und des Detektivromans im besonderen zum Gegenstand.},
	pagetotal = {226},
	publisher = {Frankfurt am Main: Lang 1995},
	editor = {Schier, Dagmar and Giersch, Malchus},
	date = {1995},
	langid = {german},
	keywords = {{AnalyzeQualitatively}, goal\_Interpretation, obj\_Literature},
}

@article{rudman_cherry_2003,
	title = {Cherry Picking in Nontraditional Authorship Attribution Studies},
	volume = {16},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09332480.2003.10554845},
	pages = {26--32},
	number = {2},
	journaltitle = {{CHANCE}},
	author = {Rudman, Joseph},
	date = {2003},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview, obj\_Research, t\_Stylometry},
}

@incollection{miner_chapter_2012,
	location = {Boston},
	title = {Chapter 2 - The Seven Practice Areas of Text Analytics},
	isbn = {978-0-12-386979-1},
	url = {http://www.sciencedirect.com/science/article/pii/B9780123869791000025},
	abstract = {Prelude

Presently, text mining is in a loosely organized set of competing technologies that function as analytical “city-states” with no clear dominance among them. To further complicate matters, different areas of text mining are in different stages of maturity. Some technology is easily accessible by practitioners today via commercial software (some of which is included with this book), while other areas are only now emerging from academia into the practical realm.

We can relate these technologies to seven different practice areas in text mining that are covered in the chapters in this book. In summary, this book is strongest in the practice area of document classification, solid in concept extraction and document clustering, reasonably useful on web mining, light on information extraction and natural language processing, and almost silent on the (most popular) practice area of search and information retrieval.

The unifying theme behind each of these technologies is the need to “turn text into numbers” so that powerful analytical algorithms can be applied to large document databases. Converting text into a structured, numerical format and applying analytic algorithms both require knowing how to use and combine techniques for handling text, ranging from individual words to documents to entire document databases.

Next, we provide a decision tree to help you determine which practice area is appropriate to satisfy your needs. Finally, we provide tables to relate the practice areas to appropriate technologies and show which chapter in this book deals with that subject area. That is the most organization that we can impose on the current disordered state of text mining technology. Our goal in this book is to provide an introduction to each of the seven practice areas and cover in depth only those areas that are accessible for nonexperts. We will follow that theme in Part I of the book to provide you with the basics you need to perform the tutorials. Very quickly, you will be learning by doing.},
	pages = {29--41},
	booktitle = {Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications},
	publisher = {Academic Press},
	author = {Miner, Gary},
	urldate = {2012-04-25},
	date = {2012},
	langid = {english},
	keywords = {act\_ContentAnalysis, bigdata, goal\_Analysis, obj\_Text},
}

@collection{crane_changing_2009,
	title = {Changing the Center of Gravity: Transforming Classical Studies through Cyberinfrastructure},
	url = {http://digitalhumanities.org:8080/dhq/vol/3/1/index.html},
	series = {Digital Humanities Quarterly},
	abstract = {The workshop "Changing the Center of Gravity: Transforming Classical Studies Through Cyberinfrastructure" took place at University of Kentucky on Oct. 5, 2007. The workshop, celebration of the work of A. Ross Scaife, focused on the development of cyberinfrastructure for Classical studies. Workshop presenters included leading scholars in the field of Digital Classics from across North America.},
	number = {3.1},
	editor = {Crane, Gregory and Terras, Melissa},
	date = {2009},
	langid = {english},
	note = {http://www.rch.uky.edu/{CenterOfGravity}/},
	keywords = {meta\_Assessing, obj\_Infrastructures},
}

@article{can_change_2004,
	title = {Change of Writing Style With Time},
	volume = {38},
	url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.1.8850},
	abstract = {We analyze four
İ
nce Memed
novels of Ya
ş
ar Kemal using six style markers: “most frequent words,”
“syllable counts,” “word type -or part of speech- information,” “sentence length in terms of words,” “word
length in text,” and “word length in
vocabulary.” For analysis we divi
de each novel into five thousand
word text blocks and count the frequencies of each style marker in these blocks. The principal component
analysis results show clear separation between the first
two and the last two volumes; the blocks of the first
two novels are also distinguishable from each other. The blocks of the last two volumes are intermixed.
This parallels the fact that the author planned the last two volumes as three separate novels, but later
condensed them into two. The style markers showing the best separation are “most frequent words” and
“sentence length”. We use stepwise discriminant analys
is to determine the best discriminators of each style
marker and then use them in cross validation. The related results concur with the principal component
analysis results. For example, the cross validation re
sults obtained by “most frequent words” and “sentence
length,” respectively, provide 87\% and 81\% correct classification of the text blocks to their corresponding
volumes. Further investigation based on multiple analysis of variance ({MANOVA}) reveals how the
attributes of each style marker group distinguish among the volumes.},
	pages = {61--82},
	journaltitle = {{COMPUTERS} {AND} {THE} {HUMANITIES}},
	author = {Can, Fazli and Patton, Jon M},
	date = {2004},
	langid = {english},
	keywords = {t\_Stylometry},
}

@book{brunet_ce_2011,
	location = {Paris},
	title = {Ce qui compte, écrits choisis. Tome {II}, méthodes statistiques},
	isbn = {9782745322258 2745322257},
	url = {http://llc.oxfordjournals.org/content/27/2/235.extract},
	publisher = {Champion},
	author = {Brunet, Etienne},
	date = {2011},
	langid = {french},
	keywords = {{AnalyzeStatistically}},
}

@book{wright_cataloging_2014,
	location = {Oxford ; New York},
	title = {Cataloging the World: Paul Otlet and the Birth of the Information Age},
	isbn = {9780199931415},
	shorttitle = {Cataloging the World},
	abstract = {The dream of capturing and organizing knowledge is as old as history. From the archives of ancient Sumeria and the Library of Alexandria to the Library of Congress and Wikipedia, humanity has wrestled with the problem of harnessing its intellectual output. The timeless quest for wisdom has been as much about information storage and retrieval as creative genius.In Cataloging the World, Alex Wright introduces us to a figure who stands out in the long line of thinkers and idealists who devoted themselves to the task. Beginning in the late nineteenth century, Paul Otlet, a librarian by training, worked at expanding the potential of the catalog card, the world's first information chip. From there followed universal libraries and museums, connecting his native Belgium to the world by means of a vast intellectual enterprise that attempted to organize and code everything ever published. Forty years before the first personal computer and fifty years before the first browser, Otlet envisioned a network of "electric telescopes" that would allow people everywhere to search through books, newspapers, photographs, and recordings, all linked together in what he termed, in 1934, a réseau mondial--essentially, a worldwide web.Otlet's life achievement was the construction of the Mundaneum--a mechanical collective brain that would house and disseminate everything ever committed to paper. Filled with analog machines such as telegraphs and sorters, the Mundaneum--what some have called a "Steampunk version of hypertext"--was the embodiment of Otlet's ambitions. It was also short-lived. By the time the Nazis, who were pilfering libraries across Europe to collect information they thought useful, carted away Otlet's collection in 1940, the dream had ended. Broken, Otlet died in 1944.Wright's engaging intellectual history gives Otlet his due, restoring him to his proper place in the long continuum of visionaries and pioneers who have struggled to classify knowledge, from H.G. Wells and Melvil Dewey to Vannevar Bush, Ted Nelson, Tim Berners-Lee, and Steve Jobs. Wright shows that in the years since Otlet's death the world has witnessed the emergence of a global network that has proved him right about the possibilities--and the perils--of networked information, and his legacy persists in our digital world today, captured for all time.},
	pagetotal = {360},
	publisher = {Oxford University Press},
	author = {Wright, Alex},
	date = {2014-06-04},
	langid = {english},
	keywords = {obj\_DigitalHumanities},
}

@misc{herold_case_2011,
	title = {Case Studies – Final Report (D-{SPIN} Report R 3.4)},
	url = {http://weblicht.sfs.uni-tuebingen.de/Reports/D-SPIN_R3.4.pdf},
	abstract = {The following report presents the results of the an
alysis with respect to the in-depth inter-
views with selected researchers in the humanities t
hat were recorded between September 2009
and June 2010 at the {BBAW} and the University of Fra
nkfurt. These interviews give a broad
perspective on both the current practice and the fu
ture needs of humanities researchers.
All interviewees expressed strong needs for primary
and secondary textual sources in elec-
tronic form. Many of them base their work and resea
rch already on electronic representations
of texts. Consequently, the focus of the interviews
was on resource use rather than on tool use
reflecting the high priority of resources over (lin
guistic) tools.
A “scientific workbench of the future” should ultim
ately integrate all available resources and
provide a wide range of methods for creating virtua
l collections and perform sophisticated
search operations on them. Although unrestricted sh
aring of resources created by the inter-
viewees seems uncommon in general and has hardly ha
ppened so far, most of them agreed
that given the possibility to assign fine grained a
ccess rights for their resources they would be
willing to share them.
A central finding of the interviews is that the {CLA}
{RIN}/D-{SPIN} approach to an integrated
infrastructure for both linguistic resources and se
rvices accompanied by a sophisticated user
and access management meets most of the needs expre
ssed by the experts.
Since this is a survey of German researchers, the r
est of this report is written in German.},
	publisher = {D-{SPIN}/{BBAW}},
	author = {Herold, Axel and Warken, Timo and Binder, Frank and Gehrke, Ralf},
	date = {2011-03},
	langid = {german},
	keywords = {X-{CHECK}},
}

@article{kalvesmaki_canonical_2014,
	title = {Canonical References in Electronic Texts: Rationale and Best Practices},
	volume = {8},
	url = {http://www.digitalhumanities.org/dhq/vol/8/2/000181/000181.html},
	shorttitle = {Canonical References in Electronic Texts},
	abstract = {Systems of canonical references, whereby segments of written works are sequentially labeled with numbers or letters to facilitate cross-referencing, are widely used but seldom studied, undeservedly so. Canonical numbers are complex interpretive mechanisms with a great deal of potential for anyone editing and using electronic texts. In this essay I consider the rationale for and nature of canonical reference systems, to recommend principles to consider when deploying them in digital projects. After briefly reviewing the history of canonical references I note how they have been used so far, emphasizing the advances made by Canonical Text Services ({CTS}). I argue that the practical and theoretical problems that remain unaddressed require engagement with descriptions of how textual scholarship works and how notional literary works relate to the artefacts that carry them (using Functional Requirements for Bibliographic Records, {FRBR}). By correlating a theory of canonical reference numbers with those two models — editorial workflow and creative works — I offer key principles that should be addressed when planning, writing, and using digital projects.},
	number = {2},
	author = {Kalvesmaki, Joel},
	urldate = {2014-07-21},
	date = {2014},
	langid = {english},
	keywords = {act\_Annotating, act\_Identifying, goal\_Enrichment, obj\_Text},
}

@report{kirkham_building_2007,
	title = {Building a Virtual Research Environment for the Humanities ({BVREH}). Final Report},
	url = {http://bvreh.humanities.ox.ac.uk/files/Microsoft%20Word%20-%20JISC_Final_Report_Web.pdf},
	institution = {{JISC}},
	author = {Kirkham, Ruth},
	date = {2007},
	langid = {english},
	keywords = {act\_Conceptualizing, meta\_ProjectManagement, obj\_VREs},
}

@article{dambeck_blinde_2011,
	title = {Blinde Computer sollen sehen lernen},
	url = {http://www.spiegel.de/netzwelt/gadgets/0,1518,775650,00.html},
	abstract = {Ein Schloss! Ein Schuh! Der Himmel! Wir Menschen erfassen den Inhalt von Fotos binnen Sekundenbruchteilen. Computer sind damit jedoch vollkommen überfordert. Nun bringen Forscher den Maschinen das Sehen bei - ein Vorhaben, von dem auch Google profitieren will.},
	journaltitle = {Spiegel Online},
	author = {Dambeck, Aus Vancouver berichtet Holger},
	urldate = {2011-07-23},
	date = {2011-07-22},
	langid = {german},
	keywords = {{AnalyzeQualitatively}, obj\_Images},
}
@article{muller-jung_bioforschung_2013,
	location = {Frankfurt},
	title = {Bioforschung an Grenzen Wird „Big Data“ zur Chiffre für den digitalen {GAU}?},
	issn = {0174-4909},
	url = {http://www.faz.net/aktuell/wissen/mensch-gene/bioforschung-an-grenzen-wird-big-data-zur-chiffre-fuer-den-digitalen-gau-12103088.html},
	abstract = {04.03.2013 · Es ist der Fluch der Moderne: Ein Daten-Tsunami überrollt die Wissenschaften. Beispiel Biomedizin. Sie ist in digitalen Nöten. Hilfe? Einen Plan gibt es schon.},
	journaltitle = {{FAZ}.{NET}},
	author = {Müller-Jung, Joachim},
	urldate = {2013-03-08},
	date = {2013-03-04},
	langid = {german},
	keywords = {bigdata{\textasciitilde}, goal\_Storage, obj\_Data},
}

@article{chern_biochemical_1976,
	title = {Biochemical and electrophoretic studies of erythrocyte pyridoxine kinase in white and black Americans},
	volume = {28},
	issn = {0002-9297},
	abstract = {The mean {PNK} activity in red blood cells from black subjects was only about 40\% of that in whites. Among 51 whites examined, one was found to have enzyme deficiency. The estimated gene frequencies for {PNKH} (the common allele in whites which codes for higher enzyme activity) and {PNKL} (the common allele in blacks which codes for lower enzyme activity) were .35 and .65, respectively, for black donors, and .81 and .19, respectively, for white donors, The variant enzyme in persons with enzyme deficiency was associated with an increased rate of degradation in red cells during aging. No other biochemical or electrophoretic differences were detected.},
	pages = {9--17},
	number = {1},
	journaltitle = {American journal of human genetics},
	shortjournal = {Am. J. Hum. Genet.},
	author = {Chern, C J and Beutler, E},
	date = {1976-01},
	langid = {english},
	pmid = {2009},
}

@book{mayer-schonberger_big_2013,
	location = {Boston},
	title = {Big data: a revolution that will transform how we live, work, and think},
	isbn = {9780544002692},
	shorttitle = {Big data},
	abstract = {A revelatory exploration of the hottest trend in technology and the dramatic impact it will have on the economy, science, and society at large.

Which paint color is most likely to tell you that a used car is in good shape? How can officials identify the most dangerous New York City manholes before they explode? And how did Google searches predict the spread of the H1N1 flu outbreak?

The key to answering these questions, and many more, is big data. “Big data” refers to our burgeoning ability to crunch vast collections of information, analyze it instantly, and draw sometimes profoundly surprising conclusions from it. This emerging science can translate myriad phenomena—from the price of airline tickets to the text of millions of books—into searchable form, and uses our increasing computing power to unearth epiphanies that we never could have seen before. A revolution on par with the Internet or perhaps even the printing press, big data will change the way we think about business, health, politics, education, and innovation in the years to come. It also poses fresh threats, from the inevitable end of privacy as we know it to the prospect of being penalized for things we haven’t even done yet, based on big data’s ability to predict our future behavior.

In this brilliantly clear, often surprising work, two leading experts explain what big data is, how it will change our lives, and what we can do to protect ourselves from its hazards. Big Data is the first big book about the next big thing.

www.big-data-book.com},
	pagetotal = {242},
	publisher = {Houghton Mifflin Harcourt},
	author = {Mayer-Schönberger, Viktor},
	editora = {Cukier, Kenneth},
	editoratype = {collaborator},
	date = {2013},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata{\textasciitilde}, meta\_Assessing},
}

@article{gotzner_big_2013,
	title = {Big Data. Alle 50 Jahre Lynchmob [Kliodynamik: Mit Archiven und Google leiten Forscher die Zukunft ab]},
	url = {http://www.newscientist.de/inhalt/kliodynamik-mit-archiven-und-google-leiten-forscher-die-zukunft-ab-a-898872.html},
	number = {19},
	journaltitle = {New Scientist},
	author = {Gotzner, Peter},
	date = {2013},
	langid = {german},
	keywords = {{AnalyzeStatistically}, bigdata{\textasciitilde}},
}

@article{giron_bayesian_2005,
	title = {Bayesian Analysis of a Multinomial Sequence and Homogeneity of Literary Style},
	volume = {59},
	url = {http://www.biblioteca.uma.es/bbldoc/tesisuma/16615748.pdf},
	doi = {10.1198/000313005X21311},
	abstract = {To help settle the debate around the authorship of Tirant lo Blanc, all words in each chapter are categorized according to their length, and the appearances of certain words are counted, thus forming two contingency tables of ordered rows. A Bayesian multinomial change-point analysis of the sequence of rows, reveals a clear stylistic boundary, estimated to be near chapters 371 and 382. A Bayesian cluster analysis of these rows confirms the existence of that boundary, and reveals a few chapters that are misclassified by the estimated change-point. The statistical evidence supports the hypotheses of one main author writing about four fifths of the book, with a second author finishing the book by filling in material, mainly at the end of it.},
	pages = {19--30},
	number = {1},
	journaltitle = {American Statistician},
	author = {Girón, Javier and Ginebra, Josep and Riba, Alex},
	date = {2005},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Literature, t\_Stylometry},
}

@inproceedings{tan_barriers_2008,
	location = {Florence, Italy},
	title = {Barriers to virtual collaboration},
	isbn = {978-1-60558-012-X},
	url = {http://portal.acm.org/citation.cfm?id=1358636&dl=GUIDE&coll=GUIDE&CFID=32938343&CFTOKEN=88771719},
	doi = {10.1145/1358628.1358636},
	abstract = {This paper reports on the implementation and use of a virtual collaboration system - a virtual collaborative desk ({VCD}) that has been introduced to a software design team in an organizational context. Virtual collaboration systems are complex and can be considered as social-technical systems, oftentimes encompassing several layers of both technical and social issues. If this multi-layered social-technical system is to work effectively and provide a dependable service, then all the layers must be well understood and structured accordingly. Otherwise, these layers can become barriers to virtual collaboration if they impede the collaborating users of a virtual team from attaining their goals. An amalgamation of principles from life-cycle and ethnomethodologically informed ethnography approaches in the evaluation of a virtual collaborative system is demonstrated in a case-study to enable researchers to understand what these issues are and how the different types of issues can prevent effective virtual collaboration.},
	pages = {2045--2052},
	publisher = {{ACM}},
	author = {Tan, Amy and Kondoz, Ahmet M.},
	urldate = {2009-05-02},
	date = {2008},
	langid = {english},
	keywords = {goal\_Collaboration, obj\_Infrastructures, obj\_People, t\_Usability},
}

@article{smithies_view_2011,
	title = {A View from {IT}},
	volume = {5},
	url = {http://digitalhumanities.org/dhq/vol/5/3/000107/000107.html},
	abstract = {As digital humanities projects grow in size and complexity university programs will need to adapt, balancing the needs of technological systems with the imperatives of the humanities tradition. While it makes sense to adapt the accumulated expertise of the commercial and government {IT} sectors, care needs to be taken to ensure any new approaches enhance rather than undermine the aims of the humanities generally. While digital humanists are uniquely positioned to help the humanities, care needs to be taken to ensure new project management and design techniques sourced from the {IT} world are applied critically and do not undermine the core aims of the discipline. If these caveats are kept in mind the {IT} world has a lot to offer digital humanists, however, especially in the field of Enterprise Architecture ({EA}), which aims to produce a holistic, high level view of technological systems with a view to understanding social and cultural as well as technological issues.},
	number = {2},
	journaltitle = {Digital Humanities Quarterly},
	author = {Smithies, James},
	urldate = {2011-11-30},
	date = {2011},
	langid = {english},
	keywords = {X-{CHECK}},
}

@article{simanowski_autorschaften_2001,
	title = {Autorschaften in digitalen Medien. Eine Einleitung},
	url = {http://dichtung-digital.de/cv/Simanowski-Autorschaften.pdf},
	pages = {3--21},
	journaltitle = {Arnold 2001},
	author = {Simanowski, Roberto},
	date = {2001},
	langid = {german},
}

@inproceedings{kessler_automatic_1997,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Automatic Detection of Text Genre},
	url = {http://dx.doi.org/10.3115/976909.979622},
	doi = {10.3115/976909.979622},
	series = {{ACL} '98},
	abstract = {As the text databases available to users become larger and more heterogeneous, genre becomes increasingly important for computational linguistics as a complement to topical and structural principles of classification. We propose a theory of genres as bundles of facets, which correlate with various surface cues, and argue that genre detection based on surface cues is as successful as detection based on deeper structural properties.},
	pages = {32--38},
	booktitle = {Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Kessler, Brett and Numberg, Geoffrey and Schütze, Hinrich},
	urldate = {2013-05-08},
	date = {1997},
	keywords = {{AnalyzeStatistically}},
}

@inproceedings{koppel_authorship_2004,
	title = {Authorship verification as a one-class classification problem},
	url = {http://dl.acm.org/citation.cfm?id=1015448},
	abstract = {In the authorship verification problem, we are given examples of the writing of a single author and are asked to determine if given long texts were or were not written by this author. We present a new learning-based method for adducing the "depth of difference" between two example sets and offer evidence that this method solves the authorship verification problem with very high accuracy. The underlying idea is to test the rate of degradation of the accuracy of learned models as the best features are iteratively dropped from the learning process},
	pages = {62},
	booktitle = {Proceedings of the twenty-first international conference on Machine learning},
	author = {Koppel, M. and Schler, J.},
	urldate = {2012-12-22},
	date = {2004},
	langid = {english},
}

@article{dabagh_authorship_2007,
	title = {Authorship attribution and statistical text analysis},
	volume = {4},
	url = {http://www.dlib.si/stream/URN:NBN:SI:doc-B0ZL7BYX/a8985ac7-d01f-4cfe-b980-35017a00e756/PDF},
	abstract = {In the study of ancient literature, a major problem is to deal with uncertain authorship. Ambiguity 
about authorship is not limited to  the works from remote era. Different reasons cause uncertainty 
in authorship, such as reproduction of books by hand, prestige, having good  sells  for works with 
forged reputable names on them, and sometimes social or political pressures. Whatever the reason, 
authorship case studies offer the statistician an interesting opportunity to deal with various 
applied problems, where many standard statistical techniques have been introduced.
In statistical analysis of literary texts one tries to apply an objective methodology to works that 
have received impressionistic  treatment  for  a long time. In subjective analysis of literary 
style, experts use literary style of the text, which is not quantifiable, as an important criterion 
in their judgments. Subjective approach can rarely lead to a unique solution acceptable to all the 
scholars. Statistical quantitative methods provide objective components for judgments.
In the quantitative approach, by carefully analyzing the style of the text one tries to find out 
how to characterize the style of an author numerically and determine sets of features (variables) 
in a text that most accurately describe the author’s style.
Much work has been done covering different aspects of this field. Different variables are proposed 
as distinguishing characteristics of writers, a wide range of mathematical methods is employed, and 
there is still a lot to be done in the future.
The paper presents a brief history and a review of the statistical analysis of literary style, 
looks at several variables that have been used as stylistic criteria of authors, as well as the 
methods used. This is followed by some illustrations on Farsi text, implying that there are some 
general rules that hold for different languages.},
	pages = {149--163},
	number = {2},
	journaltitle = {Metodološki zvezki},
	author = {Dabagh, {RM}},
	date = {2007},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview, t\_Stylometry},
}

@article{holmes_authorship_1994,
	title = {Authorship Attribution},
	volume = {28},
	url = {http://www.jstor.org/discover/10.2307/30200315?uid=3737864&uid=2&uid=4&sid=21104660148143},
	abstract = {This paper considers the problem of quantifying literary style and looks at several variables which may be used as stylistic "fingerprints" of a writer. A review of work done on the statistical analysis of "change over time" in literary style is then presented, followed by a look at a specific application area, the authorship of Biblical texts.},
	pages = {87--106},
	number = {2},
	journaltitle = {Computers and the Humanities},
	author = {Holmes, David I.},
	date = {1994},
	langid = {english},
}

@incollection{hoover_authorial_2010,
	title = {Authorial Style},
	abstract = {"Inspired by exploring the language of poems, plays and prose, Mick Short's classic introduction to stylistics, language and style represents the state-of-the-art in literary stylistics and encompasses the full breadth of current research in the discipline. Written by leading scholars in the field, chapters cover a variety of methodological and analytical approaches, from traditional qualitative analysis to more recent developments in cognitive and corpus stylistics. Addressing the three, key literary genres of poetry, drama and narrative, Language and style is divided into carefully balanced sections. Based on original research, each chapter demonstrates a particular analytic technique and explains how this might be applied to a text from one of the literary genres. Framed by helpful introductory material covering the foundational principles of stylistics, the chapters act as practical exemplars of how to carry out stylistic analysis. Comprehensive and engaging, this invaluable resource is essential reading for anyone interested in stylistics"},
	pages = {250--271},
	booktitle = {Language and Style: Essays in Honour of Mick Short},
	publisher = {Palgrave},
	author = {Hoover, David L. and Mcintyre, Dan and Busse, Beatrix},
	date = {2010},
	langid = {english},
}

@article{thiel_auf_2011,
	title = {Auf dem Weg zur digitalen Großarchitektur},
	issn = {0174-4909},
	url = {http://www.faz.net/aktuell/feuilleton/forschung-und-lehre/digital-humanities-auf-dem-weg-zur-digitalen-grossarchitektur-11561481.html},
	abstract = {Zentralisiert, standardisiert, europäisiert: Die Geistes- und Sozialwissenschaften überlegen, wie die Strukturen ihrer Forschung dem Wandel der Wissensordnung anzupassen sind.},
	journaltitle = {{FAZ}.{NET}},
	author = {Thiel, Thomas},
	urldate = {2011-12-16},
	date = {2011-12-13},
	langid = {german},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, obj\_Infrastructures},
}

@book{love_attributing_2002,
	location = {Cambridge [u.a.]},
	title = {Attributing authorship : an introduction},
	isbn = {0521789486 9780521789486 0521783399 9780521783392},
	url = {http://www.cambridge.org/gb/academic/subjects/literature/literary-theory/attributing-authorship-introduction},
	shorttitle = {Attributing authorship},
	abstract = {Description
    Contents
    Resources
    Courses
    About the Authors

    Recent literary scholarship has seen a shift of interest away from questions of attribution. Yet these questions remain urgent and important for any historical study of writing, and have been given a powerful new impetus by advances in statistical studies of language and the coming on line of large databases of texts in machine-searchable form. The present book is the first comprehensive survey of the field from a literary perspective to appear for forty years. It covers both traditional and computer based approaches to attribution, and evaluates each in respect of their potentialities and limitations. It revisits a number of famous controversies, including those concerning the authorship of the Homeric poems, books from the Old and New Testaments, and the plays of Shakespeare. Written with wit as well as erudition Attributing Authorship will make this intriguing field accessible for students and scholars alike.},
	publisher = {Cambridge Univ. Press},
	author = {Love, Harold},
	date = {2002},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_GiveOverview, t\_Stylometry},
}

@report{van_der_graaf_surfboard_2011,
	title = {A Surfboard for Riding the Wave. Towards a four country action programme on research data},
	url = {A Surfboard for Riding the Wave. Towards a four country action programme on research data},
	abstract = {This paper presents an overview of the present situation with regard to research data in Denmark, Germany, the Netherlands and the United Kingdom and offers broad outlines for a possible action programme for the four countries in realising the envisaged collaborative data infrastructure. An action programme at the level of four countries needs the involvement of all stakeholders from the scientific community. We
identified four key drivers: incentives; training in relation to researchers in their role as data producers and users of information infrastructures; infrastructure; funding of the infrastructure in relation to further developments in data logistics.},
	institution = {{KE} Knowledge Exchange},
	author = {van der Graaf, Maurits and Waaijers, Leo},
	date = {2011-11},
	langid = {english},
	keywords = {meta\_Advocating, meta\_Assessing, obj\_Infrastructures},
}

@book{ellegard_statistical_1962,
	location = {Göteborg},
	title = {A statistical method for determining authorship: the Junius letters, 1769-1772.},
	shorttitle = {A statistical method for determining authorship},
	author = {Ellegård, Alvar},
	date = {1962},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@article{burrows_second_2012,
	title = {A Second Opinion on 'Shakespeare and Authorship Studies in the Twenty-First Century},
	volume = {63},
	url = {http://muse.jhu.edu/login?auth=0&type=summary&url=/journals/shakespeare_quarterly/v063/63.3.burrows.html},
	abstract = {In a review essay entitled “Shakespeare and Authorship Studies in the Twenty-First Century,” published in Shakespeare Quarterly (62 [2011]: 106–42), Brian Vickers makes a scathing attack upon computational stylistics as a scholarly enterprise and as represented by the book he is reviewing. His principal target is the use of word-frequency patterns as evidence of authorship. He argues that his current approach to the attribution of authorship, an approach that deals in collocations of words, is effective and superior. The book in question, Shakespeare, Computers, and the Mystery of Authorship, edited by Hugh Craig and Arthur Kinney (2009) is best assessed by scholars deeply versed in Shakespeare studies; the consensus may be far more favorable than Vickers expects. The main purpose of this “Second Opinion” is neither to defend Craig and Kinney nor to offer a general survey of the field, but to show that such methods are well suited to the work of attribution. The present essay concludes that, when Vickers overcomes several shortcomings in his own new method, it may well become a useful addition to the scholar’s armory. The reason all this matters is that new methods of analysis are yielding very accurate results. As is sometimes the case with advanced bibliographical methods and the close study of printing-house practice, apparent technical complexity is a barrier to their acceptance. But all of these methods should be judged by their results.},
	pages = {355--92},
	number = {3},
	journaltitle = {Shakespeare Quarterly},
	author = {Burrows, John F.},
	date = {2012},
	langid = {english},
	keywords = {X-{CHECK}},
}

@book{kansa_archaeology_2011,
	location = {Cotsen Institute of Archaeology at U C L A},
	title = {Archaeology 2. 0 and Beyond},
	url = {http://www.escholarship.org/uc/item/1r6137tb#page-2},
	author = {Kansa, Eric C and Kansa, Sarah Whitcher and Watrall, Ethan},
	date = {2011-08},
	langid = {english},
	keywords = {X-{CHECK}, meta\_GiveOverview, obj\_Artefacts},
}

@article{carroll_rationale_2013,
	title = {A rationale for evolutionary studies of literature},
	volume = {3},
	url = {http://www.jbe-platform.com/content/journals/10.1075/ssol.3.1.03car},
	doi = {10.1075/ssol.3.1.03car},
	abstract = {I identify converging lines of evidence for the proposition that the human mind has evolved, argue that the evolved character of the mind influences the products of the mind, including literature, and conclude that scholarly and scientific commentary on literature would benefit from being explicitly lodged within an evolutionary conceptual framework. I argue that a biocultural perspective has comprehensive scope and can encompass all the topics to which other schools of literary theory give attention. To support this contention, I appeal to axiomatic logic: the behavior of any organism is a result of interactions between its genetically determined characteristics and its environmental influences. Summarizing the debate over the adaptive function of literature, I argue that literature and its oral antecedents are adaptations, not merely by-products of adaptations.},
	pages = {8--15},
	number = {1},
	journaltitle = {Scientific Study of Literature},
	shortjournal = {Scientific Study of Literature},
	author = {Carroll, Joseph},
	date = {2013},
	langid = {english},
}

@online{williamson_hidden_nodate,
	title = {A hidden computing curriculum? How 'learning to code' campaigns and edtech industry helped shape school policy},
	url = {http://codeactsineducation.wordpress.com/2014/09/05/hidden-computing-curriculum/},
	shorttitle = {A hidden computing curriculum?},
	abstract = {By Ben Williamson In the last few years, the idea of ‘learning to code’ and ‘digital making’ has grown from a minority focus among computing educators, grassroots computing organizations, and compu...},
	titleaddon = {code acts in education},
	author = {Williamson, Ben},
	urldate = {2014-09-05},
	keywords = {act\_Teaching/Learning, meta\_Assessing, obj\_Code},
}

@article{juola_prototype_2006,
	title = {A Prototype for Authorship Attribution Studies},
	volume = {21},
	url = {http://llc.oxfordjournals.org/content/21/2/169.abstract},
	doi = {10.1093/llc/fql019},
	abstract = {Despite a century of research, statistical and computational methods for authorship attribution are neither reliable, well-regarded, widely used, or well-understood. This article presents a survey of the current state of the art as well as a framework for uniform and unified development of a tool to apply the state of the art, despite the wide variety of methods and techniques used. The usefulness of the framework is confirmed by the development of a tool using that framework that can be applied to authorship analysis by researchers without a computing specialization. Using this tool, it may be possible both to expand the pool of available researchers as well as to enhance the quality of the overall solutions [for example, by incorporating improved algorithms as discovered through empirical analysis (Juola, P. (2004a). Ad-hoc Authorship Attribution Competition. In Proceedings 2004 Joint International Conference of the Association for Literary and Linguistic Computing and the Association for Computers and the Humanities ({ALLC}/{ACH} 2004), Göteborg, Sweden)].},
	pages = {169 --178},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Juola, Patrick and Sofko, John and Brennan, Patrick},
	urldate = {2011-12-14},
	date = {2006-06},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Theorizing, t\_Stylometry},
}

@book{mosteller_applied_1983,
	edition = {2nd ed.},
	title = {Applied Bayesian and Classical Inference: The Case of the Federalist Papers},
	isbn = {0387909915},
	shorttitle = {Applied Bayesian and Classical Inference},
	abstract = {The new version has two additions. First, at the suggestion of Stephen Stigler I we have replaced the Table of Contents by what he calls an Analytic Table of Contents. Following the title of each section or subsection is a description of the content of the section. This material helps the reader in several ways, for example: by giving a synopsis of the book, by explaining where the various data tables are and what they deal with, by telling what theory is described where. We did several distinct full studies for the Federalist papers as well as many minor side studies. Some or all may offer information both to the applied and the theoretical reader. We therefore try to give in this Contents more than the few cryptic words in a section heading to {\textasciitilde}peed readers in finding what they want. Seconq, we have prepared an extra chapter dealing with authorship work published from. about 1969 to 1983. Although a chapter cannot compre- hensively Gover a field where many books now appear, it can mention most ofthe book-length works and the main thread of authorship' studies published in English. We founq biblical authorship studies so extensive and com- plicated that we thought it worthwhile to indicate some papers that would bring out the controversies that are taking place. We hope we have given the flavor of developments over the 15 years mentioned. We have also corrected a few typographical errors.},
	pagetotal = {303},
	publisher = {Springer New York},
	author = {Mosteller, F. and Wallace, D. L.},
	date = {1983-01-01},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@book{easley_networks_2010,
	title = {Networks, Crowds, and Markets. Reasoning About a Highly Connected World},
	url = {http://www.cambridge.org/us/academic/subjects/computer-science/algorithmics-complexity-computer-algebra-and-computational-g/networks-crowds-and-markets-reasoning-about-highly-connected-world},
	abstract = {A unique probabilistic approach to studying pattern matching problems in computer science, telecommunications, molecular biology and more.},
	publisher = {{CUP}},
	author = {Easley, David and Kleinberg, Jon},
	urldate = {2014-09-05},
	date = {2010},
	langid = {english},
	keywords = {*****, act\_RelationalAnalysis, goal\_Analysis, t\_NetworkAnalysis},
}
@article{pivorun_biotelemetry_1976,
	title = {A biotelemetry study of the thermoregulatory patterns of Tamias striatus and Eutamias minimus during hibernation},
	volume = {53},
	issn = {0300-9629},
	url = {http://www.sciencedirect.com/science/article/pii/S0300962976800345},
	abstract = {The hibernation period of Tamias striatus is composed of a test drop, pre-plateau and plateau stage. The number of test drops during the test drop stage increases with a lowering of ambient temperature. The number of torpor bouts during the pre-plateau stage increases with a lowering of ambient temperature. The duration of bouts of torpor during the plateau stage increases linearly with a decrease in ambient temperature. (duration in hr = 139·60−5·93 Ta).
Tamias striatus displays a control on the rate of cooling during the entry into a torpor bout. Differential rates of cooling are observed between torpor bouts occurring during the test drop and post-test drop stages at the ambient temperatures of 8°, 6° and 3°C but not at 16°, 13° and 10°C. The hibernation period of Eutamias minimus is composed of a pre-plateau and plateau stage. Eutamias minimus displays significantly longer durations of torpor during the plateau stage than Tamias striatus. Eutamias minimus displays a control on the rate of cooling during the entry into a torpor bout. Eutamias minimus is a “deeper” hibernator than Tamias striatus, i.e. less prone to arousals.},
	pages = {265--271},
	number = {3},
	journaltitle = {Comparative biochemistry and physiology. A, Comparative physiology},
	shortjournal = {Comp Biochem Physiol A Comp Physiol},
	author = {Pivorun, E B},
	date = {1976},
	langid = {english},
}

@article{machlis_22_2011,
	title = {22 free tools for data visualization and analysis},
	url = {http://www.computerworld.com/article/2506820/business-intelligence/chart-and-image-gallery-30-free-tools-for-data-visualization-and-analysis.html},
	abstract = {Got data? These useful tools can turn it into informative, engaging graphics.},
	journaltitle = {Computerworld},
	author = {Machlis, Sharon},
	date = {2011-04-20},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, act\_Visualizing, meta\_GiveOverview, obj\_Tools},
}

@online{hitchcock_twitter_2014,
	title = {Twitter and blogs are not add-ons to academic research, but a simple reflection of the passion that underpins it.},
	url = {http://blogs.lse.ac.uk/impactofsocialsciences/2014/07/28/twitter-and-blogs-academic-public-sphere/},
	abstract = {The role of the academic humanist has always been a public one - however mediated through teaching and publication, argues Tim Hitchcock. As central means to participate in public conversations, Tw...},
	titleaddon = {Impact of Social Sciences},
	author = {Hitchcock, Tim},
	urldate = {2014-09-03},
	date = {2014-07-28},
	langid = {english},
	keywords = {act\_Publishing, goal\_Dissemination, obj\_Research},
}

@inproceedings{hakinson_interchange_2010,
	location = {Utrecht, Netherlands},
	title = {An Interchange Format for Optical Music Recognition Applications},
	url = {http://ismir2010.ismir.net/proceedings/ismir2010-11.pdf},
	abstract = {Page
appearance and
layout for
music notation is a
cri
tical component of the overall musical information
contained in a document
.
To capture and transfe
r this
information, we outline an interchange format for {OMR}
applications, the {OMR} Interchange Package ({OIP})
format,
which is
designed to allow layout information
and page images to be
preserved and
transferred along
with semantic musical content. We ident
ify a number of
uses for this format that can enhance digital
representations of music, and introduce a novel
idea for
distributed optical music recognition
system
based on this
format},
	pages = {51--56},
	booktitle = {In Proceedings of the 11th International Society for Music Information Retrieval Conference ({ISMIR} 2010)},
	author = {Hakinson, A. and Pugin, Laurent and Fujinaga, I.},
	date = {2010},
	langid = {english},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
}

@online{froehlich_introductory_2014,
	title = {An introductory bibliography to corpus linguistics},
	url = {http://hfroehlich.wordpress.com/2014/05/11/intro-bibliography-corpus-linguistics/},
	abstract = {This is a short bibliography meant to get you started in corpus linguistics – it is by no means comprehensive, but should serve to be a good introductory overview of the field.},
	titleaddon = {http://hfroehlich.wordpress.com},
	author = {Froehlich, Heather},
	date = {2014},
	langid = {english},
}

@article{siemens_new_2002,
	title = {A New Computer-Assisted Literary Criticism?},
	volume = {36},
	issn = {0010-4817},
	url = {http://www.jstor.org/stable/30200526},
	abstract = {If there is such a thing as a new computer-assisted literary criticism, its expression lies in a model that is as broad-based as that presented in John Smith's seminal article, "Computer Criticism," and is as encompassing of the discipline of literary studies as it is tied to the evolving nature of the electronic literary text that lies at the heart of its intersection with computing. It is the desire to establish the parameters of such a model for the interaction between literary studies and humanities computing - for a model of the new computer-assisted literary criticism - that gave rise to the papers in this collection and to the several conference panel-presentations and discussions that, in their print form, these papers represent.},
	pages = {259--267},
	number = {3},
	journaltitle = {Computers and the Humanities},
	author = {Siemens, Raymond G.},
	urldate = {2011-10-11},
	date = {2002},
	langid = {english},
	note = {{ArticleType}: research-article / Issue Title: A New Computer-Assisted Literary Criticism? / Full publication date: Aug., 2002 / Copyright © 2002 Springer},
	keywords = {goal\_Interpretation, meta\_Assessing, obj\_DigitalHumanities, obj\_Literature},
}

@article{jannidis_alte_2005,
	title = {Alte Romane und neue Bibliotheken. Zum Projekt eines digitalen historischen Referenzkorpus des Deutschen},
	pages = {139--150},
	journaltitle = {Die innovative Bibliothek. Elmar Mittler zum 65. Geburtstag},
	author = {Jannidis, Fotis and Lauer, Gerhard and Rapp, Andrea},
	date = {2005},
	langid = {german},
	note = {Jannidis, Fotis/ Gerhard Lauer/Andrea Rapp: Alte Romane und neue Bibliotheken. Zum Projekt eines digitalen historischen Referenzkorpus des Deutschen. In: Erand Kolding Nielsen/Klaus G. Saur/Klaus Ceynowa (Hg.): Die innovative Bibliothek. Elmar Mittler zum 65. Geburtstag. München: Saur 2005, S. 139-150.},
	keywords = {goal\_Creation, goal\_Enrichment},
}

@article{meister_alle_1994,
	title = {Alle Erklärung muß fort und nur Beschreibung an ihre Stelle treten. Vor-Überlegungen zu einer computergestützten empirischen Analyse fiktionaler Handlungen},
	journaltitle = {Internationales Jahrbuch für Germanistik 2 (1994), 30-58},
	author = {Meister, Jan Christoph},
	date = {1994},
	langid = {german},
	keywords = {{AnalyzeStatistically}, meta\_Theorizing, obj\_Literature},
}

@article{dalbello_genealogy_2011,
	title = {A genealogy of digital humanities},
	volume = {67},
	issn = {0022-0418},
	url = {http://www.emeraldinsight.com/10.1108/00220411111124550},
	doi = {10.1108/00220411111124550},
	pages = {480--506},
	number = {3},
	journaltitle = {Journal of Documentation},
	shortjournal = {Journal of Documentation},
	author = {Dalbello, Marija},
	urldate = {2011-08-31},
	date = {2011},
	langid = {english},
	keywords = {meta\_Assessing, meta\_GiveOverview, obj\_DigitalHumanities},
}

@collection{hardmeier_ad_2000,
	location = {Amsterdam},
	title = {Ad fontes! Quellen erfassen - lesen - deuten. Was ist Computerphilologie? Ansatzpunkte und Methodologie-Instrumente und Praxis. Contributions to the Conference Computerphilologie 5},
	isbn = {90-5383-677-2},
	publisher = {{VU} University Press},
	editor = {Hardmeier, Christof},
	date = {2000},
	langid = {german},
}

@article{jarvisalo_action_1975,
	title = {Action of propranolol on mitochondrial functions--effects on energized ion fluxes in the presence of valinomycin},
	volume = {24},
	issn = {0006-2952},
	url = {http://www.sciencedirect.com/science/article/pii/000629527590009X},
	abstract = {The effects of propranolol on mitochondrial ion fluxes have been studied in oscillatory conditions in the presence of potassium plus valinomycin. The drug was able to decrease slightly the rate of swelling and that of passive contraction a little more. When the concentration of external salt was below 10 {mM}, propranolol decreased the extent of swelling; above 15 {mM} it was increased by the drug. The inhibition of contraction was abolished by nigericin or raised {pH} and was less evident with {ATP} rather than succinate as the source of energy. Propranolol only slightly affected the oxidative phosphorylation and latent {ATPase} activity of mitochondria. At low concentrations of salts the rate of valinomycin-stimulated respiration and {ATPase} activity were both inhibited. At higher concentrations of the salts, both were stimulated. It is suggested that the stimulation of respiration and {ATPase} activity result from increased ion uptake. Propranolol seems to stabilize an “energized” state, possibly by inhibiting the entry of protons, thereby retarding the equilibration of ionic gradients across the inner membrane.},
	pages = {1701--1705},
	number = {18},
	journaltitle = {Biochemical pharmacology},
	shortjournal = {Biochem. Pharmacol.},
	author = {Järvisalo, J and Saris, N E},
	date = {1975-09-15},
	langid = {english},
}

@inproceedings{vicknair_comparison_2010,
	title = {A comparison of a graph database and a relational database},
	isbn = {9781450300643},
	url = {http://noduslabs.com/research/pathways-meaning-circulation-text-network-analysis/},
	doi = {10.1145/1900008.1900067},
	abstract = {In this work we propose a method and algorithm for identifying the pathways for meaning circulation within a text. This is done by visualizing normalized textual data as a graph and deriving the key metrics for the concepts and for the text as a whole using network analysis. The resulting data and graph representation are then used to detect the key concepts, which function as junctions for meaning circulation within a text, contextual clusters comprised of word communities (themes), as well as the most often used pathways for meaning circulation. We then discuss several practical applications of our method ranging from automatic recovery of hidden agendas within a text and intertextual navigation graph-interfaces, to enhancing reading and writing, quick text summarization, as well as group sentiment profiling and text diagramming. We also make a quick overview of the existing computer-assisted text analysis (and, specifically, network text analysis), and text visualization methods in order to position our research in relation to the other available approaches.},
	pages = {1},
	publisher = {{ACM} Press},
	author = {Vicknair, Chad and Macias, Michael and Zhao, Zhendong and Nan, Xiaofei and Chen, Yixin and Wilkins, Dawn},
	urldate = {2013-05-12},
	date = {2010},
	langid = {english},
}

@article{xiao_academic_2014,
	title = {Academic opinions of Wikipedia and open-access publishing},
	volume = {38},
	issn = {1468-4527},
	pages = {2--2},
	number = {3},
	journaltitle = {Online Information Review},
	author = {Xiao, Lu and Askin, Nicole},
	urldate = {2014-05-14},
	date = {2014-04-29},
	langid = {english},
}

@article{burrows_all_2007,
	title = {All the Way Through: Testing for Authorship in Different Frequency Strata},
	volume = {22},
	url = {http://llc.oxfordjournals.org/content/22/1/27.abstract},
	doi = {10.1093/llc/fqi067},
	shorttitle = {All the Way Through},
	abstract = {This article describes the operation of two new tests of authorship and offers some results. Both tests rely on controlled contrasts of word-frequency and both exclude the very common words, which have been put to such good use in recent years. One test treats of words used with some consistency by a target-author but more sporadically by others. The second treats of words used sporadically by the target-author but not by most others. (The inclusion of words that some other authors use avoids the strict constraint that has impoverished this form of evidence.) In suitable cases, both tests prove very accurate. The fact that evidence of authorship can be detected in these three distinct frequency-strata helps to explain why such tests should work at all and so encourages the development of even better ones.},
	pages = {27 --47},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	author = {Burrows, John},
	urldate = {2011-12-14},
	date = {2007-04},
	langid = {english},
	keywords = {t\_Stylometry},
}

@incollection{ramsay_algorithmic_2008,
	location = {Oxford},
	title = {Algorithmic Criticism},
	isbn = {9781405148641},
	url = {http://www.digitalhumanities.org/companionDLS/},
	series = {Blackwell Companions to Literature and Culture},
	abstract = {A Companion to Digital Literary Studies provides the most thorough single-source exploration to date of the many intersections between literary studies and computation. These intersections are documented and explored by leading scholars, theorists, and practitioners and together they record the history, survey the present, and speculate on the future of this emerging and multi-faceted discipline. Articles are grouped into sections which gather and engage the most important scholarship from the field on topics ranging from scholarly editing and literary criticism, to interactive fiction, multimedia, immersive environments, and beyond.The Companion opens with period-specific surveys and engagements of how computational methods have been applied to a number of areas of literary studies, followed by an overview of the field's major theoretical and methodological perspectives related to new modes of writing and reading enabled by electronic text. Further sections investigate the new genres of electronic literature, including hypertext literature, installations, gaming, and weblogs, while providing an overview of formatting concerns and best practices for digital preservation. This volume is an accessible and practical resource for those who want to understand, use, or create digital literature.},
	booktitle = {Companion to Digital Literary Studies},
	publisher = {Blackwell Publishing Professional},
	author = {Ramsay, Stephen},
	urldate = {2010-02-24},
	date = {2008-12},
	langid = {english},
	note = {Review: http://www.digitalstudies.org/ojs/index.php/digital\_studies/article/view/233/275},
	keywords = {*****, {AnalyzeStatistically}, meta\_Assessing, obj\_Literature},
}

@online{gibbs_conversation_2011,
	title = {A Conversation with Data: Prospecting Victorian Words and Ideas [from: Victorian Studies, 54.1, 2011, 69-77, 10.1353/vic.2011.0146]},
	url = {http://www.dancohen.org/2012/05/30/a-conversation-with-data-prospecting-victorian-words-and-ideas/},
	titleaddon = {Dancohen.org},
	author = {Gibbs, Frederick W. and Cohen, Daniel J.},
	date = {2011},
	langid = {english},
	keywords = {X-{CHECK}},
}

@book{underwood_why_2013,
	location = {Standford},
	title = {Why literary periods mattered: historical contrast and the prestige of English studies},
	isbn = {9780804784467},
	url = {http://www.sup.org/book.cgi?id=22262},
	shorttitle = {Why literary periods mattered},
	abstract = {In the mid-nineteenth century, the study of English literature began to be divided into courses that surveyed discrete "periods." Since that time, scholars' definitions of literature and their rationales for teaching it have changed radically. But the periodized structure of the curriculum has remained oddly unshaken, as if the exercise of contrasting one literary period with another has an importance that transcends the content of any individual course.

Why Literary Periods Mattered explains how historical contrast became central to literary study, and why it remained institutionally central in spite of critical controversy about literature itself. Organizing literary history around contrast rather than causal continuity helped literature departments separate themselves from departments of history. But critics' long reliance on a rhetoric of contrasted movements and fateful turns has produced important blind spots in the discipline. In the twenty-first century, Underwood argues, literary study may need digital technology in particular to develop new methods of reasoning about gradual, continuous change.},
	publisher = {Stanford University Press},
	author = {Underwood, Ted},
	date = {2013},
	langid = {english},
}

@article{drucker_humanities_2011,
	title = {Humanities Approaches to Graphical Display},
	volume = {5},
	url = {http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html},
	abstract = {As digital humanists have adopted visualization tools in their work, they have borrowed methods developed for the graphical display of information in the natural and social sciences. These tools carry with them assumptions of knowledge as observer-independent and certain, rather than observer co-dependent and interpretative. This paper argues that we need a humanities approach to the graphical expression of interpretation. To begin, the concept of data as a given has to be rethought through a humanistic lens and characterized as capta, taken and constructed. Next, the forms for graphical expression of capta need to be more nuanced to show ambiguity and complexity. Finally, the use of a humanistic approach, rooted in a co-dependent relation between observer and experience, needs to be expressed according to graphics built from interpretative models. In summary: all data have to be understood as capta and the conventions created to express observer-independent models of knowledge need to be radically reworked to express humanistic interpretation.},
	number = {1},
	journaltitle = {{DHQ}: Digital Humanities Quarterly},
	author = {Drucker, Johanna},
	urldate = {2013-07-25},
	date = {2011},
	langid = {english},
	keywords = {*****, act\_Visualizing, goal\_Interpretation},
}

@article{cleveland_graphical_1983,
	title = {Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods},
	volume = {79},
	doi = {10.1080/01621459.1984.10478080},
	abstract = {The subject of graphical methods for data analysis and for data presentation needs a scientific foundation. In this article we take a few steps in the direction of establishing such a foundation. Our approach is based on graphical perception—the visual decoding of information encoded on graphs—and it includes both theory and experimentation to test the theory. The theory deals with a small but important piece of the whole process of graphical perception. The first part is an identification of a set of elementary perceptual tasks that are carried out when people extract quantitative information from graphs. The second part is an ordering of the tasks on the basis of how accurately people perform them. Elements of the theory are tested by experimentation in which subjects record their judgments of the quantitative information on graphs. The experiments validate these elements but also suggest that the set of elementary tasks should be expanded. The theory provides a guideline for graph construction: Graphs should employ elementary tasks as high in the ordering as possible. This principle is applied to a variety of graphs, including bar charts, divided bar charts, pie charts, and statistical maps with shading. The conclusion is that radical surgery on these popular graphs is needed, and as replacements we offer alternative graphical forms—dot charts, dot charts with grouping, and framed-rectangle charts.},
	pages = {531--554},
	number = {387},
	journaltitle = {Journal of the American Statistical Association},
	author = {Cleveland, William S. and {McGill}, Robert},
	date = {1983},
	langid = {english},
}

@article{bradley_towards_2012,
	title = {Towards a Richer Sense of Digital Annotation: Moving Beyond a "Media" Orientation of the Annotation of Digital Objects},
	volume = {6},
	url = {http://www.digitalhumanities.org/dhq/vol/6/2/000121/000121.html},
	shorttitle = {Towards a Richer Sense of Digital Annotation},
	abstract = {Digital technology often gives us the chance to re-conceive common scholarly practices with the humanities, and one of these is the practice of annotation. Whereas many in the digital humanities look at annotation through the lens of social media, in this paper we consider annotation’s already established function in scholarship: to support the development of an interpretation of a body of material. It begins by applying a “software application” perspective to annotation and it notes that personal annotation sits at the nexus between the publishing application of the material being annotated, and an interpretation development application that aims to support the reader’s thinking. Once this application orientation is taken up, it becomes evident that it is useful to re-conceptualise aspects of annotation beyond the annotation-of-media focus which the World Wide Web has encouraged in all of us. The paper does this by considering annotation in an application that is not media oriented in nature, Northwestern University’s {WordHoard}, and it explores some of the significance of annotation where the application’s data model – with its inherent semantic significance – is available to be annotated. There is a growing interest in thinking of the {WWW} as a delivery mechanism for software applications rather than merely for documents, and thus many of the issues that this paper raises could apply to the work of web-oriented developers too.},
	number = {2},
	author = {Bradley, John},
	urldate = {2013-07-01},
	date = {2012},
	langid = {english},
	keywords = {act\_Annotating, obj\_AnyObject},
}

@collection{vanhoutte_tei_2010,
	title = {{TEI} by Example},
	url = {http://tbe.kantl.be/TBE/TBE.htm},
	abstract = {{TEI} By Example offers a series of freely available online tutorials walking individuals through the different stages in marking up a document in {TEI} (Text Encoding Initiative).},
	publisher = {Centre for Scholarly Editing and Document Studies ({CTB}) of the Royal Academy of Dutch Language and Literature, the Centre for Computing in the Humanities ({CCH}) of King's College London, and the Department of Information Studies of University College London.},
	editor = {Vanhoutte, Edward and Terras, Melissa and Van den Branden, Ron},
	date = {2010},
	langid = {english},
	keywords = {act\_Modeling, meta\_Teaching, t\_Encoding},
}

@article{rehbein_reading_2013,
	title = {Reading Environments for Genetic Editions},
	volume = {4},
	rights = {{SRC} embraces online publishing and open access to back issues under the Creative Commons Attribution-Noncommercial-No Derivative Works 2.5 Licence. This license allows users to download an article and share it with others as long as authorship and original publication is acknowledged and a link is made (in electronic media) to the original article. The article can be quoted but not changed and presented differently.},
	url = {http://src-online.ca/index.php/src/article/view/123},
	abstract = {This paper discusses the state-of-the-art in digital \&ldquo;genetic\&rdquo; editing, that is the philological analysis (and presentation) of the processes behind the creation of literary texts. Research on such processes is mainly based on draft manuscripts or typescripts that authors have left behind intentionally or accidentally. Creative note-taking, revisions, proof-readings, cross-linkings and additional material makes them a complex and interwoven set of data requiring specific analytic tools and reading and research environments for both general and specialist readers and users to understand them better.   The paper traces the idea of pre-electronic genetic editing and the significant changes it is undergoing in the digital era. It compares two editorial projects on renowned authors, one in print and one digital: the so-called \&lsquo;Frankfurt edition\&rsquo; of Friedrich H\&ouml;lderlin, and the Samuel Beckett Digital Manuscript Project. The paper discusses these in particular as \&ldquo;reading environments\&rdquo; (or user interfaces) designed for \&ldquo;critically experiencing\&rdquo; authorial writing processes in both the print and the digital medium.},
	number = {3},
	journaltitle = {Scholarly and Research Communication},
	author = {Rehbein, Malte and Gabler, Hans Walter},
	urldate = {2014-03-05},
	date = {2013-12-16},
	langid = {english},
	keywords = {goal\_Enrichment, obj\_Text},
}

@inproceedings{grassi_pundit:_2012,
	title = {Pundit: Semantically Structured Annotations for Web Contents and Digital Libraries},
	url = {http://ceur-ws.org/Vol-912/paper4.pdf},
	shorttitle = {Pundit},
	abstract = {This paper introduces Pundit
3
: a novel semantic annotation
tool that allows users to create structured data while annotating Web
pages relying on stand-off mark-up techniques. Pundit provides support
for different types of annotations, ranging from simple comments to se-
mantic links to Web of data entities and fine granular cross-references
and citations. In addition, it can be configured to include custom con-
trolled vocabularies and has been designed to enable groups of users to
share their annotations and collaboratively create structured knowledge.
Pundit allows creating semantically typed relations among heterogeneous
resources, both having different multimedia formats and belonging to dif-
ferent pages and domains. In this way, annotations can reinforce existing
data connections or create new ones and augment original information
generating new semantically structured aggregations of knowledge. These
can later be exploited both by other users to better navigate {DL} and Web
content, and by applications to improve data management.},
	eventtitle = {{SDA}},
	pages = {49--60},
	booktitle = {Proceedings of the 2nd International Workshop on Semantic Digital Archives ({SDA} 2012)},
	author = {Grassi, Marco and Morbidoni, Christian and Nucci, Michele and Fonda, Simone and Ledda, Giovanni},
	urldate = {2013-05-09},
	date = {2012},
	langid = {english},
	keywords = {act\_Annotating},
}

@book{kepper_musikedition_2011,
	location = {Norderstedt},
	title = {Musikedition im Zeichen neuer Medien : historische Entwicklung und gegenwärtige Perspektiven musikalischer Gesamtausgaben},
	isbn = {9783844800760 384480076X},
	url = {http://ifb.bsz-bw.de/bsz36848601Xrez-1.pdf},
	shorttitle = {Musikedition im Zeichen neuer Medien},
	abstract = {Die Keimzelle der Musikwissenschaft als geisteswissenschaftlicher Disziplin liegt in den Bemühungen des 19. Jahrhunderts, die Werke herausragender Komponisten zu konservieren und einer breiteren Öffentlichkeit zu erschließen. In diesem Umfeld erschien im Jahr 1851 der erste Band der Bach-Gesamtausgabe, herausgegeben von der Leipziger Bachgesellschaft. Alle nachfolgenden Musiker-Ausgaben entwickelten sich auf dieser Basis und reizten die Möglichkeiten des Buchmediums in zunehmenden Maße aus. Seit etwa zehn Jahren wird versucht, das Potential digitaler Medien für die Musikphilologie zu erschließen. Ausgehend von der Geschichte musikwissenschaftlicher Ausgaben und einer kritischen Reflektion des bisher Geleisteten, weist dieser Band mögliche neue Perspektiven für zukünftige, dem neuen Medium angemessene Editionsformen auf.},
	pagetotal = {416},
	publisher = {Books on Demand},
	author = {Kepper, Johannes},
	date = {2011},
	langid = {german},
	keywords = {goal\_Enrichment, meta\_Theorizing, obj\_Music, obj\_SheetMusic, t\_Encoding},
}
@article{gorz_kognitive_2007,
	title = {Kognitive Karten des Mittelalters: Digitale Erschließung mittelalterlicher Weltkarten.},
	volume = {10},
	url = {http://wwwdh.cs.fau.de/IMMD8/staff/Goerz/mappae2006jb.pdf},
	shorttitle = {Kognitive Karten des Mittelalters},
	abstract = {Mittelalterliche Weltkarten sind in erster Linie als kognitive Karten zu verstehen. Um den
Bestand und das Ver
̈
anderungspotential des Text-Bild-Materials zu erfassen und zu erschlie-
ßen, wird im Rahmen eines interdisziplin
̈
aren Projekts in Erlangen ein systematischer verglei-
chender Stellenkatalog mittelalterlicher und fr
̈
uhneuzeitlicher Weltkarten erarbeitet. In einem
ersten Schritt wurde eine multimediale Datenbank aus hochaufgel
̈
osten digitalen Bildern cir-
ca 300 repr
̈
asentativer Weltkarten aufgebaut, die durch zugeordnete Metadaten erschlossen
sind. Die Datenbank wird erg
̈
anzt durch spezielle Software-Werkzeuge zur Bildbearbeitung
und zum Bildvergleich. Mit diesen Mitteln wird der Stellenkatalog erarbeitet, der alle Posi-
tionen umfassen soll, die auf den Mappaemundi des 13.–16. Jahrhunderts mit Bildern, Legen-
den und Bild-Text-Kombinationen verzeichnet sind. Als begriffliche Grundlage hierf
̈
ur dient
eine formale Ontologie, die zur Zeit in der beschreibungslogischen
”
Web Ontology Language“
{OWL}-{DL} formuliert wird; sie erweitert eine zun
̈
achst f
̈
ur den Behaim-Globus erstellte Konzept-
hierarchie und wird in das
”
Conceptual Reference Model“ der {ICOM}-{CIDOC} (International
Committee for Documentation of the International Council of Museums) eingebettet. Diese Art
der Formalisierung erm
̈
oglicht komplexe Anfragen, die weit
̈
uber die M
̈
oglichkeiten herk
̈
omm-
licher Datenbanken hinausgehen. Fernziel des Projekts ist die Ausarbeitung der in den Karten
dargestellten kognitiven Beziehungen und deren Wandel auf der Grundlage des Stellenkata-
logs.},
	journaltitle = {Historisches Forum},
	author = {Görz, Günther},
	date = {2007},
	langid = {german},
	keywords = {act\_Annotating, goal\_Enrichment, obj\_Maps},
}

@misc{thanos_global_2011,
	title = {Global Scientific Data Infrastructures: The {GRDI}2020 Vision},
	url = {http://www.grdi2020.eu/Repository/FileScaricati/fc14b1f7-b8a3-41f8-9e1e-fd803d28ba76.pdf},
	publisher = {{GRDI}2020 / European Commission},
	author = {Thanos, Costantino},
	date = {2011-12},
	langid = {english},
	keywords = {goal\_Storage, meta\_Advocating, obj\_Data, obj\_Data/Databases, obj\_Infrastructures},
}

@book{tomasi_metodologie_2008,
	location = {Roma},
	title = {Metodologie informatiche e discipline umanistiche},
	isbn = {9788843043033},
	url = {http://books.google.it/books?id=yG_kPAAACAAJ},
	series = {Manuali universitari},
	abstract = {Quale vincolo lega le discipline umanistiche all'informatica come scienza dell'informazione? Quale rapporto sussiste fra le "humanae litterae" e una disciplina che si occupa di forme di rappresentazione, trattamento ed elaborazione dell'informazione? Il connubio non coinvolge i soli aspetti strumentali di uso della macchina informatica. Obiettivo del volume è ragionare su aspetti e settori della ricerca umanistica che possono essere ripensati alla luce dei presupposti teorici, metodologici e tecnici della scienza dell'informazione: dal concetto di informazione alla sua formalizzazione, dalle reti ai sistemi di basi di dati, dai linguaggi di markup agli ipertesti e ai sistemi multimediali. Ma è soprattutto sull'elaborazione dell'informazione che si concentra la "computabilità umanistica": dal trattamento automatico del linguaggio naturale al Web semantico, dai metadati alle biblioteche digitali. Fonte dell'abstract: http://books.google.it/books?id={yG}\_kPAAACAAJ},
	publisher = {Carocci},
	author = {Tomasi, Francesca},
	date = {2008},
	langid = {italian},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@online{spiro_digital_2010,
	title = {Digital Humanities Education [Bibliography]},
	url = {http://www.zotero.org/groups/digital_humanities_education},
	abstract = {Examines different approaches to digital humanities education.  Includes syllabi and curriculum planning documents, as well as as articles about open education, networked pedagogies, and more.  Members are encouraged to help build the collection.},
	titleaddon = {Zotero.org},
	author = {Spiro, Lisa},
	date = {2010},
	keywords = {act\_Teaching/Learning},
}

@article{zarsky_transparent_2013,
	title = {Transparent Predictions},
	url = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID2324240_code499135.pdf?abstractid=2324240&mirid=1},
	abstract = {The growing use of predictive practices is generating serious concerns regarding the lack of transparency. Although echoed across the policy, legal, and academic debate, the nature of transparency, in this context, is unclear. Transparency flows from different, even competing, rationales, as well as very different legal and philosophical backgrounds. This Article sets forth a unique and comprehensive conceptual framework for understanding the role transparency must play as a regulatory concept in the crucial and innovative realm of automated predictive modeling.},
	number = {4},
	journaltitle = {University of Illinois Law Review},
	author = {Zarsky, Tal},
	date = {2013},
	langid = {english},
	keywords = {act\_Modeling, goal\_Interpretation, obj\_AnyObject},
}

@online{murtagh_history_2013,
	title = {History of the Rising, in our own words},
	url = {http://www.irishtimes.com/culture/heritage/history-of-the-rising-in-our-own-words-1.1580994},
	abstract = {A project at Trinity College Dublin invites people to share family stories and documents and help put ordinary faces on the tumult of the early 20th century},
	titleaddon = {Irish Times},
	author = {Murtagh, Peter},
	urldate = {2013-11-03},
	date = {2013},
	langid = {english},
	keywords = {goal\_Capture, obj\_Letters},
}

@article{rice_open_2013,
	title = {Open access publishing hoax: what Science magazine got wrong},
	url = {http://www.theguardian.com/higher-education-network/blog/2013/oct/04/science-hoax-peer-review-open-access},
	shorttitle = {Open access publishing hoax},
	abstract = {The sting operation on publishers doesn't point to the real crisis, says Curt Rice – the meltdown of the peer review system},
	journaltitle = {The Guardian},
	author = {Rice, Curt},
	urldate = {2013-10-06},
	date = {2013},
	langid = {english},
	keywords = {act\_Publishing, goal\_Dissemination, meta\_Assessing, obj\_Research},
}

@collection{hall_digital_2011,
	title = {The Digital Humanities: Beyond Computing},
	isbn = {1465-4121},
	url = {http://www.culturemachine.net/index.php/cm/issue/view/23},
	series = {Culture Machine},
	abstract = {Just as interesting as what computer science has to offer the humanities is the question of what the humanities have to offer computer science; what the humanities themselves can bring to the understanding of computing and the shaping of the digital. Do the humanities really need to draw quite so heavily on computer science to develop a sense of their identity and role in an era of digital media technology? Along with a computational turn in the humanities, might we not also benefit from more of a humanities turn in our understanding of the computational and the digital? 


Not just to what extent is it possible for the emerging digital humanities to go beyond the disciplinary objects, affiliations, assumptions and methodological practices of computing science and engineering, science and technology, or even science in general; but to what extent is it possible for the emerging digital humanities to go beyond the human-ities too?},
	volumes = {12},
	publisher = {Gary Hall et al.},
	editor = {Hall, Gary},
	date = {2011},
	langid = {english},
	keywords = {meta\_Theorizing, obj\_DigitalHumanities},
}

@online{gibbs_coding_2011,
	title = {Coding in the humanities},
	url = {http://historyproef.org/blog/teaching/coding-in-the-humanities/},
	titleaddon = {historyproef},
	author = {Gibbs, Frederick W.},
	urldate = {2011-08-05},
	date = {2011},
	langid = {english},
}

@online{beieler_animated_2013,
	title = {Animated Protest Mapping (1979-2013)},
	url = {http://johnbeieler.org/blog/2013/07/31/animated-protest-mapping/},
	abstract = {Many people, including myself, were interested in how protest behavior changes over time. I created an animated protest map (events of every type over time).},
	titleaddon = {john Beieler},
	type = {Blog},
	author = {Beieler, John},
	date = {2013},
	langid = {english},
	note = {Political Science},
	keywords = {meta\_GiveOverview, obj\_Events, obj\_Maps},
}

@article{dieter_virtues_2014,
	title = {The Virtues of Critical Technical Practice},
	volume = {25},
	pages = {216--230},
	number = {1},
	journaltitle = {differences},
	author = {Dieter, Michael},
	date = {2014},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{schanze_computer-unterstutzte_1969,
	title = {Computer-unterstützte Literaturwissenschaft. Probleme und Perspektiven im Zusammenhang mit dem maschinell erstellten Kleist-Index},
	pages = {315--321},
	number = {79},
	journaltitle = {Muttersprache},
	author = {Schanze, Helmut},
	date = {1969},
	keywords = {goal\_Analysis, goal\_Enrichment, obj\_Text},
}

@article{epstein_why_2008,
	title = {Why Model?},
	volume = {11},
	rights = {{JASSS}@soc.surrey.ac.uk},
	url = {http://jasss.soc.surrey.ac.uk/11/4/12.html},
	abstract = {This lecture treats some enduring misconceptions about modeling. One of these is that the goal is always prediction. The lecture distinguishes between explanation and prediction as modeling goals, and offers sixteen reasons other than prediction to build a model. It also challenges the common assumption that scientific theories arise from and 'summarize' data, when often, theories precede and guide data collection; without theory, in other words, it is not clear what data to collect. Among other things, it also argues that the modeling enterprise enforces habits of mind essential to freedom.  It is based on the author's 2008 Bastille Day keynote address to the Second World Congress on Social Simulation, George Mason University, and earlier addresses at the Institute of Medicine, the University of Michigan, and the Santa Fe Institute.},
	number = {4},
	journaltitle = {Journal of Artificial Societies and Social Simulation},
	author = {Epstein, Joshua M.},
	urldate = {2014-06-09},
	date = {2008},
	langid = {english},
	keywords = {act\_Modeling},
}

@article{lennon_digital_nodate,
	title = {The Digital Humanities and National Security},
	volume = {25},
	issn = {1040-7391},
	url = {https://www.academia.edu/6876563/The_Digital_Humanities_and_National_Security},
	doi = {doi:10.1215/10407391-2420027},
	series = {differences},
	abstract = {A geneaology and discussion to the question “Should {DHers} accept military/
defense funding?,” conducted during July 2011 on the “Digital Humanities
Questions and Answers” question by M. Kirschenbaum.},
	pages = {132--155},
	number = {1},
	author = {Lennon, Brian},
	langid = {english},
	keywords = {obj\_Code, obj\_DigitalHumanities, obj\_Institutions, obj\_Language},
}

@article{kirschenbaum_.txtual_2013,
	title = {The .txtual Condition: Digital Humanities, Born-Digital Archives, and the Future Literary},
	volume = {7},
	url = {http://digitalhumanities.org/dhq/vol/7/1/000151/000151.html},
	abstract = {In 1995 in the midst of the first widespread wave of digitization, the Modern Language Association issued a Statement on the Significance of Primary Records in order to assert the importance of retaining books and other physical artifacts even after they have been microfilmed or scanned for general consumption. "A primary record," the {MLA} told us then, "can appropriately be defined as a physical object produced or used at the particular past time that one is concerned with in a given instance" (27). Today, the conceit of a "primary record" can no longer be assumed to be coterminous with that of a "physical object." Electronic texts, files, feeds, and transmissions of all sorts are also now, indisputably, primary records. In the specific domain of the literary, a writer working today will not and cannot be studied in the future in the same way as writers of the past, because the basic material evidence of their authorial activity — manuscripts and drafts, working notes, correspondence, journals — is, like all textual production, increasingly migrating to the electronic realm. This essay therefore seeks to locate and triangulate the emergence of a .txtual condition — I am of course remediating Jerome {McGann}’s influential notion of a “textual condition” — amid our contemporary constructions of the "literary", along with the changing nature of literary archives, and lastly activities in the digital humanities as that enterprise is now construed. In particular, I will use the example of the Maryland Institute for Technology in the Humanities ({MITH}) at the University of Maryland as a means of illustrating the kinds of resources and expertise a working digital humanities center can bring to the table when confronted with the range of materials that archives and manuscript repositories will increasingly be receiving.},
	number = {7},
	journaltitle = {Digital Humanities Quarterly},
	author = {Kirschenbaum, Matthew},
	date = {2013},
	keywords = {act\_Archiving, meta\_Theorizing, obj\_Artefacts},
}

@incollection{ramsay_developing_2012,
	location = {Minneapolis},
	title = {Developing Things: Notes toward an Epistemology of Building in the Digital Humanities},
	booktitle = {Debates in the Digital Humanities},
	publisher = {Univ. of Minnesota Press},
	author = {Ramsay, Stephen and Rockwell, Geoffrey},
	editor = {Gold, Matthew K.},
	date = {2012},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}
@online{jockers_so_2014,
	title = {So What?},
	url = {http://www.matthewjockers.net/2014/05/07/so-what/},
	shorttitle = {So What?},
	abstract = {What matters to me, and I think what should matter to most of us is the work itself, and I believe, perhaps naively, that the value of the work is, or should be, self-evident. The answer to the question of “so what?” should be obvious. Unfortunately, it is not always obvious, especially to readers likes Kirsch who are not working in the sub fields of this massive big tent we have come to call “digital humanities” (and for the record, I do despise that term for its lack of specificity). Kirsch and others inevitably gravitate to the most easily accessible and generalized resources often avoiding or missing some of the best work in the field.

“So what?” is, of course, the more informal and less tactful way of asking what one sometimes hears (or might wish to hear) asked after an academic paper given at the Digital Humanities conference, e.g. “I was struck by your use of latent Dirichlet allocation, but where is the new knowledge gained from your analysis?”},
	titleaddon = {matthewjockers.net},
	author = {Jockers, Matthew},
	urldate = {2014-05-07},
	date = {2014-05-07},
	langid = {english},
}

@collection{turow_hyperlinked_2008,
	location = {Ann Arbor},
	title = {The Hyperlinked Society: Questioning Connections in the Digital Age},
	url = {http://hdl.handle.net/2027/spo.5680986.0001.001},
	shorttitle = {The Hyperlinked Society},
	publisher = {Univ. of Michigan Press},
	editor = {Turow, Joseph and Tsui, Lokman},
	date = {2008},
	langid = {english},
	keywords = {X-{CHECK}},
}

@book{rettberg_blogging_2013,
	edition = {Second Edition},
	title = {Blogging},
	isbn = {0745641342},
	url = {http://books.google.com/books/about/Blogging.html?id=lo0WUF0YpsMC},
	series = {Digital Media and Society},
	abstract = {Blogging has profoundly influenced not only the nature of the internet today, but also the nature of modern communication, despite being a genre invented less than a decade ago. This book–length study of a now everyday phenomenon provides a close look at blogging while placing it in a historical, theoretical and contemporary context. Scholars, students and bloggers will find a lively survey of blogging that contextualises blogs in terms of critical theory and the history of digital media. Authored by a scholar–blogger, the book is packed with examples that show how blogging and related genres are changing media and communication. It gives definitions and explains how blogs work, shows how blogs relate to the historical development of publishing and communication and looks at the ways blogs structure social networks and at how social networking sites like {MySpace} and Facebook incorporate blogging in their design. Specific kinds of blogs discussed include political blogs, citizen journalism, confessional blogs and commercial blogs.},
	pagetotal = {192},
	publisher = {Polity Press},
	author = {Rettberg, Jill Walker},
	date = {2013},
	langid = {english},
	keywords = {act\_Publishing, goal\_Dissemination, obj\_ResearchResults},
}

@article{suarez_digital_2014,
	title = {Digital Humanities in Spanish? (¿Humanidades digitales en español?, 2010)},
	url = {http://dayofdh2014.matrix.msu.edu/redhd/2014/04/08/digital-humanities-in-spanish/},
	journaltitle = {{RedHD} en Traducción},
	author = {Suárez, Juan Luis},
	translator = {Ortega, Élika},
	date = {2014},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@article{galina_what_2014,
	title = {What Are The Digital Humanities? (¿Qué son las humanidades digitales?, 2011)},
	url = {http://dayofdh2014.matrix.msu.edu/redhd/2014/04/08/what-are-the-digital-humanities/},
	journaltitle = {{RedHD} en Traducción},
	author = {Galina, Isabel},
	translator = {Thompson, Tim},
	date = {2014},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{stinson_amazing_2014,
	title = {An Amazing Discovery: Andy Warhol’s Groundbreaking Computer Art},
	url = {http://www.wired.com/2014/04/an-amazing-discovery-andy-warhols-seminal-computer-art/},
	shorttitle = {An Amazing Discovery},
	abstract = {Until now, it was unknown if Warhol had made any digital artworks on his own time.},
	journaltitle = {Wired.com},
	author = {Stinson, Liz},
	urldate = {2014-04-28},
	date = {2014-04-28},
	keywords = {act\_Preservation, obj\_Data},
}
@article{gershman_exploitative_2014,
	title = {The Exploitative Economics Of Academic Publishing},
	url = {http://footnote1.com/the-exploitative-economics-of-academic-publishing/},
	abstract = {How journal publishers are profiting at the expense of taxpayers, researchers, and universities.},
	journaltitle = {Footnote1},
	author = {Gershman, Samuel},
	urldate = {2014-04-24},
	date = {2014-04-18},
	langid = {english},
	keywords = {act\_Publishing, goal\_Dissemination, obj\_ResearchResults, t\_OpenAccess},
}

@online{riddell_simple_2012,
	title = {A Simple Topic Model (Mixture of Unigrams)},
	url = {http://ariddell.org/weblog/2012/07/22/simple-topic-model/},
	abstract = {{NB}: This is an extended version of the appendix of my paper exploring trends in German Studies in the {US} between 1928 and 2006. In that paper I used a topic model (Latent Dirichlet Allocation); this tutorial is intended to help readers understand how {LDA} works.},
	titleaddon = {ariddell.org},
	author = {Riddell, Allen Beye},
	date = {2012-07-22},
	langid = {english},
	keywords = {act\_ContentAnalysis, goal\_Analysis, t\_TopicModeling},
}

@article{oconnell_bright_2014,
	location = {New York},
	title = {Bright Lights, Big Data},
	url = {http://www.newyorker.com/online/blogs/books/2014/03/bright-lights-big-data.html},
	journaltitle = {The New Yorker},
	author = {O'Connell, Mark},
	date = {2014-03-20},
	langid = {english},
	keywords = {act\_ContentAnalysis, bigdata{\textasciitilde}, goal\_Analysis, obj\_Text},
}

@unpublished{dh_summer_school_berne_2013,
	location = {Berne},
	title = {Berne {DH} Summer School declaration on research ethics in Digital Humanities},
	url = {https://docs.google.com/document/d/1A4MJ05qS0WhNlLdlozFV3q3Sjc2kum5GQ4lhFoNKcYU/edit?pli=1#},
	abstract = {This document is aimed as a contribution to the current debates in the digital humanities about how digital humanists conduct themselves as professionals ethically, and as a reflection of their core values. It does not (yet) have any formal authority but is instead aimed to part of a conversation in the digital humanities about what this could entail.},
	pagetotal = {12},
	author = {{DH Summer School}},
	date = {2013},
	langid = {english},
	keywords = {meta\_Theorizing, obj\_DigitalHumanities},
}

@article{gumbrecht_denken_2014,
	location = {Frankfurt},
	title = {Das Denken muss nun auch den Daten folgen},
	url = {http://www.faz.net/aktuell/feuilleton/geisteswissenschaften/geisteswissenschaften-das-denken-muss-nun-auch-den-daten-folgen-12840532.html?printPagedArticle=true},
	abstract = {Die Geisteswissenschaften reagieren ratlos auf die digitale Revolution. Doch die verändert die Welt und die Art, wie wir uns selbst erleben, dramatisch. Es ist überlebenswichtig, diesen Wandel mit neuen Begriffen fassen und beeinflussen zu können.},
	journaltitle = {{FAZ}.net},
	author = {Gumbrecht, Hans-Ulrich},
	date = {2014-03-12},
	langid = {german},
	keywords = {meta\_Advocating, meta\_Assessing, obj\_DigitalHumanities},
}
@report{carusi_virtual_2010,
	location = {London},
	title = {Virtual research environment collaborative landscape study},
	rights = {Copyright Higher Education Funding Council for England ({HEFCE}), on behalf of the Joint Information Systems Commmittee ({JISC}), unless explicitly acknowledged otherwise.},
	url = {http://www.jisc.ac.uk/publications/reports/2010/vrelandscapestudy.aspx},
	abstract = {This study investigated international developments in Virtual Research Communities ({VRCs}) and to evaluate them in relation to the activities in the {JISC}’s {VRE} programme. The study examined programmes in a number of key countries along with significant projects and communities as well as some countries where developments on this front are just beginning. There has been a great deal of activity over the past few years in terms of prototype and demonstration systems moving into the mainstream of research practice. Notable trends are emerging as researchers increasingly apply collaborative systems to everyday research tasks.},
	institution = {{JISC}},
	author = {Carusi, Annamaria and Reimer, Torsten},
	urldate = {2011-05-20},
	date = {2010},
	langid = {english},
	keywords = {goal\_Collaboration, obj\_Infrastructures, obj\_VREs},
}

@report{kommission_zukunft_der_informationsinfrastruktur_gesamtkonzept_2011,
	location = {Bonn},
	title = {Gesamtkonzept für die Informationsinfrastruktur in Deutschland},
	url = {http://www.allianzinitiative.de/fileadmin/user_upload/KII_Gesamtkonzept.pdf},
	abstract = {Empfehlungen der Kommission Zukunft der Informationsinfrastruktur im Auftrag der
Gemeinsamen Wissenschaftskonferenz des Bundes und der Länder},
	institution = {Gemeinsame Wissenschaftskonferenz},
	author = {Kommission Zukunft der Informationsinfrastruktur},
	date = {2011-04},
	langid = {german},
	keywords = {meta\_DefinePolicy, obj\_VREs},
}

@article{benardou_understanding_2010,
	title = {Understanding the Information Requirements of Arts and Humanities Scholarship},
	volume = {5},
	rights = {Copyright for papers and articles published in this journal is retained by the authors, with first publication rights granted to the University of Edinburgh. It is a condition of publication that authors license their paper or article under a  Creative Commons Attribution Licence .},
	issn = {1746-8256},
	url = {http://www.ijdc.net/index.php/ijdc/article/view/144},
	doi = {10.2218/ijdc.v5i1.141},
	abstract = {This paper reports on research of scholarly research practices and requirements conducted in the context of the Preparing {DARIAH} European e-Infrastructures project, with a view to ensuring current and future fitness for purpose of the planned digital infrastructure, services and tools. It summarises the findings of earlier research, primarily from the field of human information behaviour as applied in scholarly work, it presents a conceptual perspective informed by cultural-historical activity theory, it introduces briefly a formal conceptual model for scholarly research activity compliant with {CIDOC} {CRM}, it describes the plan of work and methodology of an empirical research project based on open-questionnaire interviews with arts and humanities researchers, and presents illustrative examples of segmentation, tagging and initial conceptual analysis of the empirical evidence. Finally, it presents plans for future work, consisting, firstly, of a comprehensive re-analysis of interview segments within the framework of the scholarly research activity model, and, secondly, of the integration of this analysis with the extended digital curation process model we presented in earlier work.},
	pages = {18--33},
	number = {1},
	journaltitle = {International Journal of Digital Curation},
	author = {Benardou, Agiatis and Constantopoulos, Panos and Dallas, Costis and Gavrilis, Dimitris},
	urldate = {2014-01-31},
	date = {2010-06-22},
	langid = {english},
	keywords = {act\_Modeling, act\_Theorizing, meta\_Assessing, obj\_Research},
}

@inproceedings{pugin_optical_2006,
	location = {Victoria, Canada},
	title = {Optical Music Recognition of Early Typographic Prints using Hidden Markov Models},
	url = {http://www.aruspix.net/publications/pugin06optical.pdf},
	abstract = {Music printed with movable type (typographic music) from
the 16th and 17th centuries contains specific graphic features.
In this paper, we present a technique and associated
experiments for performing optical music recognition
on such music prints using {HiddenMarkovModels} ({HMM}).
Our original approach avoids the difficult and unreliable removal
of staff lines usually required before processing. The
modeling of symbols on the staff is based on low-level simple
features. We show that, using our technique, these features
are robust enough to obtain good recognition rates even
with poor quality images scanned from microfilm of originals.
The music content retrieved by the optical recognition
process can be put to significant use in, for example, the
creation of searchable digital music libraries.},
	eventtitle = {7th International Conference on Music Information Retrieval ({ISMIR}’06)},
	author = {Pugin, Laurent},
	date = {2006},
	langid = {english},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
}

@inproceedings{pugin_reducing_2007,
	location = {Budapest, Hungary},
	title = {Reducing Costs for Digitising Early Music with Dynamic Adaptation},
	url = {http://www.aruspix.net/publications/pugin07reducing.pdf},
	abstract = {Optical music recognition ({OMR}) enables librarians to digi-
tise early music sources on a large scale. The cost of expert human
labour to correct automatic recognit
ion errors dominates the cost of such
projects. To reduce the number of recognition errors in the {OMR} pro-
cess, we present an innovative approach to adapt the system dynamically,
taking advantage of the human editing work that is part of any digitisa-
tion project. The corrected data are used to perform {MAP} adaptation,
a machine-learning technique used previously in speech recognition and
optical character recognition ({OCR}). Our experiments show that this
technique can reduce editing costs by more than half.},
	pages = {471--74},
	booktitle = {In Proceedings of the European Conference on Digital Libraries ({ECDL} 2007)},
	author = {Pugin, Laurent and Burgoyne, J. A. and Fujinaga, I.},
	date = {2007},
	langid = {english},
	keywords = {goal\_Capture, obj\_SheetMusic},
}

@article{rebelo_optical_2012,
	title = {Optical music recognition: state-of-the-art and open issues},
	url = {http://download.springer.com/static/pdf/527/art%253A10.1007%252Fs13735-012-0004-6.pdf?auth66=1386596381_fe5f560bc7d93233c0ad75e5b7607b50&ext=.pdf},
	doi = {10.1007/s13735-012-0004-6},
	series = {1},
	abstract = {For centuries, music has been shared and remembered by two traditions: aural transmission and in the form of written documents normally called musical scores. Many of these scores exist in the form of unpublished manuscripts and hence they are in danger of being lost through the normal ravages of time. To preserve the music some form of typesetting or, ideally, a computer system that can automatically decode the symbolic images and create new scores is required. Programs analogous to optical character recognition systems called optical music recognition ({OMR}) systems have been under intensive development for many years. However, the results to date are far from ideal. Each of the proposed methods emphasizes different properties and therefore makes it difficult to effectively evaluate its competitive advantages. This article provides an overview of the literature concerning the automatic analysis of images of printed and handwritten musical scores. For self-containment and for the benefit of the reader, an introduction to {OMR} processing systems precedes the literature overview. The following study presents a reference scheme for any researcher wanting to compare new {OMR} algorithms against well-known ones.},
	pages = {173--190},
	author = {Rebelo, Ana and Fujinaga, Ichiro and Paszkiewicz, Filipe and Marcal, Andre R. S. and Guedes, Carlos and Cardoso, Jamie S.},
	date = {2012},
	langid = {english},
	keywords = {act\_DataRecognition, meta\_GiveOverview, obj\_SheetMusic},
}

@inproceedings{pugin_map_2007,
	location = {Vienna, Austria},
	title = {{MAP} Adaptation to Improve Optical Music Recognition of Early Music Documents Using Hidden Markov Models},
	url = {http://www.aruspix.net/publications/pugin07map.pdf},
	abstract = {Despite steady improvement in optical music recognition
({OMR}), early documents remain challenging because of
the high variability in their contents. In this paper, we
present an original approach using maximum a posteriori
({MAP}) adaptation to improve an {OMR} tool for early
typographic prints dynamically based on hidden Markov
models. Taking advantage of the fact that during the normal
usage of any {OMR} tool, errors will be corrected, and
thus ground-truth produced, the system can be adapted in
real-time. We experimented with five 16th-century music
prints using 250 pages of music and two procedures in
applying {MAP} adaptation. With only a handful of pages,
both recall and precision rates improved even when the
baseline was above 95 percent.},
	pages = {513--16},
	booktitle = {In Proceedings of the 8th International Conference on Music Information Retrieval ({ISMIR} 2007)},
	author = {Pugin, Laurent and Burgoyne, J. A. and Fujinaga, I.},
	date = {2007},
	langid = {english},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
}

@inproceedings{pugin_goal-directed_2007,
	location = {Vancouver, Canada},
	title = {Goal-Directed Evaluation for the Improvement of Optical Music Recognition on Early Music Prints},
	url = {http://www.aruspix.net/publications/pugin07goal-directed.pdf},
	abstract = {Optical music recognition ({OMR}) systems are promising
tools for the creation of searchable digital music libraries.
Using an adaptive {OMR} system for early music prints based
on hidden Markov models, we leverage an edit-distance evaluation
metric to improve recognition accuracy. Baseline results
are computed with new labeled training and test sets
drawn from a diverse group of prints. We present two experiments
based on this evaluation technique. The first resulted
in a significant improvement to the feature extraction function
for these images. The second is a goal-directed comparison
of several popular adaptive binarization algorithms,
which are often evaluated only subjectively. Accuracy increased
by as much as 55\% for some pages, and the experiments
suggest several avenues for further research.},
	pages = {303--04},
	booktitle = {In Proceedings of the {ACM}-{IEEE} Joint Conference on Digital Libraries ({JCDL} 2007)},
	author = {Pugin, Laurent and Burgoyne, John Ashley and Fujinaga, Ichiro},
	date = {2007},
	langid = {english},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
}

@incollection{pugin_gamera_2008,
	location = {Philadelphia, {PA}},
	title = {Gamera Versus Aruspix: Two Optical Music Recognition Approaches},
	url = {http://www.aruspix.net/publications/pugin08aruspix.pdf},
	abstract = {Optical music recognition ({OMR}) applications are predominantly
designed for common music notation and as such,
are inherently incapable of adapting to specialized notation
forms within early music. Two {OMR} systems, namely Gamut
(a Gamera application) and Aruspix, have been proposed
for early music. In this paper, we present a novel
comparison of the two systems, which use markedly different
approaches to solve the same problem, and pay close
attention to the performance and learning rates of both applications.
In order to obtain a complete comparison of Gamut
and Aruspix, we evaluated the core recognition systems and
the pitch determination processes separately. With our experiments,
we were able to highlight the advantages of both
approaches as well as causes of problems and possibilities
for future improvements.},
	pages = {419--424},
	booktitle = {Proceedings of the 9th International Conference on Music Information Retrieval ({ISMIR} 2008)},
	author = {Pugin, Laurent and Hockman, J. and Burgoyne, J. A. and Fujinaga, I.},
	date = {2008},
	langid = {english},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
}

@inproceedings{burgoyne_enhanced_2008,
	location = {Philadelphia, {PA}},
	title = {Enhanced Bleedthrough Correction for Early Music Documents with Recto-Verso Registration},
	url = {http://www.aruspix.net/publications/bourgoyne08enhanced.pdf},
	abstract = {Ink bleedthrough is common problem in early music documents.
Even when such bleedthrough does not pose problems
for human perception, it can inhibit the performance
of optical music recognition ({OMR}). One way to reduce
the amount of bleedthrough is to take into account what is
printed on the reverse of the page. In order to do so, the
reverse of the page must be registered to match the front of
the page on a pixel-by-pixel basis. This paper describes our
approach to registering scanned early music scores as well
as our modifications to two robust binarization approaches
to take into account bleedthrough and the information available
from the registration process. We determined that although
the information from registration itself often makes
little difference in recognition performance, other modifications
to binarization algorithms for correcting bleedthrough
can yield dramatic increases in {OMR} results.},
	pages = {407--12},
	booktitle = {Proceedings of the 9th International Conference on Music Information Retrieval ({ISMIR} 2008)},
	author = {Burgoyne, John Ashley and Devaney, Johanna and Pugin, Laurent and Fujinaga, Ichiro},
	date = {2008},
	langid = {english},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
}

@incollection{ouyang_complex_2009,
	location = {Montreal, {QC}, Canada},
	title = {Complex Layout Analysis of Medieval Music Manuscripts for Information Extraction and Optical Recognition},
	pages = {101--104},
	booktitle = {In Proceedings of the 2009 International Computer Music Conference ({ICMC} 2009)},
	author = {Ouyang, Y. and Burgoyne, J. A. and Pugin, Laurent and Fujinaga, I.},
	date = {2009},
	langid = {english},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
}

@book{fitzpatrick_publishing_nodate,
	title = {Publishing Futures for the Arts and Humanities},
	url = {http://www.culturemachine.net/index.php/cm/article/view/493/514},
	series = {Culture Machine},
	abstract = {The ‘future publishing’ that we discussed coalesces around the emerging moment in the history of technologies and the adaptive strategies deployed by the disseminators of information to accommodate them. The opportunities and challenges that they seed have extraordinary implications for the distribution and consumption of information; perhaps the most radical since the development of moveable type and its consequent market in reading.

This is Project 5 of the International Association for Visual Culture ({IAVC}). This project is constituted as a collaborative and Open Access forum on the possible futures of publishing. The project is published on-line and simultaneously across a number of distinct scholarly, creative, and critical research platforms: the College Art Association’s Art Journal website, the open-access journal Culture Machine, The Institute for Modern and Contemporary Culture ({IMCC}, University of Westminster), the {IAVC}, the journal of visual culture’s satellite website, Vectors: Journal of Culture and Technology in a Dynamic Vernacular, and the Modern Language Association Commons.},
	pagetotal = {33},
	number = {Interzone},
	author = {Fitzpatrick, Kathleen and Hall, Gary and {McPherson}, Tara },
	langid = {english},
	keywords = {goal\_Dissemination},
}
@online{connor_software_2013,
	title = {Software Takes Command: An Interview with Lev Manovich},
	url = {http://rhizome.org/editorial/2013/jul/10/lev-manovich-interview/},
	abstract = {Lev Manovich is a leading theorist of cultural objects produced with digital technology, perhaps best known for The Language of New Media ({MIT} Press, 2001). I interviewed him about his most recent book, Software Takes Command (Bloomsbury Academic, July 2014).},
	titleaddon = {Rhizome},
	author = {Connor, Michael},
	date = {2013-10-07},
	langid = {english},
	keywords = {meta\_Theorizing, obj\_Databases, obj\_Software},
}

@article{rapp_memex-idee._2013,
	title = {Die Memex-Idee. Vannevar Bush und die maschinelle Erweiterung des Denkens},
	volume = {4},
	issn = {1863-8937},
	url = {http://www.z-i-g.de/vorschau.cfm?akt=1},
	pages = {53--64},
	author = {Rapp, Andrea and Bender, Michael},
	date = {2013},
	langid = {german},
	keywords = {meta\_Collaborating, obj\_Documents, obj\_Infrastructures, obj\_Knowledge, obj\_Text},
}

@article{hedges_textgrid_2013,
	title = {{TextGrid}, {TEXTvre}, and {DARIAH}: Sustainability of Infrastructures for Textual Scholarship},
	rights = {{TEI} Consortium 2013 (Creative Commons Attribution-{NoDerivs} 3.0 Unported License)},
	issn = {2162-5603},
	url = {http://jtei.revues.org/774},
	doi = {10.4000/jtei.774},
	shorttitle = {{TextGrid}, {TEXTvre}, and {DARIAH}},
	abstract = {A variety of initiatives for developing virtual research environments, research infrastructures, and cyberinfrastructures have been funded in recent years. The systems produced vary considerably, but they all face the issue of sustainability, namely how to ensure the continued existence of a resource once the project that created it has finished. This paper addresses the sustainability issues faced by the {TextGrid} and {TEXTvre} virtual research environments for textual scholarship, examining the inter-project collaboration and cross-fertilization that took place, and investigating how the projects benefited from this exchange. It also examines how their sustainability is being facilitated by the more general-purpose {DARIAH} infrastructure, and conversely how their existing collaboration can serve as a model for future collaborations within the {DARIAH} community.},
	issue = {Issue 5},
	journaltitle = {Journal of the Text Encoding Initiative},
	author = {Hedges, Mark and Neuroth, Heike and Smith, Kathleen M. and Blanke, Tobias and Romary, Laurent and Küster, Marc and Illingworth, Malcolm},
	editora = {Blanke, Tobias and Romary, Laurent},
	editoratype = {collaborator},
	urldate = {2013-07-01},
	date = {2013-06-25},
	langid = {english},
	keywords = {*****, goal\_Collaboration, obj\_Infrastructures},
}

@online{schlesinger_feminism_2013,
	title = {Feminism and Programming Languages},
	url = {http://www.hastac.org/blogs/ari-schlesinger/2013/11/26/feminism-and-programming-languages},
	abstract = {a feminist programming language is to be built around a non-normative paradigm that represents alternative ways of abstracting. The intent is to encourage and allow new ways of thinking about problems such that we can code using a feminist ideology.  Well I took a look at the major programming paradigms, the following are the four main groups a programming language can fall into: imperative, functional, object-oriented, and logic. I decided to explore feminist logic such that a feminist programming language could be derived.

I am currently exploring feminist critiques of logic in hopes of outlining a working framework for the creation of a feminist programming language.},
	titleaddon = {{HASTAC}},
	author = {Schlesinger, Arielle},
	date = {2013-11-26},
	langid = {english},
	note = {(update, 13.12.13) also see: http://www.hastac.org/blogs/ari-schlesinger/2013/12/13/feminist-programmer},
	keywords = {meta\_DefinePolicy, meta\_Theorizing, obj\_AnyObject, obj\_Databases, obj\_DigitalHumanities},
}

@article{stiegler_avenir_2013,
	title = {L'avenir numérique de l'Université},
	url = {http://arsindustrialis.org/l-avenir-numerique-de-l-universite},
	abstract = {Le numérique constitue une nouvelle épistémè : c’est la nature même des savoirs sous toutes leurs formes qui s’en trouve affectée. Cette technologie fait à notre époque ce que l’écriture fit à l’Antiquité (et dont on peut dire qu’elle fit l’Antiquité en la défaisant).},
	journaltitle = {{ArsIndustrialis}-Blog},
	author = {Stiegler, Bernard},
	date = {2013-07-11},
	langid = {french},
	keywords = {meta\_Theorizing},
}

@article{baldwin_idiocy_2013,
	title = {The Idiocy of the Digital Literary (and what does it have to do with digital humanities)},
	volume = {7},
	url = {http://digitalhumanities.org/dhq/vol/7/1/000155/000155.html},
	abstract = {This essay considers the "idiocy" of the literary: its unaccountable singularity, which guarantees that we continue to return to it as a source, inspiration, and challenge. As a consequence, digital humanities is inspired and irritated by the literary.

My essay shows this in three ways. First, through a speculative exploration of the relation between digital humanities and the category of "the literary." Second, through a quick survey of the use of literature in digital humanities project. Thirdly, through a specific examination of {TEI} and character rendering as digital humanities concerns that necessarily engage with the literary. Once again, the literary remains singular and not abstract, literal in a way that challenges and provokes us towards new digital humanities work.},
	number = {7},
	author = {Baldwin, Sandy},
	date = {2013},
	langid = {english},
	keywords = {act\_Annotating, act\_Conceptualizing, goal\_Create, goal\_Interpretation, meta\_Assessing, meta\_Theorizing, obj\_Data/Databases, obj\_DigitalHumanities},
}

@article{hafner_kulturwissenschaften_2013,
	location = {Zürich},
	title = {Kulturwissenschaften im digitalen Zeitalter},
	url = {http://www.nzz.ch/aktuell/feuilleton/uebersicht/kulturwissenschaften-im-digitalen-zeitalter-1.18195895},
	journaltitle = {Neue Züricher Zeitung ({NZZ})},
	author = {Hafner, Urs},
	date = {2013-12-02},
	langid = {german},
	keywords = {goal\_Interpretation},
}

@online{graf_wir_2011,
	title = {"Wir brauchen mehr Experimentierfreude." Interview mit Dr. Klaus Graf},
	url = {http://www.lisa.gerda-henkel-stiftung.de/content.php?nav_id=1791},
	titleaddon = {L.I.S.A. - Das Wissenschaftsportal der Gerda Henkel Stiftung},
	author = {Graf, Klaus and Chatzoudis, Georgios},
	urldate = {2012-01-05},
	date = {2011-08-19},
	langid = {german},
}

@article{nowviskie_where_2011,
	title = {Where Credit Is Due: Preconditions for the Evaluation of Collaborative Digital Scholarship},
	volume = {2011},
	issn = {0740-6959},
	url = {http://www.mlajournals.org/doi/abs/10.1632/prof.2011.2011.1.169},
	doi = {10.1632/prof.2011.2011.1.169},
	shorttitle = {Where Credit Is Due},
	abstract = {In assessing digital humanities scholarship for purposes of tenure and promotion, committees must focus as much on process as on product, because digital work is situated in especially complex and collaborative networks of production and reception. Necessary shifts in evaluative practice require us to rethink internalized notions of solitary authorship, develop new standards for attribution, and revise institutional policies that govern intellectual property. This essay offers a set of preconditions for the evaluation of digital projects and argues that fair and full acknowledgment of the work of others (including non–faculty members and alternative academic contributors) will contribute to a scholarly communications ecosystem in which new work in the humanities is better fostered, designed, distributed, and preserved. ({BN})},
	pages = {169--181},
	number = {1},
	journaltitle = {Profession},
	author = {Nowviskie, Bethany},
	urldate = {2011-12-09},
	date = {2011-11},
	langid = {english},
}

@online{anonym_umfassendes_2012,
	title = {Umfassendes Plädoyer für Open Access im Wissenschaftsbereich},
	url = {http://www.bundestag.de/presse/hib/2012_06/2012_314/03.html},
	type = {Deutscher Bundestag Pressestelle},
	author = {{Anonym}},
	date = {2012-06-25},
	langid = {german},
	keywords = {X-{CHECK}},
}

@book{bailey_transforming_2010,
	location = {Houston, {TX}},
	title = {Transforming scholarly publishing through open access : a bibliography},
	isbn = {9781453780817 1453780815},
	url = {http://digital-scholarship.org/tsp/},
	shorttitle = {Transforming scholarly publishing through open access},
	abstract = {Can scholarly journal articles and other scholarly works be made freely available on the Internet? The open access movement says "yes," and it is having a significant impact on scholarly publishing},
	publisher = {Digital Scholarship},
	author = {Bailey, Charles W},
	date = {2010},
	langid = {english},
}

@article{korbmann_be_2011,
	title = {To be or not to be – die digitalen Medien},
	url = {http://wissenschaftkommuniziert.wordpress.com/2011/12/08/to-be-or-not-to-be-die-digitalen-medien/},
	abstract = {Die Welt der Wissenschaftskommunikation ist kompliziert genug. Nicht nur das schwierige Verhältnis von Wissenschaftlern und Öffentlichkeitsarbeit, nicht nur die komplexen Inhalte, über die zu berichten ist, nicht nur das verwirrend geknüpfte Netz, in dem sich Kommunikation in unserer Informationsgesellschaft abspielt. Das genügt nicht. Denn jetzt soll alles noch viel komplizierter werden: Soziale Netze, Markenprofile, Zielgruppen und Bezugsgruppen sind angesagt. Das ist das Fazit des 4. Forums Wissenschaftskommunikation der Initiative Wissenschaft im Dialog ({WiD}), das am Donnerstag in Köln zu Ende ging.},
	journaltitle = {Wissenschaft kommuniziert},
	author = {Korbmann, Reiner},
	urldate = {2012-01-05},
	date = {2011-12-08},
	langid = {german},
}

@article{darling_role_2013,
	title = {The role of twitter in the life cycle of a scientific publication},
	url = {http://arxiv.org/abs/1305.0435},
	abstract = {Twitter is a micro-blogging social media platform for short messages that can have a long-term impact on how scientists create and publish ideas. We investigate the usefulness of twitter in the development and distribution of scientific knowledge. At the start of the life cycle of a scientific publication, twitter provides a large virtual department of colleagues that can help to rapidly generate, share and refine new ideas. As ideas become manuscripts, twitter can be used as an informal arena for the pre-review of works in progress. Finally, tweeting published findings can communicate research to a broad audience of other researchers, decision makers, journalists and the general public that can amplify the scientific and social impact of publications. However, there are limitations, largely surrounding issues of intellectual property and ownership, inclusiveness and misrepresentations of science sound bites. Nevertheless, we believe twitter is a useful social media tool that can provide a valuable contribution to scientific publishing in the 21st century.},
	journaltitle = {{arXiv}:1305.0435},
	author = {Darling, Emily S. and Shiffman, David and Côté, Isabelle M. and Drew, Joshua A.},
	urldate = {2013-05-06},
	date = {2013-05-02},
	langid = {english},
	keywords = {goal\_Dissemination, obj\_Research, obj\_ResearchResults},
}

@article{bauerlein_research_2011,
	title = {The Research Bust},
	issn = {0009-5982},
	url = {http://chronicle.com/article/The-Research-Bust/129930/},
	journaltitle = {The Chronicle of Higher Education},
	author = {Bauerlein, Mark},
	urldate = {2012-01-02},
	date = {2011-12-04},
	langid = {english},
}

@article{spier_history_2002,
	title = {The history of the peer-review process},
	volume = {20},
	issn = {0167-7799},
	url = {http://www.sciencedirect.com/science/article/pii/S0167779902019856},
	doi = {10.1016/S0167-7799(02)01985-6},
	abstract = {The peer-review process is a turf battle with the ultimate prize of the knowledge, science or doctrine being published. On the one side, we have the writers and originators of ideas, on the other, we have the editors and critics. But it was not always so.},
	pages = {357--358},
	number = {8},
	journaltitle = {Trends in Biotechnology},
	author = {Spier, Ray},
	urldate = {2012-01-17},
	date = {2002-08-01},
	langid = {english},
	keywords = {act\_Publishing, meta\_Assessing, meta\_GiveOverview, obj\_Research},
}

@incollection{bollier_growth_2007,
	location = {Cambridge, Mass.},
	title = {The growth of the commons paradigm},
	url = {http://www.hebrewcollege.edu/sites/default/files/SOVA_Growth%20of%20Commons.pdf},
	pages = {27--40},
	booktitle = {Understanding knowledge as a commons: From theory to practice},
	publisher = {{MIT} Press},
	author = {Bollier, D},
	editor = {Ostrom, E and Hess, C.},
	date = {2007},
	langid = {english},
	keywords = {act\_Sharing, meta\_GiveOverview, obj\_AnyObject},
}
@article{borgman_conundrum_2011,
	title = {The Conundrum of Sharing Research Data},
	url = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1869155},
	abstract = {of research data has excited researchers, policy makers, and the general public. Not only might research be reproducible, but new questions can be asked, with great benefit to research, innovation, education, and the citizenry. However, very little data is being shared, despite the best efforts of funding agencies and journals. This article explores the complexities of data, research practices, innovation, incentives, economics, intellectual property, and public policy associated with the data sharing conundrum – “an intricate and difficult problem.” Research data take many forms, are collected for many purposes, via many approaches, and often are difficult to interpret once removed from their initial context. Rationales for sharing data vary along two dimensions: whether motivated by research concerns or by leveraging public investments, and whether intended to serve the interests of researchers who produce data or the interests of potential re-users of data. Four rationales for sharing research data are identified and positioned on these dimensions. Researchers’ incentives to share their data depend not only on these rationales, but on characteristics of their data and research practices, funding agency policies, and resources for data management. Much more is understood about why researchers do not share data than about when, why, and how researchers do share data, or about when, how, and why researchers or the public reuse data. The model and research agenda are illustrated with examples from the sciences, social sciences, and humanities.Requested citation to this version: Borgman, Christine L. (2011, submitted). The conundrum of sharing research data. Journal of the American Society for Information Science and Technology.},
	journaltitle = {{SSRN} {eLibrary}},
	author = {Borgman, Christine L.},
	urldate = {2011-07-19},
	date = {2011-06-21},
	langid = {english},
	keywords = {goal\_Collaboration, goal\_Dissemination, meta\_GiveOverview},
}

@book{darnton_case_2010,
	location = {New York},
	title = {The case for books: past, present, and future},
	isbn = {9781586489021},
	url = {http://www.publicaffairsbooks.com/publicaffairsbooks-cgi-bin/display?book=9781586488260},
	shorttitle = {The case for books},
	abstract = {The era of the printed book is at a crossroad. E-readers are flooding the market, books are available to read on cell phones, and companies such as Google, Amazon, and Apple are competing to command near monopolistic positions as sellers and dispensers of digital information. Already, more books have been scanned and digitized than were housed in the great library in Alexandria. Is the printed book resilient enough to survive the digital revolution, or will it become obsolete? In this lasting collection of essays, Robert Darnton—an intellectual pioneer in the field of this history of the book—lends unique authority to the life, role, and legacy of the book in society.},
	pagetotal = {219},
	publisher = {{PublicAffairs}},
	author = {Darnton, Robert},
	date = {2010},
	langid = {english},
	keywords = {goal\_Dissemination, meta\_GiveOverview, obj\_Documents, obj\_Text},
}

@book{willinsky_access_2009,
	edition = {1},
	title = {The Access Principle: The Case for Open Access to Research and Scholarship},
	isbn = {0262512661},
	url = {http://mitpress.mit.edu/books/access-principle},
	shorttitle = {The Access Principle},
	abstract = {Questions about access to scholarship go back farther than recent debates over subscription prices, rights, and electronic archives suggest. The great libraries of the past—from the fabled collection at Alexandria to the early public libraries of nineteenth-century America—stood as arguments for increasing access. In The Access Principle, John Willinsky describes the latest chapter in this ongoing story—online open access publishing by scholarly journals—and makes a case for open access as a public good.

A commitment to scholarly work, writes Willinsky, carries with it a responsibility to circulate that work as widely as possible: this is the access principle. In the digital age, that responsibility includes exploring new publishing technologies and economic models to improve access to scholarly work. Wide circulation adds value to published work; it is a significant aspect of its claim to be knowledge. The right to know and the right to be known are inextricably mixed. Open access, argues Willinsky, can benefit both a researcher-author working at the best-equipped lab at a leading research university and a teacher struggling to find resources in an impoverished high school.

Willinsky describes different types of access—the New England Journal of Medicine, for example, grants open access to issues six months after initial publication, and First Monday forgoes a print edition and makes its contents immediately accessible at no cost. He discusses the contradictions of copyright law, the reading of research, and the economic viability of open access. He also considers broader themes of public access to knowledge, human rights issues, lessons from publishing history, and "epistemological vanities." The debate over open access, writes Willinsky, raises crucial questions about the place of scholarly work in a larger world—and about the future of knowledge.},
	publisher = {The {MIT} Press},
	author = {Willinsky, John},
	date = {2009-04-30},
	langid = {english},
}

@article{flanders_tapas:_2013,
	title = {{TAPAS}: Building a {TEI} Publishing and Repository Service},
	rights = {{TEI} Consortium 2013 (Creative Commons Attribution-{NoDerivs} 3.0 Unported License)},
	issn = {2162-5603},
	url = {http://jtei.revues.org/788},
	doi = {10.4000/jtei.788},
	shorttitle = {{TAPAS}},
	abstract = {This project report details the goals and development of the {TEI} Archiving, Publishing, and Access Service ({TAPAS}). The project's mission centers on providing repository and publication services to individuals and small projects working with {TEI} data: a constituency that typically lacks access to institutional resources such as server space, {XML} expertise, and programming time. In developing this service, {TAPAS} addresses several important challenges: the creation of a publication ecology that operates gracefully at the level of the individual project and the {TAPAS} corpus; the problem of the vulnerability of {TEI} data in cases where projects cease their activity; and the variability and complexity of {TEI} data. When completed, the {TAPAS} service will enable its contributors to upload, manage, and publish their {TEI} data, and it will offer a publication interface through which both individual project collections and the {TAPAS} collection as a whole can be read, searched, and explored. It will also provide a range of related services such as technical consulting, data curation and conversion work, {TEI} workshops, tutorials, and community forums. This article discusses the philosophy and rationale behind the project's current development efforts, and examines some of the challenges the project faces.},
	issue = {Issue 5},
	journaltitle = {Journal of the Text Encoding Initiative},
	author = {Flanders, Julia and Hamlin, Scott},
	editora = {Blanke, Tobias and Romary, Laurent},
	editoratype = {collaborator},
	urldate = {2013-10-17},
	date = {2013-06-24},
	langid = {english},
	keywords = {goal\_Dissemination},
}

@report{maron_sustaining_2013,
	title = {Sustaining Our Digital Future: Institutional Strategies for Digital Content},
	url = {http://sca.jiscinvolve.org/wp/portfolio-items/sustaining-our-digital-future-institutional-strategies-for-digital-content/},
	abstract = {This study, conducted by Ithaka S+R, with funding from the Jisc-led Strategic Content Alliance, grew from the findings of earlier studies showing that both funders and project leaders alike rely very heavily on their host institutions to support and sustain digital content, beyond the end of the grant. While the primary focus of this study is the lush, if unruly, terrain of higher education institutions, academia is not the only sector enjoying an era of digital growth. As museums and public-facing libraries seek to expand their reach beyond their physical spaces, digital activities have become a core part of their strategy. And so, as well as an assessment of the university environment as a “host” for digital content, this study includes a more exploratory look at how cultural heritage institutions think about and plan for sustaining and enhancing the value of their digital collections. The cultural sector offers very different models and allows us to draw initial conclusions around these useful models for others to replicate, experiment with and develop further.},
	institution = {{JISC}},
	author = {Maron, Nancy and Yun, Jason and Pickle, Sarah},
	date = {2013},
	langid = {english},
	keywords = {goal\_Storage, meta\_Assessing, obj\_AnyObject},
}

@article{cohen_scholars_2010,
	title = {Scholars Test Web Alternative to Peer Review},
	url = {http://www.nytimes.com/2010/08/24/arts/24peer.html},
	abstract = {Dan Cohen, director of the Center for History and New Media at George Mason University, is among the academics who advocate a more open, Web-based approach to reviewing scholarly works.},
	journaltitle = {New York Times},
	author = {Cohen, Patricia},
	date = {2010-08-23},
	langid = {english},
}

@article{howard_scholars_2012,
	title = {Scholars Seek Better Ways to Track Impact Online},
	url = {http://chronicle.com/article/As-Scholarship-Goes-Digital/130482/},
	abstract = {Jason Priem, a graduate student in library sciences at the U. of North Carolina at Chapel Hill, helped write a manifesto on "altmetrics."},
	journaltitle = {The Chronicle of Higher Education},
	author = {Howard, Jennifer},
	date = {2012-01-29},
	langid = {english},
}

@incollection{puschmann_scholarly_2012,
	location = {Düsseldorf},
	title = {Scholarly Blogging: A New Form of Publishing or Science Journalism 2.0?},
	url = {http://nfgwin.uni-duesseldorf.de/sites/default/files/Puschmann.pdf},
	abstract = {This paper examines scholarly blogging as an emergent phe-
nomenon among academics of different disciplinary back-
grounds, as well as science enthusiasts and practitioners wish-
ing to communicate about topics related to a specific academic
field with a broader public. We give a brief historical account
of scholarly blogging, paired with a review of academic litera-
ture about the phenomenon. Results from a survey conducted
among bloggers active on scilogs.de, a German-language sci-
ence blogging platform, show that considerable differences ex-
ist between conceptualizations of scholarly blogging as “pub-
lishing 2.0,” i.e., a replacement for traditional venues of
scholarly communication, and blogging as a new form of sci-
ence journalism. Building on this differentiation, we ask what
relevance scholarly blogs have today and in the future, both
from the internal perspective of science and from the external
vantage points of funders, lawmakers, and civil society.},
	pages = {171--181},
	booktitle = {Science and the Internet},
	publisher = {Düsseldorf University Press},
	author = {Puschmann, Cornelius and Mahrt, Merja},
	editor = {{Tokar, A., Beurskens, M., Keuneke, S., Mahrt, M., Peters, I., Puschmann, C., van Treeck, T., \&} and {Weller, K.}},
	date = {2012},
	langid = {english},
	keywords = {act\_Publishing, obj\_ResearchResults, t\_Blogging},
}

@online{freistetter_schadet_2012,
	title = {Schadet Wissenschaftskommunikation und Medienpräsenz der akademischen Karriere?},
	url = {http://www.scienceblogs.de/astrodicticum-simplex/2012/01/schadet-wissenschaftskommunikation-und-medienprasenz-der-akademischen-karriere.php},
	titleaddon = {Astrodicticum Simplex},
	author = {Freistetter, Florian},
	date = {2012-01-02},
	langid = {german},
}

@report{high_level_expert_group_on_scientific_data_riding_2010,
	location = {Brussels},
	title = {Riding the wave. How Europe can gain from the rising tide of scientific data. A submission to the European Commission},
	url = {http://ec.europa.eu/information_society/newsroom/cf/itemlongdetail.cfm?item_id=6204},
	abstract = {The High-Level Group on Scientific Data presented today their final report to Neelie Kroes, European Commission Vice-President for the Digital Agenda. The report "Riding the Wave: How Europe can gain from the rising tide of scientific data" is the result of six months of intense brainstorming and consultations with experts around the world to prepare a "vision 2030" for Scientific Data e-Infrastructures.

The report describes long term scenarios and associated challenges regarding scientific data access, curation and preservation as well as the strategy and actions necessary to realise the vision. The High-Level Group is composed of twelve top-level European experts in different fields of science and is chaired by Prof John Wood, also chair of {ERAB}.

More information about the High-Level Group on Scientific Data and on their report can be found on the Cordis website (http://cordis.europa.eu/fp7/ict/e-infrastructure/high-level-group\_en.html)},
	institution = {European Commission},
	author = {{High level Expert Group on Scientific Data}},
	date = {2010-10},
	langid = {english},
	keywords = {meta\_DefinePolicy},
}

@book{nielsen_reinventing_2011,
	location = {Princeton, N.J.},
	title = {Reinventing discovery : the new era of networked science},
	isbn = {9780691148908  0691148902},
	url = {http://press.princeton.edu/titles/9517.html},
	shorttitle = {Reinventing discovery},
	abstract = {In Reinventing Discovery, Michael Nielsen argues that we are living at the dawn of the most dramatic change in science in more than 300 years. This change is being driven by powerful new cognitive tools, enabled by the internet, which are greatly accelerating scientific discovery. There are many books about how the internet is changing business or the workplace or government. But this is the first book about something much more fundamental: how the internet is transforming the nature of our collective intelligence and how we understand the world. Reinventing Discovery tells the exciting story of an unprecedented new era of networked science. We learn, for example, how mathematicians in the Polymath Project are spontaneously coming together to collaborate online, tackling and rapidly demolishing previously unsolved problems. We learn how 250,000 amateur astronomers are working together in a project called Galaxy Zoo to understand the large-scale structure of the Universe, and how they are making astonishing discoveries, including an entirely new kind of galaxy. These efforts are just a small part of the larger story told in this book--the story of how scientists are using the internet to dramatically expand our problem-solving ability and increase our combined brainpower. This is a book for anyone who wants to understand how the online world is revolutionizing scientific discovery today--and why the revolution is just beginning"--. Reinventing Discovery argues that we are in the early days of the most dramatic change in how science is done in more than 300 years. This change is being driven by new online tools, which are transforming and radically accelerating scientific discovery"--.},
	publisher = {Princeton University Press},
	author = {Nielsen, Michael A},
	date = {2011},
	langid = {english},
}

@article{hartley_refereeing_2012,
	title = {Refereeing academic articles in the information age},
	volume = {43},
	issn = {00071013},
	url = {http://doi.wiley.com/10.1111/j.1467-8535.2011.01211.x},
	doi = {10.1111/j.1467-8535.2011.01211.x},
	abstract = {The new technology (such as {ScholarOne}) used for submitting papers to academic journals (such as this one) increases the possibilities for gathering, analysing and presenting summary data on stages in the refereeing process. Such data can be used to clarify the roles played by editors and publishers as well as referees—roles less widely discussed in the previous literature. I conclude, after a review of related issues, that refereeing should be “open” in this information age—i.e. correspondence between editors, referees and authors should be open and available, and not private. Some of the issues involved in achieving this are outlined and discussed.},
	pages = {520--528},
	number = {3},
	journaltitle = {British Journal of Educational Technology},
	author = {Hartley, James},
	urldate = {2012-05-02},
	date = {2012-05},
	langid = {english},
}

@article{prosser_reassessing_2011,
	title = {Reassessing the value proposition: First steps towards a fair(er) price for scholarly journals},
	volume = {24},
	url = {http://uksg.metapress.com/content/g849j76241787320/?genre=article&issn=0953-0460&volume=24&issue=1&spage=60#},
	doi = {doi:10.1629/2460},
	abstract = {Business models surrounding scholarly communications have been much debated over the past two decades. However, interest in the discussion has been sharpened in the {UK} by the funding issues facing the higher education sector, the reality of budget cuts and the need to focus on affordability. There is a growing realization that notions of ‘moderate’ price increases associated with ‘big deals’ over the past ten years will need to be radically reassessed. This paper discusses the financial realities facing {UK} libraries and the attempt by Research Libraries {UK} ({RLUK}) to look for solutions that maintain access to the maximum amount of material possible.},
	pages = {60--63},
	number = {1},
	journaltitle = {Serials: The Journal for the Serials Community},
	author = {Prosser, David C},
	date = {2011},
	langid = {english},
}

@article{lauber-ronsberg_raubkopierer_2012,
	title = {Raubkopierer und Content-Mafia: Die Debatte um das Urheberrecht},
	volume = {62},
	url = {http://www.bpb.de/apuz/145382/raubkopierer-und-content-mafia-die-debatte-um-das-urheberrecht},
	abstract = {"Wir sind die Urheber!“ Mit diesen Worten riefen im Mai 2012 100 bekannte Autorinnen und Autoren, Künstlerinnen und Künstler dazu auf, den Schutz ihrer Werke durch das Urheberrecht zu stärken und den heutigen Bedingungen des schnellen und massenhaften Zugangs zu den Produkten geistiger Arbeit anzupassen.[1] "Wir sind die Bürgerinnen und Bürger!“ hielten ihnen mehr als 7.000 Netz-Nutzer entgegen und plädierten für eine Reform und Annäherung des Urheberrechts "an gesellschaftliche Realitäten“.[2] Die Proteste gegen das im Juli 2012 vom Europäischen Parlament abgelehnte internationale Handelsabkommen {ACTA} (Anti-Counterfeiting Trade Agreement) brachten europaweit zigtausend Demonstranten auf die Straße. Wohl noch nie zuvor befand sich das Urheberrecht derartig in der öffentlichen Diskussion – und in einer solchen Legitimationskrise.},
	pages = {32--38},
	number = {41},
	journaltitle = {Aus Politik und Zeitgeschichte},
	author = {Lauber-Rönsberg, Anne},
	date = {2012-10},
	langid = {german},
}

@report{cdrh_promotion_2008,
	location = {Unversity of Nebraska-Lincoln},
	title = {Promotion \& Tenure Criteria for Assessing Digital Research in the Humanities},
	url = {http://cdrh.unl.edu/articles/promotion_and_tenure.php},
	abstract = {Introduction and Goals: Digital Humanities crosses the boundaries between computer science and humanities disciplines such as cultural anthropology, archaeology, classics, English, history, modern languages and literatures, library science, and the arts. The emphasis is on humanities as a whole rather than specific disciplines; however some scholarship is more pertinent to specific discliplines than others. Where it comes closest to computer science is in the development of scholarly tools. Largely, however, the emphasis is on the humanities, and faculty may be engaged in creating new approaches to understanding the humanities through technological means.[1] Faculty engaged in digital humanities scholarship need to be evaluated rigorously and fairly. This document strives to provide a resource which outlines criteria for evaluating dossiers in this scholarly area.},
	institution = {Center for Digital Research in the Humanities},
	author = {{CDRH}},
	date = {2008},
	langid = {english},
	keywords = {meta\_Assessing, meta\_DefinePolicy, obj\_ResearchResults},
}

@book{fitzpatrick_planned_2011,
	title = {Planned Obsolescence: Publishing, Technology, and the Future of the Academy},
	isbn = {0814727883},
	url = {http://nyupress.org/books/book-details.aspx?bookid=5008},
	shorttitle = {Planned Obsolescence},
	abstract = {Academic institutions are facing a crisis in scholarly publishing at multiple levels: presses are stressed as never before, library budgets are squeezed, faculty are having difficulty publishing their work, and promotion and tenure committees are facing a range of new ways of working without a clear sense of how to understand and evaluate them.  

Planned Obsolescence is both a provocation to think more broadly about the academy’s future and an argument for reconceiving that future in more communally-oriented ways. Facing these issues head-on, Kathleen Fitzpatrick focuses on the technological changes—especially greater utilization of internet publication technologies, including digital archives, social networking tools, and multimedia—necessary to allow academic publishing to thrive into the future. But she goes further, insisting that the key issues that must be addressed are social and institutional in origin. 

Springing from original research as well as Fitzpatrick’s own hands-on experiments in new modes of scholarly communication through {MediaCommons}, the digital scholarly network she co-founded, Planned Obsolescence explores these aspects of scholarly work, as well as issues surrounding the preservation of digital scholarship and the place of publishing within the structure of the contemporary university.  Written in an approachable style designed to bring administrators and scholars into a conversation, Planned Obsolescence explores both symptom and cure to ensure that scholarly communication will remain relevant in the digital future.},
	pagetotal = {256},
	publisher = {{NYU} Press},
	author = {Fitzpatrick, Kathleen},
	date = {2011-11-01},
	langid = {english},
	keywords = {*****, act\_Publishing, meta\_GiveOverview, obj\_ResearchResults},
}

@article{fitzpatrick_peer_2011,
	title = {Peer Review, Judgment, and Reading},
	volume = {2011},
	issn = {0740-6959},
	url = {http://www.mlajournals.org/doi/abs/10.1632/prof.2011.2011.1.196},
	doi = {10.1632/prof.2011.2011.1.196},
	abstract = {Evaluating new forms of scholarship for tenure and promotion requires taking those forms, and the methods of peer review they bring with them, on their own terms. Even more, it requires exercising the critical judgment on which our profession relies instead of outsourcing that judgment to others. Such evaluation requires reading both the work and the available evidence of the ways that scholars have responded to that work. ({KF})},
	pages = {196--201},
	number = {1},
	journaltitle = {Profession},
	author = {Fitzpatrick, Kathleen},
	urldate = {2011-12-09},
	date = {2011-11},
	langid = {english},
}

@article{tamburri_opening_2012,
	title = {Opening up peer review},
	url = {http://www.universityaffairs.ca/opening-up-peer-review.aspx},
	abstract = {As journals test the waters of open peer review, authors and editors remain divided over the merits of tinkering with a tried-and-true system},
	journaltitle = {University Affairs / Affaires universitaires},
	author = {Tamburri, Rosanna},
	date = {2012-03-05},
	langid = {english},
}

@article{eve_open_2012,
	title = {Open access journals: are we asking the right questions?},
	url = {http://www.guardian.co.uk/higher-education-network/blog/2012/feb/08/open-access-journals-elsvier-boycott},
	shorttitle = {Open access journals},
	abstract = {The academic publisher Elsevier is being boycotted by the online {HE} community due to the prohibitive costs of its journals. But is an open access model the right solution, asks Martin Paul Eve},
	journaltitle = {The Guardian},
	author = {Eve, Martin Paul},
	urldate = {2012-02-08},
	date = {2012-02-08},
	langid = {english},
}

@book{suber_open_2012,
	location = {Cambridge, Mass},
	title = {Open access},
	isbn = {9780262517638},
	url = {http://mitpress.mit.edu/books/open-access},
	series = {{MIT} Press essential knowledge series},
	abstract = {The Internet lets us share perfect copies of our work with a worldwide audience at virtually no cost. We take advantage of this revolutionary opportunity when we make our work “open access”: digital, online, free of charge, and free of most copyright and licensing restrictions. Open access is made possible by the Internet and copyright-holder consent, and many authors, musicians, filmmakers, and other creators who depend on royalties are understandably unwilling to give their consent. But for 350 years, scholars have written peer-reviewed journal articles for impact, not for money, and are free to consent to open access without losing revenue.

In this concise introduction, Peter Suber tells us what open access is and isn’t, how it benefits authors and readers of research, how we pay for it, how it avoids copyright problems, how it has moved from the periphery to the mainstream, and what its future may hold. Distilling a decade of Suber’s influential writing and thinking about open access, this is the indispensable book on the subject for researchers, librarians, administrators, funders, publishers, and policy makers.},
	pagetotal = {242},
	publisher = {{MIT} Press},
	author = {Suber, Peter},
	date = {2012},
	langid = {english},
	keywords = {*****, goal\_Dissemination, obj\_Research, t\_OpenAccess},
}

@article{rockwell_evaluation_2011,
	title = {On the Evaluation of Digital Media as Scholarship},
	volume = {2011},
	issn = {0740-6959},
	url = {http://www.mlajournals.org/doi/abs/10.1632/prof.2011.2011.1.152},
	doi = {10.1632/prof.2011.2011.1.152},
	abstract = {As more and more scholarship is digital, we need to develop a culture of conversation around the evaluation of digital academic work. We have to be able to evaluate new types of research, like analytic tools and hypermedia fiction, that are difficult to review. The essay surveys common types of digital scholarly work, discusses what evaluators should ask, discusses how digital researchers can document their scholarship, and then discusses the types of conversations hires and evaluators (like chairs) should have and when they should have them. Where there is a conversation around evaluation in a department, both hires and evaluators are more likely to come to consensus as to what is appropriate digital research and how it should be documented.},
	pages = {152--168},
	number = {1},
	journaltitle = {Profession},
	author = {Rockwell, Geoffrey},
	urldate = {2011-12-09},
	date = {2011-11},
	langid = {english},
}

@online{bonisch_offentlichkeitsarbeit_2012,
	title = {Öffentlichkeitsarbeit als Pflicht für Wissenschaftler},
	url = {http://www.scilogs.de/chrono/blog/vergangenheitsstaub/allgemein/2012-01-04/oeffentlichkeitsarbeit-als-pflicht-fuer-wissenschaftler},
	abstract = {Dieser Artikel wird kein geschichtswissenschaftlicher sein, aber es brennt mir unter den Fingern, auch wenn dieses Thema hier auf Scilogs Eulen nach Athen tragen bedeutet. In letzter Zeit bin ich vermehrt auf ein Thema gestoßen, daß mich schon lange beschäftigt: Öffentlichkeitsarbeit von Wissenschaftlern im Social Web. Sollten oder müssen sogar Wissenschaftler durch Nutzung des Social Web (Bloggen, Wiki, Twitter, Facebook) ihre Forschungen der Gesellschaft als Öffentlichkeit, die sie in nicht unerheblichen Maße finanziert, erklären? Ist Öffentlichkeitsarbeit eine Pflicht für Wissenschaftler?},
	titleaddon = {Vergangenheitsstaub},
	author = {Bönisch, Wenke},
	date = {2012-01-04},
	langid = {german},
}

@online{nature_natures_2006,
	title = {Nature's peer review debate},
	url = {http://www.nature.com/nature/peerreview/debate/},
	abstract = {Peer review is commonly accepted as an essential part of scientific publication. But the ways peer review is put into practice vary across journals and disciplines. What is the best method of peer review? Is it truly a value-adding process? What are the ethical concerns? And how can new technology be used to improve traditional models?

This Nature web debate consists of 22 articles of analyses and perspectives from leading scientists, publishers and other stakeholders to address these questions. Key links and relevant articles from our archive are listed below, with further resources available through Connotea. Visit the Peer-to-Peer blog to join the debate.},
	titleaddon = {Nature},
	author = {{Nature}},
	date = {2006},
	langid = {english},
}

@article{acevedo_network_2007,
	title = {Network Capital: an Expression of Social Capital in the Network Society},
	volume = {3},
	url = {http://www.diigo.com/annotated/e3b2ee9757c52bd3d4bd23ca11eddb9b},
	abstract = {This article deals with an emerging type of social capital which is labeled as ‘network capital’. It is formed from collaborative practices emerging from e-enabled human networks. It is proposed that network capital is a specific type of social capital in the Network Society, and that it holds significant value for the advancement of human development around the world.},
	number = {2},
	journaltitle = {Community Informatics},
	author = {Acevedo, Manuel},
	urldate = {2009-12-13},
	date = {2007},
	langid = {english},
}

@article{soong_measuring_2009,
	title = {Measuring Citation Advantages of Open Accessibility},
	volume = {15},
	rights = {Copyright © 2009 Samson C. Soong},
	issn = {1082-9873},
	url = {http://www.dlib.org/dlib/november09/soong/11soong.html},
	doi = {doi:10.1045/november2009-soong},
	abstract = {Over the past decade, a good number of studies have attempted to demonstrate that open access journal articles have higher citation rates than traditionally published ones.1 These open access articles are either published in open access journals or are made publicly available through open access repositories. More methodologically sound studies, however, are needed to pinpoint more exactly the impact, if any, of open accessibility on citation rates and to look at the issue of impact (or non-impact) more closely. For instance, of those journal articles that have been made openly accessible, about what percent of them benefit from open online availability? What percent of them do not?

This article describes a study, involving a set of articles published in scholarly journals by faculty members of the Hong Kong University of Science and Technology ({HKUST}) that have also been deposited in the {HKUST} Institutional Repository. The study was conducted to measure the actual effect of their open accessibility on citation rates. More importantly, the study suggests another quantitative method, and it is hoped that the study will inspire more effective methods of measuring the net impact of open access.},
	number = {11},
	journaltitle = {D-Lib Magazine},
	author = {Soong, Samson C.},
	urldate = {2009-11-18},
	date = {2009-12},
	langid = {english},
}
@book{pryor_managing_2012,
	location = {London},
	title = {Managing research data},
	isbn = {1856047563  9781856047562},
	url = {http://www.amazon.de/Managing-Research-Data-Graham-Pryor/dp/1856047563},
	abstract = {Data management has become an essential requirement for information professionals in the last decade, particularly for the higher education research community, as more and more digital information is created and stored. As budgets shrink and funders of research increasingly demand evidence of value for money and demonstrable benefits for society, there is increasing pressure to provide plans of sustainable management of data. Ensuring that important information remains discoverable, accessible and intelligible and is shared as part of a larger web of data will mean research has a life beyond its initial purpose and can offer real utility to the wider institution and beyond. This edited collection, bringing together leading figures in the field from the {UK} and around the world, provides an introduction to all the key data issues facing the {HE} and information management communities. Using the authors' expertise and relevant international case studies, it defines what is required to achieve a culture of effective data management offering practical advice on the skills required, legal and contractual obligations, strategies and management plans and the data management infrastructure of specialists and services. Each chapter covers a critical element of data management including: the meaning of data management; the lifecycle of data management; the policy environment; an organisational approach to achieving digital sustainability; data management plans and planning; roles and responsibilities for libraries and librarians; the challenge to university information services; an analysis of the New World approach; and resources and sources of support to data management. This is essential reading for librarians and information professionals working in the higher education sector, the research community, policy makers and university managers. It will also be a useful introduction for students taking courses in information management, archivists and national library services.},
	publisher = {Facet Publishing},
	author = {Pryor, Graham},
	date = {2012},
	langid = {english},
	keywords = {goal\_Capture, goal\_Storage, obj\_AnyObject},
}

@misc{whyte_making_2011,
	title = {Making the Case for Research Data Management},
	url = {http://www.dcc.ac.uk/resources/briefing-papers/making-case-rdm},
	abstract = {This briefing paper aims to help managers in research institutions build support for developing new services for research data management. It also gives a brief snapshot of the {JISC}-led programmes on Managing Research Data and Shared Services and the Cloud. - See more at: http://www.dcc.ac.uk/resources/briefing-papers/making-case-rdm\#sthash.{mD}0seRyu.dpuf},
	publisher = {{JISC}},
	author = {Whyte, Angus and Tedds, Jonathan},
	date = {2011-01-09},
	langid = {english},
}

@article{hanson_making_2011,
	title = {Making Data Maximally Available},
	volume = {331},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1203354},
	doi = {10.1126/science.1203354},
	abstract = {Science is driven by data. New technologies have vastly increased the ease of data collection and consequently the amount of data collected, while also enabling data to be independently mined and reanalyzed by others. And society now relies on scientific data of diverse kinds; for example, in responding to disease outbreaks, managing resources, responding to climate change, and improving transportation. It is obvious that making data widely available is an essential element of scientific research. The scientific community strives to meet its basic responsibilities toward transparency, standardization, and data archiving. Yet, as pointed out in a special section of this issue (pp. 692–729), scientists are struggling with the huge amount, complexity, and variety of the data that are now being produced.},
	pages = {649--649},
	journaltitle = {Science},
	author = {Hanson, B. and Sugden, A. and Alberts, B.},
	urldate = {2011-11-30},
	date = {2011-02-10},
	langid = {english},
}

@article{wheary_living_1997,
	title = {Living Reviews in Relativity: Making an Electronic Journal Live},
	volume = {3},
	issn = {1080-2711},
	url = {http://hdl.handle.net/2027/spo.3336451.0003.105},
	doi = {10.3998/3336451.0003.105},
	shorttitle = {Living Reviews in Relativity},
	number = {1},
	journaltitle = {The Journal of Electronic Publishing},
	author = {Wheary, Jennnifer and Schutz, Bernard F.},
	urldate = {2013-02-18},
	date = {1997-09-01},
	langid = {english},
}

@collection{neuroth_langzeitarchivierung_2012,
	location = {Boizenburg \& Göttingen},
	title = {Langzeitarchivierung von Forschungsdaten : eine Bestandsaufnahme.},
	isbn = {9783864880087 3864880084},
	url = {http://nestor.sub.uni-goettingen.de/bestandsaufnahme/index.php},
	shorttitle = {Langzeitarchivierung von Forschungsdaten},
	publisher = {{VWH} Verlag \& Universitätsverlag Göttingen},
	editor = {Neuroth, Heike and Strathmann, Stefan and Oßwald, Achim and Scheffel, Regine and Klump, Jens and Ludwig, Jens},
	date = {2012},
	langid = {german},
	keywords = {goal\_Storage, meta\_GiveOverview, obj\_Data},
}

@online{sahle_kriterien_2012,
	title = {Kriterien für die Besprechung digitaler Editionen (Version 1.0)},
	url = {http://www.i-d-e.de/aktivitaeten/reviews/kriterien-version-1},
	titleaddon = {Institut für Dokumentologie und Editorik (Köln)},
	author = {Sahle, Patrick},
	editora = {Vogeler, Georg},
	editoratype = {collaborator},
	date = {2012},
	langid = {german},
	note = {Köln: {IDE}, 2012},
}

@article{stapelfeldt_islandora_2013,
	title = {Islandora and {TEI}: Current and Emerging Applications/Approaches},
	rights = {{TEI} Consortium 2013 (Creative Commons Attribution-{NoDerivs} 3.0 Unported License)},
	issn = {2162-5603},
	url = {http://jtei.revues.org/790},
	doi = {10.4000/jtei.790},
	shorttitle = {Islandora and {TEI}},
	abstract = {Islandora is an open-source software framework developed since 2006 by the University of Prince Edward Island's Robertson Library. The Islandora framework is designed to ease the management of security and workflow for digital assets, and to help implementers create custom interfaces for display, search, and discovery. Turnkey options are provided via tools and modules ("solution packs") designed to support the work of a particular knowledge domain (such as chemistry), a particular content type (such as a digitized newspaper), or a particular task (such as {TEI} encoding). While it does not yet have native support for {TEI}, Islandora provides a promising basis on which digital humanities scholars could manage the creation, editing, validation, display, and comparison of {TEI}-encoded text. {UPEI}'s {IslandLives} project, with its forthcoming solution pack, provides insight into how an Islandora version 6 installation can support {OCR} text extraction, automatic structural/semantic encoding of text, and web-based {TEI} editing and display functions for site administrators. This article introduces the Islandora framework and its suitability for {TEI}, describes the {IslandLives} approach in detail, and briefly discusses recent work and future directions for {TEI} work in Islandora. The authors hope that interested readers may help contribute to the expansion of {TEI}-related services and features available to be used with Islandora.},
	issue = {Issue 5},
	journaltitle = {Journal of the Text Encoding Initiative},
	author = {Stapelfeldt, Kirsta and Moses, Donald},
	editora = {Blanke, Tobias and Romary, Laurent},
	editoratype = {collaborator},
	urldate = {2013-10-17},
	date = {2013-06-24},
	langid = {english},
	keywords = {goal\_Dissemination},
}

@report{hilse_implementing_2006,
	title = {Implementing persistent identifiers},
	url = {http://webdoc.sub.gwdg.de/edoc/ah/2006/hilse_kothe/urn%3Anbn%3Ade%3Agbv%3A7-isbn-90-6984-508-3-8.pdf},
	abstract = {Traditionally, organisations have relied on {URL} hyperlinks to provide interested
parties with access to their digitised content via the internet. However, over time,
more and more of these hyperlinks are ‘broken’. The {URL} relies on providing the
specific location details for a document. When, for example, an organisation’s
website is re-organised and its directories are renamed, the {URL} no longer pro-
vides a correct location path, thus rendering the documents effectively inacces-
sible to the end-user.
In the mid 1990s, a number of schemes were developed that, rather than rely-
ing on the precise address of a document, introduced the idea of name spaces for
recording the names and locations of documents. The identifiers for documents
are registered centrally. When an end-user wishes to access a certain document,
the identifier in his request is ‘resolved’, i.e. the correct document is retrieved,
without the end-user needing to know the exact location of the document. This
report describes a number of such schemes in detail.
Key concepts introduced include Handles, Digital Object Identifiers ({DOIs}),
Archival Resource Keys ({ARKs}), Persistent Uniform Resource Locators
({PURLs}), Uniform Resource Names ({URNs}), National Bibliography Numbers
({NBNs}), and the {OpenURL}. These schemes are described with examples and
extensive references.
The report emphasises that supporting persistent identification requires admin-
istrative effort and commitment. The systems presented support these administra-
tive tasks but do not render them obsolete. All changes in location, ownership or
metadata must be reflected in the name-space system – causing the organisations
that run an identification system to incur costs. To assist organisations that wish
to implement a persistent identification scheme, the report details questions that
need to be addressed and offers possible strategies to tackle a number of scenarios.
Organisations are strongly recommended to investigate collaboration with part-
ners with existing schemes that have similar problems to solve and to choose the
syntax for their persistent identifiers in such a way that they can be integrated into
any of the schemes introduced in this report.},
	pages = {58},
	institution = {Consortium of European Research Libraries, European Commission on Preservation and Access},
	author = {Hilse, Hans-Werner and Kothe, Jochen},
	date = {2006},
	langid = {english},
	keywords = {act\_Identifying, obj\_Research},
}

@article{shuai_how_2012,
	title = {How the Scientific Community Reacts to Newly Submitted Preprints: Article Downloads, Twitter Mentions, and Citations},
	url = {http://arxiv.org/abs/1202.2461},
	shorttitle = {How the Scientific Community Reacts to Newly Submitted Preprints},
	abstract = {We analyze the online response of the scientific community to the preprint publication of scholarly articles. We employ a cohort of 4,606 scientific articles submitted to the preprint database {arXiv}.org between October 2010 and April 2011. We study three forms of reactions to these preprints: how they are downloaded on the {arXiv}.org site, how they are mentioned on the social media site Twitter, and how they are cited in the scholarly record. We perform two analyses. First, we analyze the delay and time span of article downloads and Twitter mentions following submission, to understand the temporal configuration of these reactions and whether significant differences exist between them. Second, we run correlation tests to investigate the relationship between Twitter mentions and both article downloads and article citations. We find that Twitter mentions follow rapidly after article submission and that they are correlated with later article downloads and later article citations, indicating that social media may be an important factor in determining the scientific impact of an article.},
	journaltitle = {{arXiv}:1202.2461},
	author = {Shuai, Xin and Pepe, Alberto and Bollen, Johan},
	urldate = {2012-02-15},
	date = {2012-02-11},
	langid = {english},
}

@report{mla_committee_on_information_technology_guidelines_2012,
	location = {New York},
	title = {Guidelines for Evaluating Work in Digital Humanities and Digital Media},
	url = {http://www.mla.org/guidelines_evaluation_digital},
	institution = {{MLA}},
	author = {{MLA} Committee on Information Technology},
	date = {2012-04},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Research},
}

@report{uhlir_for_2012,
	title = {For Attribution, Developing Data Attribution and Citation Practices and Standards summary of an international workshop},
	url = {http://www.nap.edu/catalog.php?record_id=13564},
	abstract = {The growth of electronic publishing of literature has created new challenges, such as the need for mechanisms for citing online references in ways that can assure discoverability and retrieval for many years into the future. The growth in online datasets presents related, yet more complex challenges. It depends upon the ability to reliably identify, locate, access, interpret, and verify the version, integrity, and provenance of digital datasets. Data citation standards and good practices can form the basis for increased incentives, recognition, and rewards for scientific data activities that in many cases are currently lacking in many fields of research. The rapidly-expanding universe of online digital data holds the promise of allowing peer-examination and review of conclusions or analysis based on experimental or observational data, the integration of data into new forms of scholarly publishing, and the ability for subsequent users to make new and unforeseen uses and analyses of the same data-either in isolation, or in combination with, other datasets.

The problem of citing online data is complicated by the lack of established practices for referring to portions or subsets of data. There are a number of initiatives in different organizations, countries, and disciplines already underway. An important set of technical and policy approaches have already been launched by the U.S. National Information Standards Organization ({NISO}) and other standards bodies regarding persistent identifiers and online linking.


The workshop summarized in For Attribution -- Developing Data Attribution and Citation Practices and Standards: Summary of an International Workshop was organized by a steering committee under the National Research Council's ({NRC}'s) Board on Research Data and Information, in collaboration with an international {CODATA}-{ICSTI} Task Group on Data Citation Standards and Practices. The purpose of the symposium was to examine a number of key issues related to data identification, attribution, citation, and linking to help coordinate activities in this area internationally, and to promote common practices and standards in the scientific community.},
	institution = {National Research Council (U.S.). Board on Research Data and Information},
	author = {Uhlir, P. F},
	urldate = {2013-03-11},
	date = {2012},
	langid = {english},
	keywords = {act\_Identifying},
}

@report{bates_final_2006,
	location = {London},
	title = {Final report and recommendations of the 'Peer review of digital resources for the arts and humanities' project (Arts and Humanities Research Council ({AHRC}) {ICT} Strategy Project)},
	url = {http://www.history.ac.uk/projects/peer-review},
	abstract = {The mechanisms for the evaluation and peer review of the traditional print outputs of scholarly research in the arts and humanities are well established, but no equivalent exists for assessing the value of digital resources and of the scholarly work which leads to their creation. This project proposes to establish a framework for evaluating the quality, sustainability and impact over time of digital resources for the arts and humanities, using History, in its broadest sense, as a case study.

If digital resources are genuinely to contribute to the research profile of {UK} Higher Education Institutions, it is essential that a framework for evaluating digital resources, and ensuring quality control, be established. A consistently-applied system of peer review (of both the intellectual content and the technical architecture) would serve to reassure academics and their host institutions of the worth of time spent in the creation of digital resources, establish those types of resource which are of most use and interest to the academic community, contribute to the development of common standards and guidelines for accessibility and usability, and inform proposals to ensure the sustainability and preservation of high-quality scholarly material.},
	institution = {Institute of Historical Research, University of London},
	author = {Bates, David and Nelson, Janet and Rouché, Charlotte and Winters, Jane},
	date = {2006},
	langid = {english},
}

@report{unsworth_evaluating_2001,
	title = {Evaluating Digital Scholarship, Promotion \& Tenure Cases},
	url = {http://artsandsciences.virginia.edu/dean/facultyemployment/evaluating_digital_scholarship.html},
	abstract = {Evaluating the work of a scholar traditionally involves several elements:

    Reading and judging the work,
    Looking to outside experts in the same area for their assessment of the work,
    Taking note of the work’s formal peer review, from book and journal editors,
    Considering citation of the research in the field at large, and
    Sometimes, considering the impact that this work has had on the general public.

Each of these steps has a parallel in the evaluation of electronic scholarship, but not all forms of evaluation are always available.  And some issues that would not arise in evaluating scholarship in print are important to consider when evaluating electronic scholarship.},
	institution = {University of Virginia, Arts \& Sciences, Office of the Dean},
	author = {Unsworth, John},
	date = {2001},
	langid = {english},
}

@report{wissenschaftsrat_empfehlungen_2010,
	location = {Köln},
	title = {Empfehlungen zur vergleichenden Forschungsbewertung in den Geisteswissenschaften},
	url = {http://www.wissenschaftsrat.de/download/archiv/10039-10.pdf},
	number = {Drs. 10 03 9 -10},
	institution = {Wissenschaftsrat},
	author = {{Wissenschaftsrat}},
	date = {2010-06},
	langid = {german},
	keywords = {meta\_DefinePolicy},
}

@report{ausschuss_fur_wissenschaftliche_bibliotheken_und_informationssysteme_unterausschuss_fur_informationsmanagement_empfehlungen_2009,
	location = {Bonn},
	title = {Empfehlungen zur gesicherten Aufbewahrung und Bereitstellung digitaler Forschungsprimärdaten},
	url = {http://www.dfg.de/download/pdf/foerderung/programme/lis/ua_inf_empfehlungen_200901.pdf},
	institution = {{DFG}},
	author = {{Ausschuss für Wissenschaftliche Bibliotheken und Informationssysteme, Unterausschuss für Informationsmanagement}},
	date = {2009-01},
	langid = {german},
	keywords = {goal\_Storage, meta\_DefinePolicy, obj\_Data},
}

@report{wissenschaftsrat_empfehlungen_2006,
	location = {Berlin},
	title = {Empfehlungen zur Entwicklung und Förderung der Geisteswissenschaften in Deutschland (Drs. 7068-06)},
	url = {http://www.wissenschaftsrat.de/download/archiv/7068-06.pdf},
	institution = {Wissenschaftsrat},
	author = {{Wissenschaftsrat}},
	date = {2006-01},
	langid = {german},
	keywords = {meta\_DefinePolicy},
}

@collection{neuroth_kleine_2010,
	edition = {Versio 2.3},
	title = {Eine kleine Enzyklopädie der digitalen Langzeitarchivierung},
	rights = {(c) 2004 Niedersächsische Staats- und Universitätsbibliothek Göttingen, Germany},
	url = {http://www.nestor.sub.uni-goettingen.de/handbuch/index.php},
	abstract = {nestor ist ein vom {BMBF} gefördertes Projekt zum Aufbau eines Kompetenznetzwerkes zur Langzeitarchivierung digitaler Objekte in Deutschland. Die nestor Informationsdatenbank ist Bestandteil eines Subject Gateways zu Fragen der Langzeitarchivierung digitaler Objekte., The project's objective is to create a network of expertise in digital preservation for Germany. The subject gateway offers informations on all topics of long time preservation of digital objects},
	publisher = {nestor},
	editor = {Neuroth, {HeikeKarsten} and Huth, Karsten and Oßwald, Achim and Scheffel, Regine and Strathmann, Stefan},
	urldate = {2012-09-16},
	date = {2010},
	langid = {german},
	keywords = {act\_Archiving, bigdata{\textasciitilde}, goal\_Storage, meta\_GiveOverview, obj\_Data},
}

@online{international_doi_foundation_doi_nodate,
	title = {{DOI} Handbook Introduction},
	url = {http://www.doi.org/doi_handbook/1_Introduction.html},
	abstract = {This is the web site of the International {DOI} Foundation ({IDF}), which provides information on the {DOI} (Digital Object Identifier) system and its activities. The {DOI} system provides a technical and social infrastructure for the registration and use of persistent interoperable identifiers for use on digital networks. The {DOI} system implements the Handle System and the indecs Framework.},
	author = {{International DOI Foundation}},
	urldate = {2013-02-11},
	langid = {english},
	keywords = {act\_Identifying, obj\_Research},
}

@report{bader_digitale_2012,
	location = {Gießen},
	title = {Digitale Wissenschaftskommunikation 2010-2011 – Eine Online-Befragung. Unter Mitarbeit von Jurgita Baranauskaite, Kerstin Engel und Sarah Rögl},
	url = {http://geb.uni-giessen.de/geb/volltexte/2012/8539/},
	abstract = {Digitale Formate wie Mailinglists, Blogs, digitale Rezensionsportale und Open-Peer-Review-Zeitschriften haben in den letzten Jahren verstärkt Eingang in die Praxis der Kommunikation unter Wissenschaftlern gefunden und damit die Voraussetzung für Veränderungen in der Wissenschaftskommunikation und der Wissenschaftspraxis allgemein geschaffen.
Der vorliegende Band bietet die Auswertung und Analyse einer Online-Befragung zur digitalen Wissenschaftskommunikation von {WissenschaftlerInnen} in Deutschland, die in der Zeit vom 23.06.2010 bis 09.03.2011 in zwei Wellen durchgeführt wurde (Rücklauf: 1053 Fragebogen). Die wesentlichen Ziele der hier ausgewerteten Befragung bestehen darin, für unterschiedliche Gruppen von {WissenschaftlerInnen} in Deutschland Aufschluss zu bekommen über ihre Nutzung digitaler Formate in ihrer wissenschaftlichen Praxis und über ihre Einschätzung des Potenzials und der tatsächlichen Nutzung dieser Formate in verschiedenen wissenschaftlichen Disziplinen.
In der Einleitung werden zunächst methodische Aspekte der Befragung diskutiert, es wird die Datenbasis beschrieben und die Struktur des Fragebogens erläutert. Die Einleitung schließt mit einem Überblick über die wichtigsten Ergebnisse der Befragung. Diese lassen sich folgendermaßen zusammenfassen: 1. Wissenschaftler nutzen digitale Formate derzeit vor allem dann, wenn diese Nutzung sich unmittelbar in ihre schon etablierte wissenschaftliche Praxis einbetten lässt und dort zur (besseren) Lösung schon existierender kommunikativer Aufgaben beiträgt (z.B. E-Mail). 2. Wissenschaftler nutzen digitale Formate dann, wenn die Nutzung auf erkennbare Art und Weise effizient und wenig zeitintensiv ist, z.B. die Nutzung von Mailinglists als Servicelists zur Information über Calls for Papers etc. 3. Dagegen gilt für viele Wissenschaftler die aktive Teilnahme an Diskussionen oder gar Kontroversen in digitalen Formaten (Mailinglists, Blogs, Open Peer Review) als wenig attraktiv, weil sie zeitintensiv und im Hinblick auf die Reputation auch risikoträchtig sein kann 4. Die kollaborative Nutzung von geeigneten digitalen Formaten (Blogs, Wikis) scheint bisher noch wenig etabliert zu sein. 5. Grundlegende Veränderungen der Wissenschaftspraxis, die man von der Nutzung von digitalen Formaten in der Wissenschaftskommunikation erwarten könnte, scheinen bisher in der Breite der Wissenschaften in Deutschland noch nicht eingetreten zu sein. Wir sehen hier einen Kontrast zwischen der Existenz von erfolgreichen Modellen für die Nutzung digitaler Formate auf der einen Seite und der zögernden Aufnahme dieser Formate auf der anderen. Die folgenden Kapitel untersuchen jeweils die Detailbefunde zu einzelnen Formaten, in der Reihenfolge Open-Peer-Review-Zeitschriften, wissenschaftliche Mailinglists und Blogs.
Kapitel 2 skizziert zunächst Formen der Open Peer Review und stellt dann die Ergebnisse der Befragung zu diesem Format öffentlicher Begutachtung im Überblick dar. Dabei ergeben sich differenzierte Befunde für die Nutzung und die Einschätzung des Formats in unterschiedlichen Fächerzonen und Statusgruppen (vgl. auch Kap. 5). Insgesamt wird die Veröffentlichung von Gutachten zu wissenschaftlichen Aufsätzen und die kontroverse öffentliche Diskussion von Aufsätzen und Gutachten häufig mit Skepsis betrachtet, insbesondere im Bereich der Geistes- und Kulturwissenschaften. Die damit verbundene erhöhte Transparenz wird vor allem von jungen {WissenschaftlerInnen} begrüßt.
Kapitel 3 widmet sich der Einschätzung und Nutzung von wissenschaftlichen Mailingslists. Unter den verschiedenen Funktionen – Information, Diskussion und Kollaboration – ist es besonders die Funktion der Information über wissenschaftsorganisatorische Dinge wie Calls for Papers („Servicelist“), die geschätzt wird, insbesondere im Bereich der Geistes- und Kulturwissenschaften. Die Nutzung als Forum für Diskussion und Kollaboration tritt dahinter weit zurück.
Kapitel 4 behandelt die Rolle von wissenschaftlichen Blogs. Obwohl es international hochkarätige wissenschaftliche Blogs gibt, werden in unserer Stichprobe Blogs nur in verschwindendem Umfang zur Wissenschaftskommunikation genutzt. Tendenziell gelten sie als unwissenschaftlich, und ihre Nutzung gilt als Zeitverschwendung.
In Kapitel 5 werden die Befunde statt aus der Perspektive der einzelnen Formate aus der Perspektive von Fächerzonen und einzelnen Fächern betrachtet, mit dem Ziel, für diese Bereiche jeweils ein Profil der Nutzung digitaler Formate zu erstellen. Ausführlich werden die Daten der Geistes- und Kulturwissenschaften sowie die der Naturwissenschaften betrachtet. Dazuhin wird jeweils eine Sondierung zum Fach der Geschichtswissenschaft sowie zu einer Vertreterin der Sprachwissenschaft unternommen. Weiterhin werden Spezifika der Statusgruppen von den Doktoranden bis zu den (emeritierten) Professoren untersucht.
Im Schlusskapitel werden die Ergebnisse der Untersuchung nochmals im Zusammenhang mit den bekannten Potenzialen der behandelten Formate betrachtet, und es werden Perspektiven für die Entwicklung der digitalen Wissenschaftskommunikation skizziert.},
	institution = {Zentrum für Medien und Interaktivität, {JUL} Gießen},
	author = {Bader, Anita and Fritz, Gerd and Gloning, Thomas},
	date = {2012},
	langid = {german},
	keywords = {X-{CHECK}, act\_Communicating},
}

@report{gloning_digitale_2011,
	location = {Gießen},
	title = {Digitale Wissenschaftskommunikation - Formate und ihre Nutzung},
	url = {http://geb.uni-giessen.de/geb/volltexte/2011/8227/},
	abstract = {Die Beiträge des vorliegenden Bandes behandeln neuere Entwicklungen der internen Wissenschaftskommunikation in digitalen Medien, insbesondere im Hinblick auf die Nutzung von interaktiven digitalen Formaten wie Mailinglists, wissenschaftlichen Blogs, Wikis und Open-Peer-Review-Zeitschriften. In Überblicksdarstellungen, detaillierten Fallstudien und Interviews mit Protagonisten der digitalen Kommunikationspraxis werden die Potenziale dieser Formate ausgelotet, Nutzungsprobleme und Entwicklungstendenzen skizziert und Formen der tatsächlichen Nutzung mit kommunikationsanalytischen Mitteln beschrieben. Gleichzeitig wird auf die Frage eingegangen, wie Veränderungen der Wissenschaftskommunikation sich auf die wissenschaftliche Praxis generell auswirken. Dabei werden theoretische und methodische Fragen behandelt wie etwa die Unterscheidung von Format und Nutzungsweisen eines Formats, die Aspekte einer evolutionären Betrachtung der Entwicklung von Kommunikationsformen in digitalen Formaten, die Probleme einer Texttypologie für Blogbeiträge, die Beschreibung des Themenmanagements in interaktiven Formaten oder der Status von Kontroversen in der Wissenschaftskommunikation.
In den ersten drei Beiträgen wird das derzeitige System von digitalen Formaten in der Wissenschaftskommunikation im Überblick beschrieben, u.a. mit Bezug auf die Entwicklungen des „Web 2.0“, die Entstehung von Formatvarianten und Formatkonstellationen, Fragen der Qualitätssicherung, die Möglichkeiten der kollaborativen Nutzung von digitalen Formaten sowie die Entstehung und Entwicklung von sozialen Netzwerken im Bereich der digitalen Wissenschaftskommunikation. Dabei werden gleichzeitig auch Entwicklungsperspektiven in diesem Bereich erkennbar.
Es folgt eine Gruppe von Fallstudien, die sich mit der Nutzung von Mailinglists (quantitativ und im historischen Längsschnitt), dem Open-Peer-Review-Verfahren, Texttypen in Wissenschaftsblogs sowie Kontroversen in digitalen Formaten der Wissenschaftskommunikation beschäftigen. Aus der Insider-Perspektive behandelt Ulrich Pöschl, der Hauptherausgeber von „Athmospheric Chemistry and Physics“, Fragen des Open-Peer-Review-Verfahrens. Die Fallstudien zielen auf bisher kaum dokumentierte empirische Details und verstehen sich gleichzeitig als Erprobungen von methodischen Prinzipien der linguistischen Kommunikationsanalyse für den Bereich digitaler Formate. Sie bieten z.T. überraschende Ergebnisse wie z.B. die Beobachtung, dass Mailinglists in bestimmten Funktionen weiterhin eine beträchtliche „Vitalität“ zeigen.
Abschließend folgen Interviews mit John Baez und Urs Schreiber, den Gründern des mathematischen Gruppenblogs „The n-Category Café“, sowie mit Anatol Stefanowitsch, dem Gründer des „Bremer Sprachblogs“ und heutigen Betreiber des „Sprachlog“. Diese Interviews geben vielfältige Hinweise auf die Praxis des wissenschaftlichen Blogs und die Reflexion dieser Praxis durch die Protagonisten selbst.},
	institution = {Justus-Liebig-Universität Gießen, Zentrum für Medien und Interaktivität},
	author = {Gloning, Thomas and Fritz, Gerd},
	date = {2011},
	langid = {german},
	keywords = {act\_Publishing, obj\_ResearchResults, t\_Blogging},
}

@book{andersen_digital_2004,
	location = {Armonk, N.Y.},
	title = {Digital scholarship in the tenure, promotion, and review process},
	isbn = {0765611139  9780765611130},
	url = {http://www.tandfonline.com/doi/pdf/10.1080/01972240490508144#.UozJvSeiLk0},
	abstract = {To receive tenure college and university professors have long been required to write scholarly monographs or articles, engage in serious research, and teach effectively. In recent years, however, the emergence of digital scholarship has revolutionized -- and complicated -- the picture in unexpected ways as new electronic media have enabled academics to communicate scholarly material in innovative formats such as websites, {PowerPoint} presentations, {CD}-{ROMs}, and virtual reality "tours." Despite this growing output of sophisticated digital scholarship, there has been little attempt to set standards, define basic issues and concepts, or integrate electronic scholarship into the tenure debate.

This collection of cutting-edge articles marks the first effort to evaluate the place of digital scholarship in the tenure, promotion, and review process. As a primer aimed at scholars, faculty members, and department chairs in the humanities, social sciences, and other fields, as well as deans, provosts, and university administrators, this collection examines the evolution of nontraditional scholarship, analyzes the various formats, and suggests guidelines for assessment on a scholarly level. It also examines the impact of digital scholarship in the classroom and academy and explores new directions for the future. This book will help shape policy in the murky world of tenure review and become a point of reference for scholars and administrators everywhere.},
	pagetotal = {277},
	publisher = {M.E. Sharpe},
	author = {Andersen, Deborah Lines},
	date = {2004},
	langid = {english},
}

@book{deegan_digital_2006,
	location = {London},
	title = {Digital preservation},
	isbn = {1856044858},
	url = {http://www.amazon.de/Digital-Preservation-Futures-Marilyn-Deegan/dp/1856044858},
	series = {Digital futures},
	abstract = {Digital preservation is an issue of huge importance to the library and information profession right now. With the widescale adoption of the internet and the rise of the world wide web, the world has been overwhelmed by digital information. Digital data is being produced on a massive scale by individuals and institutions: some of it is born, lives and dies only in digital form, and it is the potential death of this data, with its impact on the preservation of culture, that is the concern of this book. So how can information professionals try to remedy this? Digital preservation is a complex issue involving many different aspects and views, and each chapter of this edited collection is written by an international expert on the topic. Many case studies and examples are used to ground the ideas and theories in real concerns and practice. The book will arm the information professional with the knowledge they need about these important and pressing issues, and give examples of best practice to help find solutions. Its chapters cover: key issues in digital preservation; strategies for digital preservation; the status of preservation metadata in the digital library community; web archiving; the costs of digital preservation; it's money that matters in long-term preservation; European approaches to digital preservation; and, digital preservation projects: case studies. This is an indispensable guide for information managers, librarians and archivists worldwide. Others in the information and culture world, such as museum curators, media professionals and web content providers, will also find it essential reading, as will students of digital culture on library and information studies courses and within other disciplines.},
	pagetotal = {260},
	publisher = {Facet},
	editora = {Deegan, Marilyn and Tanner, Simon},
	editoratype = {collaborator},
	date = {2006},
	langid = {english},
	keywords = {bigdata{\textasciitilde}, goal\_Storage, obj\_AnyObject},
}

@book{hedges_digital_2014,
	location = {[S.l.]},
	title = {Digital asset management handbook.},
	isbn = {1856049353 9781856049351},
	url = {http://www.amazon.de/Digital-Asset-Management-Theory-Practice/dp/1856049353},
	abstract = {This practical handbook provides information professionals with everything they need to know to effectively manage digitial content and information. The book addresses digital asset management ({DAM}) from a practitioner's point of view but also introduces readers to the theoretical background to the subject. It will thus equip readers with a range of essential strategic, technical and practical skills required to direct digital asset management activities within their area of business, while also providing them a well-rounded and critical understanding of the issues across domains. The Digital Asset Management Handbook includes an evolving case study that serves to illustrate the topics and issues addressed in each chapter, as well as a sequence of practical exercises using freely available {DAM} software. Readership: Information professionals who work (or aim to work) in the digital content industries and managers of digital assets of various forms. Cultural and memory institutions, digital archives, and any areas of science, government and business organisation where there is a need to curate digital assets. Students taking {LIS} graduate courses worldwide.},
	publisher = {Facet Publishing},
	author = {Hedges, Mark},
	date = {2014},
	langid = {english},
	keywords = {goal\_Dissemination, goal\_Storage, obj\_AnyObject},
}

@collection{dobreva_digital_2014,
	location = {London},
	title = {Digital archives: management, use and access},
	isbn = {1856049345 9781856049344},
	url = {http://www.facetpublishing.co.uk/title.php?id=9344&category_code=301},
	shorttitle = {Digital archives},
	abstract = {This landmark edited collection offers a wide-ranging overview of how rapid technological changes and the push for providing wide access to digitized cultural heritage holdings are changing the landscape of archives. This book provides a set of inspirational and informative chapters from international experts, which will help the readers understand the drivers for change in archives and their implications. Reassessment of the role of archives in the digital environment will serve to develop critical approaches to current trends in the broader heritage sector, including cultural industries experimenting with sustainable business models for cultural production, digitization of analogue cultural heritage, and the related {IPR} issues surrounding the re-use of digital objects and data for research, education, advocacy and art. Contributors also present state-of-the-art solutions in building digital archives on networked infrastructure, trusted digital repositories to ensure long-term access, and tools to serve emerging needs in digital humanities. Readership: Digital archivists and practitioners involved in the design and support of digital archives; professionals and researchers involved in projects working with digital archival materials; students in library, information and archive studies.},
	publisher = {Facet Publishing},
	editor = {Dobreva, Milena and Ivacs, Gabriellas},
	date = {2014-06},
	langid = {english},
	keywords = {goal\_Storage, obj\_AnyObject, obj\_Data},
}

@online{mounier_werkstatt_2011,
	title = {Die Werkstatt des Historikers öffnen: Soziale Medien und Wissenschaftsblogs},
	url = {http://dhdhi.hypotheses.org/591},
	shorttitle = {Die Werkstatt des Historikers öffnen},
	abstract = {In welcher Weise verändern die digitalen Technologien die Arbeitsbedingungen des Historikers? Bereits seit mehreren Jahrzehnten gibt es darauf Antworten. Die Erstellung quantitativer Datenbanken, die Digitalisierung wichtiger Quellen, die kartografische Darstellung und die Analyse sozialer Netzwerke mit {IT}-Werkzeugen sind dabei die ältesten und wichtigsten Meilensteine. Die Retrodigitalisierung und Online-Veröffentlichung akademischer Literatur der Disziplin, seien es Zeitschriften oder Bücher, stellen eine weitere Etappe in dieser Richtung dar. Die digitale Technologie hat bis heute zugleich den Werkzeugkasten des Historikers – um den schönen Ausdruck aus dem gleichnamigen Blog La Boite à Outil des Historien aufzugreifen – und seine Publikationsmöglichkeiten grundlegend verändert.},
	titleaddon = {Digital Humanities am {DHIP}},
	author = {Mounier, Pierre},
	urldate = {2011-11-15},
	date = {2011-11-04},
	langid = {german},
	keywords = {X-{CHECK}},
}
@article{stober_internet_2004,
	title = {Das Internet als Medium geistes- und kulturwissenschaftlicher Publikation. Pragmatische und epistemologische Fragestellungen},
	url = {http://web.fu-berlin.de/phin/beiheft2/b2t15.htm},
	abstract = {The article starts with pointing out both the fundamental advantages and the actual problems created by electronic publishing in the humanities while detailing recent approaches which aim at solving these problems. In addition to this pragmatic perspective which illustrates how electronic publishing might optimize communication within the humanities, the article focuses furthermore on an epistemological perspective considering the way in which the hypertextuality of digital media could influence scientific discourse and contribute to new ways of knowledge representation.},
	pages = {282--296},
	number = {2},
	journaltitle = {Philologie im Netz ({PhiN}): Beihefte},
	author = {Stöber, Thomas},
	date = {2004},
	langid = {german},
	keywords = {goal\_Dissemination, obj\_ResearchResults},
}

@article{conway_curating_2011,
	title = {Curating Scientific Research Data for the Long Term: A Preservation Analysis Method in Context},
	volume = {6},
	rights = {Copyright for articles published in this journal is retained by the authors, with first publication rights granted to the University of Bath. By virtue of their appearance in this open access journal, articles are free to use, with proper attribution, in educational and other non-commercial settings.      This work is licenced under a  Creative Commons Licence .},
	issn = {1746-8256},
	url = {http://www.ijdc.net/index.php/ijdc/article/view/182},
	shorttitle = {Curating Scientific Research Data for the Long Term},
	abstract = {The challenge of digital preservation of scientific data lies in the need to preserve not only the dataset itself but also the ability it has to deliver knowledge to a future user community. A true scientific research asset allows future users to reanalyze the data within new contexts. Thus, in order to carry out meaningful preservation we need to ensure that future users are equipped with the necessary information to re-use the data. This paper presents an overview of a preservation analysis methodology which was developed in response to that need on the {CASPAR} and Digital Curation Centre {SCARP} projects. We intend to place it in relation to other digital preservation practices, discussing how they can interact to provide archives caring for scientific data sets with the full arsenal of tools and techniques necessary to rise to this challenge.},
	number = {2},
	journaltitle = {International Journal of Digital Curation},
	author = {Conway, Esther and Giaretta, David and Lambert, Simon and Matthews, Brian},
	urldate = {2011-11-26},
	date = {2011-10-07},
	langid = {english},
	keywords = {act\_Archiving},
}

@misc{berners-lee_cool_1998,
	title = {Cool {URIs} don't Change},
	url = {http://www.w3.org/Provider/Style/URI},
	abstract = {There are no reasons at all in theory for people to change {URIs} (or stop maintaining documents), but millions of reasons in practice.

In theory, the domain name space owner owns the domain name space and therefore all {URIs} in it. Except insolvency, nothing prevents the domain name owner from keeping the name. And in theory the {URI} space under your domain name is totally under your control, so you can make it as stable as you like. Pretty much the only good reason for a document to disappear from the Web is that the company which owned the domain name went out of business or can no longer afford to keep the server running. Then why are there so many dangling links in the world? Part of it is just lack of forethought. Here are some reasons you hear out there:},
	author = {Berners-Lee, Tim},
	date = {1998},
	langid = {english},
	keywords = {act\_Identifying, obj\_Research},
}

@article{lawrence_citation_2011,
	title = {Citation and Peer Review of Data: Moving Towards Formal Data Publication},
	volume = {6},
	rights = {Copyright for articles published in this journal is retained by the authors, with first publication rights granted to the University of Bath. By virtue of their appearance in this open access journal, articles are free to use, with proper attribution, in educational and other non-commercial settings.      This work is licenced under a  Creative Commons Licence .},
	issn = {1746-8256},
	url = {http://www.ijdc.net/index.php/ijdc/article/view/181},
	shorttitle = {Citation and Peer Review of Data},
	abstract = {This paper discusses many of the issues associated with formally publishing data in academia, focusing primarily on the structures that need to be put in place for peer review and formal citation of datasets. Data publication is becoming increasingly important to the scientific community, as it will provide a mechanism for those who create data to receive academic credit for their work and will allow the conclusions arising from an analysis to be more readily verifiable, thus promoting transparency in the scientific process. Peer review of data will also provide a mechanism for ensuring the quality of datasets, and we provide suggestions on the types of activities one expects to see in the peer review of data. A simple taxonomy of data publication methodologies is presented and evaluated, and the paper concludes with a discussion of dataset granularity, transience and semantics, along with a recommended human-readable citation syntax.},
	number = {2},
	journaltitle = {International Journal of Digital Curation},
	author = {Lawrence, Bryan and Jones, Catherine and Matthews, Brian and Pepler, Sam and Callaghan, Sarah},
	urldate = {2011-11-26},
	date = {2011-10-07},
	langid = {english},
	keywords = {act\_Publishing, meta\_Assessing, obj\_Data, obj\_Metadata},
}

@incollection{morrison_chapter_2011,
	title = {Chapter Two: Scholarly Communication in Crisis},
	url = {http://pages.cmns.sfu.ca/heather-morrison/chapter-two-scholarly-communication-in-crisis/},
	abstract = {"Scholarly communication at present is a complex system characterized by expansion of capitalism into scholarly publishing and a process of rationalization that at times leads to irrational results in conflict with the basic goals or values of scholars. The increasing enclosure of knowledge and information through the concept of intellectual property is key in the process of commodification of resources once considered a classical public good as nonrivalrous and nonexcludable. Alternatives identified to date include the commons, cooperative approaches, open access and emerging new publishers such as libraries."},
	booktitle = {Freedom for scholarship in the internet age. Doctoral dissertation (in process)},
	publisher = {Simon Fraser University, School of Communication},
	author = {Morrison, Heather},
	urldate = {2012-01-09},
	date = {2011-11-29},
	langid = {english},
}

@report{kelleher_changing_2011,
	location = {Young Researchers Forum {ESF} Humanities, June 2011, Maynooth, Ireland.},
	title = {Changing Publication Practices in the Humanities.},
	url = {http://www.esf.org/fileadmin/Public_documents/Publications/Changing_Publication_Cultures_Humanities.pdf},
	institution = {European Science Foundation ({ESF})},
	author = {Kelleher, Margaret and Hoogland, Eva},
	date = {2011},
	langid = {english},
	keywords = {goal\_Dissemination, meta\_Advocating, obj\_ResearchResults},
}

@book{chambers_catalogue_2010,
	location = {London},
	title = {Catalogue 2.0},
	isbn = {9781856047166  1856047164},
	url = {http://www.ala.org/transforminglibraries/catalogue-20},
	abstract = {New digital technologies, the Internet, and user expectations have changed the role of the catalogue in libraries considerably in recent years. This timely book takes into account developments that influence catalogue potential and patrons’ needs, such as competition from popular Web sites like Facebook, Twitter, and Wikipedia. Here, key leaders in the field like Karen Calhoun, past {OCLC} Vice-President, and Marshall Breeding, author of Cloud Computing for Libraries, explore concepts like:What do your users want?},
	publisher = {Facet},
	author = {Chambers, Sally},
	date = {2010},
	langid = {english},
	keywords = {act\_Query/Retrieve, obj\_Data/Databases},
}

@article{cronin_bibliometrics_2001,
	title = {Bibliometrics and beyond: some thoughts on web-based citation analysis},
	volume = {27},
	issn = {0165-5515},
	url = {http://jis.sagepub.com/cgi/doi/10.1177/016555150102700101},
	doi = {10.1177/016555150102700101},
	shorttitle = {Bibliometrics and beyond},
	abstract = {The idea of a unified citation index to the literature of science was first outlined by Eugene Garfield [1] in 1955 in the journal Science. Science Citation Index has since established itself as the gold standard for scientific information retrieval. It has also become the database of choice for citation analysts and evaluative bibliometricians worldwide. As scientific publication moves to the web, and novel approaches to scholarly communication and peer review establish themselves, new methods of citation and link analysis will emerge to capture often liminal expressions of peer esteem, influence and approbation. The web thus affords bibliometricians rich opportunities to apply and adapt their techniques to new contexts and content: the age of ‘bibliometric spectroscopy’ [2] is dawning.},
	pages = {1--7},
	number = {1},
	journaltitle = {Journal of Information Science},
	author = {Cronin, B.},
	urldate = {2012-06-18},
	date = {2001-02-01},
	langid = {english},
}

@article{bar-ilan_beyond_2012,
	title = {Beyond citations: Scholars' visibility on the social Web},
	volume = {abs/1205.5611},
	url = {http://arxiv.org/abs/1205.5611},
	abstract = {Traditionally, scholarly impact and visibility have been measured by counting publications and
citations in the scholarly literature. However, increasingly scholars are also visible on the Web,
establishing presences in a growing variety of social ecosystems. But how wide and established is
this presence, and how do measures of social Web impact relate to their more traditional
counterparts? To answer this, we sampled 57 presenters from the 2010 Leiden {STI} Conference,
gathering publication and citations counts as well as data from the presenters’ Web “footprints.”
We found Web presence widespread and diverse: 84\% of scholars had homepages, 70\% were on
{LinkedIn}, 23\% had public Google Scholar profiles, and 16\% were on Twitter. For sampled
scholars’ publications, social reference manager bookmarks were compared to Scopus and Web
of Science citations; we found that Mendeley covers more than 80\% of sampled articles, and that
Mendeley bookmarks are significantly correlated (r=.45) to Scopus citation counts.},
	journaltitle = {{CoRR}},
	author = {Bar-Ilan, Judit and Haustein, Stefanie and Peters, Isabella and Priem, Jason and Shema, Hadas and Terliesner, Jens},
	date = {2012},
	langid = {english},
}

@online{montgomery_assessing_2011,
	title = {Assessing Work on Digital Projects for Hiring and Tenure: Discouraging Young Scholars?},
	url = {http://hastac.org/blogs/katherine-f-montgomery/2011/11/27/assessing-work-digital-projects-hiring-and-tenure-discouragi},
	abstract = {One of the longest-running digital projects at the University of Iowa is the Walt Whitman Archive, a rich and constantly-growing interactive collection of Whitman’s life, letters, manuscripts, writing, criticism, recordings, and any and all digital (or digitizable) material on Walt Whitman.},
	titleaddon = {{HASTAC} Blog},
	author = {Montgomery, Katherine F.},
	urldate = {2011-11-29},
	date = {2011-11-27},
	langid = {english},
}

@book{harley_assessing_2010,
	location = {Berkeley},
	title = {Assessing the Future Landscape of Scholarly Communication: An Exploration of Faculty Values and Needs in Seven Disciplines},
	url = {http://www.escholarship.org/uc/item/15x7385g},
	abstract = {Since 2005, the Center for Studies in Higher Education ({CSHE}), with generous funding from the
Andrew W. Mellon Foundation, has been conducting research to understand the needs and
practices of faculty for in-progress scholarly communication (i.e., forms of communication
employed as research is being executed) as well as archival publication. This report brings
together the responses of 160 interviewees across 45, mostly elite, research institutions in seven
selected academic fields: archaeology, astrophysics, biology, economics, history, music, and
political science. The overview document summarizes the main practices we explored across all
seven disciplines: tenure and promotion, dissemination, sharing, collaboration, resource creation
and consumption, and public engagement. We published the report online in such a way that
readers can search various topics within and across case studies.∗ Our premise has always been
that disciplinary conventions matter and that social realities (and individual personality) will
dictate how new practices, including those under the rubric of Web 2.0 or cyberinfrastructure,
are adopted by scholars. That is, the academic values embodied in disciplinary cultures, as well
as the interests of individual players, have to be considered when envisioning new schemata for
the communication of scholarship at its various stages.
We identified five key topics, addressed in detail in the case studies, that require real attention:
(1) The development of more nuanced tenure and promotion practices that do not rely
exclusively on the imprimatur of the publication or easily gamed citation metrics,
(2) A reexamination of the locus, mechanisms, timing, and meaning of peer review,
(3) Competitive, high-quality, and affordable journals and monograph publishing platforms
(with strong editorial boards, peer review, and sustainable business models),
(4) New models of publication that can accommodate arguments of varied length, rich
media, and embedded links to data; plus institutional assistance to manage permissions
of copyrighted material, and
(5) Support for managing and preserving new research methods and products, including
components of natural language processing, visualization, complex distributed
databases, and {GIS}, among many others.
Although robust infrastructures are needed locally and beyond, the sheer diversity of scholars’
needs across the disciplines and the rapid evolution of the technologies themselves means that
one-size-fits-all solutions will almost always fall short. As faculty continue to innovate and pursue
new avenues in their research, both the technical and human infrastructure will have to evolve
with the ever-shifting needs of scholars. This infrastructure will, by necessity, be built within the
context of disciplinary conventions, reward systems, and the practice of peer review, all of which
undergird the growth and evolution of superlative academic endeavors.},
	publisher = {{CSHE}, {UC} Berkeley},
	author = {Harley, Diane and Acord, Sophia Krzys and Earl-Novell, Sarah and Lawrence, Shannon and King, C. Judson},
	date = {2010-01-01},
	langid = {english},
	keywords = {goal\_Dissemination, obj\_ResearchResults},
}

@report{finch_accessibility_2012,
	title = {Accessibility, sustainability, excellence: how to expand access to research publications},
	url = {http://www.researchinfonet.org/publish/finch/},
	abstract = {Report of the Working Group on Expanding Access to Published Research Findings – the Finch Group},
	author = {Finch},
	date = {2012-06},
	langid = {english},
}

@article{pablo_k._[i.e._paul_kirby]_academia_2012,
	title = {Academia in the Age of Digital Reproduction; Or, the Journal System, Redeemed},
	url = {http://thedisorderofthings.com/2012/02/08/academia-in-the-age-of-digital-reproduction-or-the-journal-system-redeemed/},
	abstract = {It took at least 200 years for the novel to emerge as an expressive form after the invention of the printing press.

So said Bob Stein in an interesting roundtable on the digital university from ...},
	journaltitle = {The Disorder Of Things},
	author = {{Pablo K. [i.e. Paul Kirby]}},
	urldate = {2012-02-09},
	date = {2012-02},
	langid = {english},
	keywords = {act\_Publishing, meta\_Advocating, obj\_ResearchResults},
}

@report{edgar_survey_2010,
	title = {A survey of the scholarly journals using open journal systems. Scholarly and Research Communication},
	url = {http://src-online.ca/index.php/src/article/view/24/41},
	abstract = {A survey of 998 scholarly journals that use Open Journal Systems ({OJS}), an open source journal software platform, captures the characteristics of an emerging class of scholar-publisher open access journals. The journals in the sample follow traditional norms for peer-reviewing, acceptance rates, and disciplinary focus, but as a group are distinguished by the number that offer open access to their content, growth rates in new titles, participation rates from developing countries, and extremely low operating budgets. The survey also documents the limited degree to which open source software can alter a field of communication, for {OJS} appears to have created a third path, dedicated to maximizing access to research and scholarship, as an alternative to traditional scholarly society and commercial publishing routes.},
	institution = {Stanford University},
	author = {Edgar, B.D. and Willinsky, John},
	date = {2010},
	langid = {english},
	keywords = {act\_Publishing, obj\_ResearchResults, obj\_Tools},
}

@article{nicholas_policy_2009,
	title = {A Policy Checklist for Enabling Persistence of Identifiers},
	volume = {15},
	issn = {1082-9873},
	url = {http://www.dlib.org/dlib/january09/nicholas/01nicholas.html},
	doi = {10.1045/january2009-nicholas},
	abstract = {One of the main tasks of the Persistent Identifier Linking Infrastructure ({PILIN}) project on persistent identifiers was to establish a policy framework for managing identifiers and identifier providers. A major finding from the project was that policy is far more important in guaranteeing persistence of identifiers than technology. Key policy questions for guaranteeing identifier persistence include: what entities should be assigned persistent identifiers, how should those identifiers be exposed to services, and what guarantees does the provider make on how long various facets of the identifiers will persist.

To make an informed decision about what to identify, information modelling of the domain is critical. Identifier managers need to know what can be identified discretely (including not only concrete artefacts like files, but also abstractions such as works, versions, presentations, and aggregations); and for which of those objects it is a priority for users and managers to keep track. Without working out what actually needs to be identified, the commitment to keep identifiers persistent becomes meaningless.

To make sure persistent identifiers meet these requirements, the {PILIN} project has formulated a six-point checklist for integrating identifiers into information management, which we present here.},
	number = {1},
	journaltitle = {D-Lib Magazine},
	author = {Nicholas, Nick and Ward, Nigel and Blinco, Kerry},
	urldate = {2013-02-01},
	date = {2009-01},
	langid = {english},
	keywords = {act\_Identifying, obj\_Research},
}

@online{tanner_new_2012,
	title = {A New Approach to Measuring Impact for Digitised Resources: do they change people’s lives?},
	url = {http://simon-tanner.blogspot.co.uk/2012/03/new-approach-to-measuring-impact-for.html},
	abstract = {This is a work in progress - more my notes and queries than a proper paper, stuff will change, references will be added. I wanted most to get this out there and to get your views, your inputs and your insights. Please comment, your thoughts are valued!},
	titleaddon = {When the Data hits the Fan!},
	author = {Tanner, Simon},
	date = {2012-03-27},
	langid = {english},
}

@article{lin_open_2012,
	title = {‘Open Science’ Challenges Journal Tradition With Web Collaboration},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2012/01/17/science/open-science-challenges-journal-tradition-with-web-collaboration.html?_r=0},
	abstract = {A {GLOBAL} {FORUM} Ijad Madisch, 31, a virologist and computer scientist, founded {ResearchGate}, a Berlin-based social networking platform for scientists that has more than 1.3 million members.},
	journaltitle = {The New York Times},
	author = {Lin, Thomas},
	urldate = {2012-01-18},
	date = {2012-01-16},
	langid = {english},
	keywords = {act\_Communicating, act\_Publishing, obj\_OnlineContent, obj\_Research},
}

@report{european_commission_commission_2012,
	location = {Brussels},
	title = {Commission Recommendation on access to and preservation of scientific information},
	url = {http://ec.europa.eu/research/science-society/document_library/pdf_06/recommendation-access-and-preservation-scientific-information_en.pdf},
	institution = {European Commission},
	author = {{European Commission}},
	date = {2012-07-17},
	langid = {english},
}

@online{thomson_is_2012,
	title = {is writing a book chapter a waste of time?},
	url = {http://patthomson.wordpress.com/2012/08/27/is-writing-a-book-chapter-a-waste-of-time/},
	abstract = {A couple of weeks ago a colleague suggested that I might want to offer some advice on whether it was better to write a book, a journal article or a book chapter. Coincidentally, just this week @dee...},
	titleaddon = {patter},
	author = {Thomson, Pat},
	urldate = {2012-08-29},
	date = {2012-08-27},
	langid = {english},
	keywords = {act\_Publishing, goal\_Dissemination, obj\_ResearchResults},
}

@collection{genet_les_2011,
	location = {Rome},
	title = {Les historiens et l'information : un métier àé rinventer},
	publisher = {École française de Rome},
	editor = {Genet, Jean-Philippe and Zorzi, Andrea},
	date = {2011},
}

@book{schreibman_companion_2008,
	location = {Oxford},
	edition = {Hardcover},
	title = {Companion to Digital Literary Studies},
	isbn = {9781405148641},
	url = {http://www.digitalhumanities.org/companionDLS/},
	series = {Blackwell Companions to Literature and Culture},
	publisher = {Blackwell Publishing Professional},
	author = {Schreibman, Susan and Siemens, Ray},
	date = {2008-12},
	langid = {english},
	keywords = {*****},
}
@article{tonkin_persistent_2008,
	title = {Persistent Identifiers: Considering the Options},
	issn = {1361-3200},
	url = {http://www.ariadne.ac.uk/issue56/tonkin/},
	abstract = {Emma Tonkin looks at the current landscape of persistent identifiers, describes several current services, and examines the theoretical background behind their structure and use.},
	number = {56},
	journaltitle = {Ariadne},
	author = {Tonkin, Emma},
	date = {2008-07},
	langid = {english},
	keywords = {act\_Identifying, meta\_GiveOverview},
}

@report{hakala_persistent_2010,
	title = {Persistent identifiers - an overview},
	url = {http://metadaten-twr.org/2010/10/13/persistent-identifiers-an-overview/},
	abstract = {This article describes five persistent identifier systems ({ARK}, {DOI}, {PURL}, {URN} and {XRI}) and compares their functionality against the cool {URIs}. The aim is to provide an overview, not to give any kind of ranking of these systems.},
	author = {Hakala, Juha},
	date = {2010},
	langid = {english},
	keywords = {act\_Identifying, meta\_GiveOverview},
}

@article{beynon_human_2006,
	title = {Human Computing—Modelling with Meaning},
	volume = {21},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/21/2/141.abstract},
	doi = {10.1093/llc/fql015},
	abstract = {This article is based on a session given by the authors at the {ACH}/{ALLC} conference at the University of Victoria in June 2005. It discusses the prospects for partnership between the humanities and computing from the alternative perspective afforded by Empirical Modelling ({EM}). Perceived dualities that separate the two cultures of science and art are identified as the primary impediment to this partnership. A vision for ‘human computing’ that promises to dissolve these dualities is outlined. The key characteristics and potential for {EM} for the humanities are illustrated with reference to a modelling exercise on the theme of Schubert's Erlkönig. This highlights how each of the six varieties of modelling identified by {McCarty} can be represented within an {EM} model. The implications of {EM} are discussed with reference to {McCarty}'s account of the key role for modelling in the humanities, in relation to James's ‘philosophic attitude’ of Radical Empiricism and to ideas from phenomenological sources.},
	pages = {141--157},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Beynon, Meurig and Russ, Steve and {McCarty}, Willard},
	urldate = {2011-11-01},
	date = {2006-04-12},
	langid = {english},
	keywords = {act\_Modeling, obj\_AnyObject},
}

@online{bailey_research_2012,
	title = {Research Data Curation Bibliography},
	url = {http://digital-scholarship.org/rdcb/rdcb.htm},
	abstract = {The Research Data Curation Bibliography includes selected English-language articles, books, and technical reports that are useful in understanding the curation of digital research data in academic and other research institutions. For broader coverage of the digital curation literature, see the author's Digital Curation Bibliography: Preservation and Stewardship of Scholarly Works,which presents over 650 English-language articles, books, and technical reports, and the Digital Curation Bibliography: Preservation and Stewardship of Scholarly Works, 2012 Supplement, which presents over 130 additional sources.},
	titleaddon = {Research Data Curation Bibliography},
	author = {Bailey, Charles W.},
	date = {2012-04-16},
	langid = {english},
	keywords = {act\_Organizing, goal\_Dissemination},
}

@inproceedings{lauritsen_toward_2009,
	location = {New York, {NY}, {USA}},
	title = {Toward a general theory of document modeling},
	isbn = {978-1-60558-597-0},
	url = {http://doi.acm.org/10.1145/1568234.1568257},
	doi = {10.1145/1568234.1568257},
	series = {{ICAIL} '09},
	abstract = {Most legal tasks involve document preparation and review. Drafting effective texts is central to lawyering, judging, legislating, and regulating. How best to support that work with intelligent tools is an ancient topic in {AI}-and-Law research. For those tools to work, they must have good quality knowledge content to work with. Many alternative theories and techniques for modeling documents have been developed for particular kinds of situations. This article sketches a basic general theory of legal document modeling, with a focus on the key role of argumentation.},
	pages = {202--211},
	booktitle = {Proceedings of the 12th International Conference on Artificial Intelligence and Law},
	publisher = {{ACM}},
	author = {Lauritsen, Marc and Gordon, Thomas F.},
	urldate = {2011-11-01},
	date = {2009},
	langid = {english},
	keywords = {act\_Modeling, obj\_AnyObject},
}
@article{kohle_fur_2013,
	title = {Für Open Access in den Geisteswissenschaften},
	url = {http://www.perlentaucher.de/essay/fuer-open-access-in-den-geisteswissenschaften.html},
	abstract = {Das Internet, in der Inkubationsphase ein militärisches, in der Frühphase ein wissenschaftliches Netzwerk, hat sich nach gut vierzig Jahren seiner Existenz zum global player im Handel mit Produkten aller Art entwickelt. Dass es damit seine früheren Funktionen nicht verloren hat, liegt auf der Hand. Die militärische Bedeutung steht im Zentrum von vielfältig angestellten Überlegungen zum zukünftigen Cyber-Krieg. Die wissenschaftliche aber bleibt in der öffentlichen Wahrnehmung unterbelichtet, obwohl die Wissenschaft verantwortlich ist für die meisten Entwicklungsschübe des Netzes. Vor allem in einer Hinsicht ist das Internet für die Wissenschaft ein unverzichtbares Medium, und zwar dort, wo es um die Veröffentlichung wissenschaftlicher Forschungen geht.},
	journaltitle = {Perlentaucher},
	author = {Kohle, Hubertus},
	date = {2013-09-16},
	langid = {german},
	keywords = {*****, goal\_Dissemination, meta\_Assessing, obj\_Research},
}

@collection{haber_historyblogosphere._2013,
	location = {Berlin},
	edition = {Open Access Edition},
	title = {Historyblogosphere. Bloggen in den Geschichtswissenschaften},
	url = {http://www.degruyter.com/view/product/227082},
	abstract = {Das Buch- und Schreibprojekt „historyblogosphere. Bloggen in den Geschichtswissenschaften“ dokumentiert und reflektiert die historische Blogosphäre: Wozu betreibt man als Historiker/in ein Blog? Was sind überhaupt geschichtswissenschaftliche Blogs? Gibt es geschlechtsspezifische Nutzungsformen? Wie schreibt man für ein Blog und wieviel Technikwissen braucht es dafür? Welche Möglichkeiten der Vernetzung gibt es und wozu braucht es diese? Haben Blogs einen wissenschaftlichen Nutzen und welche „Grenzen im Kopf“ innerhalb der wissenschaftlichen Community gilt es aufzubrechen? Können Blogs die transnationale bzw. interdisziplinäre Betrachtung historischer Prozesse fördern und aktuelle Fragen in den öffentlichen Diskurs tragen? Kurz: Wo ist sie, die Geschichte-Blogosphäre und wo ist die Community? Als ein Novum in der deutschsprachigen Wissenschaftslandschaft entsteht dieses Buch in einem offenen Arbeitsprozess, indem es Tools und Prozesse des Netzes einsetzt und eine kooperative Arbeitsweise in den Entstehungsprozess einbezieht. Insbesondere fand ein Open Peer Review im Netz statt. Der Open Peer Review Prozess lief von 10. Oktober bis 10. Dezember 2012 und ist dokumentiert unter http://historyblogosphere.oldenbourg-verlag.de/open-peer-review/},
	publisher = {De Gruyter},
	editor = {Haber, Peter and Pfanzelter, Eva and Schreiner, Julia},
	date = {2013},
	langid = {german},
	keywords = {goal\_Create, goal\_Dissemination, obj\_Research},
}

@article{kemman_just_2013,
	title = {Just Google It - Digital Research Practices of Humanities Scholars},
	url = {http://arxiv.org/abs/1309.2434},
	abstract = {The transition from analogue to digital archives and the recent explosion of online content offers researchers novel ways of engaging with data. The crucial question for ensuring a balance between the supply and demand-side of data, is whether this trend connects to existing scholarly practices and to the average search skills of researchers. To gain insight into this process we conducted a survey among nearly three hundred (N= 288) humanities scholars in the Netherlands and Belgium with the aim of finding answers to the following questions: 1) To what extent are digital databases and archives used? 2) What are the preferences in search functionalities 3) Are there differences in search strategies between novices and experts of information retrieval? Our results show that while scholars actively engage in research online they mainly search for text and images. General search systems such as Google and {JSTOR} are predominant, while large-scale collections such as Europeana are rarely consulted. Searching with keywords is the dominant search strategy and advanced search options are rarely used. When comparing novice and more experienced searchers, the first tend to have a more narrow selection of search engines, and mostly use keywords. Our overall findings indicate that Google is the key player among available search engines. This dominant use illustrates the paradoxical attitude of scholars toward Google: while provenance and context are deemed key academic requirements, the workings of the Google algorithm remain unclear. We conclude that Google introduces a black box into digital scholarly practices, indicating scholars will become increasingly dependent on such black boxed algorithms. This calls for a reconsideration of the academic principles of provenance and context.},
	journaltitle = {{arXiv}.org: Computer Science {\textgreater} Digital Libraries},
	author = {Kemman, Max and Kleppe, Martijn and Scagliola, Stef},
	date = {2013-09-10},
}

@report{hammond_preparing_2012,
	title = {Preparing for Data-driven Infrastructure},
	url = {http://observatory.jisc.ac.uk/docs/data-driven-infrastructure.pdf},
	abstract = {The final version of the {JISC} Observatory {TechWatch} report entitled Preparing for Data-driven Infrastructure was published on 17 September 2012.},
	author = {Hammond, Max},
	date = {2012-12-17},
	langid = {english},
}

@article{karnes_digital_2013,
	title = {Digital Mapping Reveals Social Networks of 18th-Century Travelers},
	url = {http://paloalto.patch.com/groups/schools/p/digital-mapping-reveals-social-networks-of-18th-centuba01d2398a},
	abstract = {Through a digital analysis of correspondence from travelers on the famed European "Grand Tour," Stanford classicist Giovanna Ceserani is discovering how international travel fostered cultural and academic trends in the 18th century.},
	journaltitle = {Palo Alto Patch},
	author = {Karnes, Beatrice},
	date = {2013-05-07},
	langid = {english},
	keywords = {bigdata{\textasciitilde}},
}

@inproceedings{mccarty_residue_2012,
	location = {Köln},
	title = {The residue of uniqueness},
	url = {http://www.cceh.uni-koeln.de/files/McCarty.pdf},
	abstract = {To build an argument for the supervening importance of agenda, I locate the digital
humanities within the context of a central human predicament: the anxiety of
identity stemming from the problematic relation of human to non-human, both
animal and machine. I identify modelling as the fundamental activity of the digital
humanities and draw a parallel between it and our developing confrontation with the
not-us. I then go on to argue that the demographics of infrastructure within the
digital humanities, therefore in part its emphasis, is historically due to the socially
inferior role assigned to those who in the early years found para-academic
employment in service to the humanities.  I do not specify an agenda, rather conclude that modelling, pursued within its humane context, offers a cornucopia of agenda if
only the “mind-forged manacles” of servitude’s mind-set can be broken.},
	eventtitle = {Cologne Dialogue on Digital Humanities},
	pages = {23},
	author = {{McCarty}, Willard},
	date = {2012-04-23},
	langid = {english},
	keywords = {act\_Modeling, obj\_AnyObject},
}

@article{clement_makings_2009,
	title = {The Makings of Digital Modernism: Rereading Gertrude Stein's The Making of Americans and Poetry by Elsa von Freytag-Loringhoven},
	url = {http://drum.lib.umd.edu/handle/1903/9160},
	shorttitle = {The Makings of Digital Modernism},
	abstract = {In this dissertation, I argue that digital methodologies offer new kinds of evidence and uncover new opportunities for changing how we do research and what we value as objects for literary study. In particular, I show how text mining, visualizations, digital editing, and social networks can be applied to make new readings of texts that have historically been undervalued within academic research. For example, I read Gertrude Stein's {\textless}italic{\textgreater}The Making of Americans{\textless}/italic{\textgreater} at a distance by analyzing large sets of data mined from the text and visualized within various applications. I also perform close readings of the poetry of Elsa von Freytag-Loringhoven differently by engaging online social networks in which textual performance, an ever-changing interpretive presentation of text, is enacted. By facilitating readings that allow submerged textual and social patterns to emerge, this research resituates digital methodologies and these modernist works within literary studies.},
	author = {Clement, Tanya E},
	urldate = {2012-04-25},
	date = {2009},
	langid = {english},
	keywords = {goal\_Analysis, goal\_Interpretation},
}

@incollection{puretskiy_survey_2010,
	title = {Survey of Text Visualization Techniques},
	rights = {Copyright © 2010 John Wiley \& Sons, Ltd},
	isbn = {9780470689646},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/9780470689646.ch6/summary},
	abstract = {This chapter contains sections titled: * Visualization in text analysis * Tag clouds * Authorship and change tracking * Data exploration and the search for novel patterns * Sentiment tracking * Visual analytics and {FutureLens} * Scenario discovery * Earlier prototype * Features of {FutureLens} * Scenario discovery example: bioterrorism * Scenario discovery example: drug trafficking * Future work * References},
	pages = {105--127},
	booktitle = {Text Mining},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Puretskiy, Andrey A. and Shutt, Gregory L. and Berry, Michael W.},
	editor = {Berry, Michael W. and Kogan, Jacob},
	urldate = {2013-05-09},
	date = {2010},
	langid = {english},
	keywords = {act\_Visualizing},
}
@inproceedings{cox_supporting_2000,
	location = {Philadelphia, Pennsylvania, United States},
	title = {Supporting collaborative interpretation in distributed Groupware},
	isbn = {1-58113-222-0},
	url = {http://portal.acm.org/citation.cfm?id=359000},
	doi = {10.1145/358916.359000},
	abstract = {Collaborative interpretationoccurs when a group interprets and transforms a diverse set of information fragments into a coherent set of meaningful descriptions. This activity is characterized byemergence, where the participants' shared understanding develops gradually as they interact with each other and the source material. Our goal is to support collaborative interpretation by small, distributed groups. To achieve this, we first observed how face-to-face groups perform collaborative interpretation in a particular work context. We then synthesized design principles from two relevant areas: the key behaviors of people engaged in activities where emergence occurs, and how distributed groups work together over visual surfaces. We built and evaluated a system that supports a specific collaborative interpretation task. This system provides a large workspace and several objects that encourages emergence in interpretation. People manipulatecardsthat contain the raw information fragments. They reduce complexity by placing duplicate cards intopiles. They suggest groupings as they manipulate the spatial layout of cards and piles. They enrich spatial layouts throughnotes, textandfreehand annotations. They record their understanding of their final groupings asreportscontaining coherent descriptions.},
	pages = {289--298},
	publisher = {{ACM}},
	author = {Cox, Donald and Greenberg, Saul},
	urldate = {2009-05-03},
	date = {2000},
	langid = {english},
	keywords = {goal\_Collaboration, goal\_Interpretation, obj\_Infrastructures, obj\_People},
}

@article{noack_modularity_2009,
	title = {Modularity clustering is force-directed layout},
	volume = {79},
	issn = {1539-3755, 1550-2376},
	url = {http://link.aps.org/doi/10.1103/PhysRevE.79.026102},
	doi = {10.1103/PhysRevE.79.026102},
	abstract = {Two natural and widely used representations for the community structure of networks are clusterings, which partition the vertex set into disjoint subsets, and layouts, which assign the vertices to positions in a metric space. This paper unifies prominent characterizations of layout quality and clustering quality, by showing that energy models of pairwise attraction and repulsion subsume Newman and Girvan's modularity measure. Layouts with optimal energy are relaxations of, and are thus consistent with, clusterings with optimal modularity, which is of practical relevance because both representations are complementary and often used together.},
	number = {2},
	journaltitle = {Physical Review E},
	author = {Noack, Andreas},
	urldate = {2013-04-05},
	date = {2009-02},
	langid = {english},
	keywords = {act\_Visualizing},
}

@article{fish_mind_2012,
	title = {Mind Your P's and B's: The Digital Humanities and Interpretation},
	url = {http://opinionator.blogs.nytimes.com/2012/01/23/mind-your-ps-and-bs-the-digital-humanities-and-interpretation/},
	shorttitle = {Mind Your P's and B's},
	abstract = {Digital humanists have new tools at their disposal, but does that necessarily make for a more accurate reading of texts?},
	journaltitle = {Opinionator, New York Times},
	author = {Fish, Stanley},
	urldate = {2012-01-24},
	date = {2012-01-23},
	langid = {english},
	keywords = {goal\_Interpretation},
}

@article{stewart_charles_2003,
	title = {Charles Brockden Brown: Quantitative Analysis and Literary Interpretation},
	volume = {18},
	url = {http://llc.oxfordjournals.org/content/18/2/129.abstract},
	doi = {10.1093/llc/18.2.129},
	shorttitle = {Charles Brockden Brown},
	abstract = {This study is a test case in the use of stylometric techniques to provide an entrance into questions of literary criticism and interpretation. The study applies multivariate analysis to two texts of Charles Brockden Brown, sometimes considered the first professional writer in the United States. Both a scatter graph of a principal components analysis and a cluster analysis show that individual chapters from each of two novels (Wieland and Carwin) group together, except for three chapters of Wieland that cluster with the Carwin chapters. One chapter of Wieland that clusters with the Carwin chapters is narrated by the same character who narrates all of Carwin, thus providing statistical evidence that Brown has created a narrator with a distinctive voice. Accounting for the clustering of the other two chapters calls for a consideration of several of the more crucial and problematic interpretative issues in the novel, and suggests that quantitative analysis can indeed provide background and evidence for literary critical discussion and understanding.},
	pages = {129 --138},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Stewart, Larry L.},
	urldate = {2011-06-03},
	date = {2003-06},
	langid = {english},
	keywords = {{AnalyzeStatistically}, goal\_Interpretation},
}

@article{oren_what_2006,
	title = {What are semantic annotations},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.97.7985&rep=rep1&type=pdf},
	abstract = {Annotations of Web resources can be created using traditional document annotation tools or more recent approaches such as semantic wikis, semantic blogs and collaborative tagging. Currently no unified model exists for all these different kinds of annotations, making it difficult both to compare and assess annotation tools and to integrate the various kinds of annotation data. We analyse annotations in various domains and present a unified formal model for semantic annotations. We evaluate existing annotation tools from these different domains and show how to map the data these tools produce onto our formal model, thus allowing to access and represent this data in a unified way.},
	pages = {14},
	journaltitle = {Relatório técnico. {DERI} Galway},
	author = {Oren, Eyal and Möller, Knud and Scerri, Simon and Handschuh, Siegfried and Sintek, Michael},
	date = {2006},
	langid = {english},
	keywords = {act\_Annotating},
}

@incollection{declerck_towards_2012,
	title = {Towards Linked Language Data for Digital Humanities},
	rights = {©2012 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-28248-5, 978-3-642-28249-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-28249-2_11},
	abstract = {We investigate the extension of classification schemes in the Humanities into semantic data repositories, the benefits of which could be the automation of so far manually conducted processes, such as detecting motifs in folktale texts. In parallel, we propose linguistic analysis of the textual labels used in these repositories. The resulting resource, which we propose to publish in the Linked Open Data ({LOD}) framework, will explicitly interlink domain knowledge and linguistically enriched language data, which can be used for knowledge-driven content analysis of literary works.},
	pages = {109--116},
	booktitle = {Linked Data in Linguistics},
	publisher = {Springer Berlin Heidelberg},
	author = {Declerck, Thierry and Lendvai, Piroska and Mörth, Karlheinz and Budin, Gerhard and Váradi, Tamás},
	editor = {Chiarcos, Christian and Nordhoff, Sebastian and Hellmann, Sebastian},
	urldate = {2013-05-09},
	date = {2012-01-01},
	langid = {english},
	keywords = {act\_Annotating},
}

@report{gabler_thesen_2010,
	title = {Thesen zur wissenschaftlichen Edition im digitalen Medium},
	url = {http://www.academia.edu/216337/Thesen_zur_wissenschaftlichen_Edition_im_digitalen_Medium},
	author = {Gabler, Hans Walter},
	date = {2010},
	langid = {german},
	keywords = {act\_Publishing, t\_Encoding},
}

@article{gabler_theorizing_2010,
	title = {Theorizing the Digital Scholarly Edition},
	volume = {7},
	issn = {17414113},
	url = {http://doi.wiley.com/10.1111/j.1741-4113.2009.00675.x},
	doi = {10.1111/j.1741-4113.2009.00675.x},
	abstract = {The scholarly edition has traditionally been conceived of as hierarchically ordered downwards from a text, buffered and augmented by apparatuses as subordinate editorial paratexts. Of old, the paratexts used to stand in a hermeneutic relationship – broadly, a commentary relationship – to the edition text. Increasingly, however, the hermeneutic dimension of the scholarly edition gave way to modes of positivist accumulation of materials, in support not so much of the interpretive reading but of the editorial establishing of the edited texts. Today, as the carrier medium for editions changes from book to the digital medium, all the main a priori assumptions about scholarly editions come into question. Editions may be reconceived as answering to the paradigm of a relational interplay of discourses, dynamically correlated both among themselves and with an edition’s readers and users: that is, to a paradigm once again of text and ongoing commentary. Relational structures will become realizable because the digital medium will be the native medium of the scholarly edition of the future. It will be the medium to study and use editions; while the print medium will remain the medium to read texts. No longer issuing in scholarly editions as books, scholarly editing of the future will be aiming instead at constructing the material foundations for research platforms as digitally explorable knowledge sites dedicated to multi-faceted historical, philosophical, cultural and literary research and criticism. The digital medium has the potential to develop into an environment suitable to re-integrate textual criticism into criticism – and, just as importantly: to ground criticism again in textual criticism.},
	pages = {43--56},
	number = {2},
	journaltitle = {Literature Compass},
	author = {Gabler, Hans Walter},
	urldate = {2011-06-28},
	date = {2010-02},
	langid = {english},
	keywords = {meta\_Theorizing, t\_Encoding},
}

@article{schmidt_inadequacy_2010,
	title = {The inadequacy of embedded markup for cultural heritage texts},
	volume = {25},
	url = {http://llc.oxfordjournals.org/content/25/3/337.abstract},
	doi = {10.1093/llc/fqq007},
	abstract = {Embedded generalized markup, as applied by digital humanists to the recording and studying of our textual cultural heritage, suffers from a number of serious technical drawbacks. As a result of its evolution from early printer control languages, generalized markup can only express a document’s ‘logical’ structure via a repertoire of permissible printed format structures. In addition to the well-researched overlap problem, the embedding of markup codes into texts that never had them when written leads to a number of further difficulties: the inclusion of potentially obsolescent technical and subjective information into texts that are supposed to be archivable for the long term, the manual encoding of information that could be better computed automatically, and the obscuring of the text by highly complex technical data. Many of these problems can be alleviated by asserting a separation between the versions of which many cultural heritage texts are composed, and their content. In this way the complex interconnections between versions can be handled automatically, leaving only simple markup for individual versions to be handled by the user.},
	pages = {337 --356},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	author = {Schmidt, Desmond},
	urldate = {2011-04-26},
	date = {2010},
	langid = {english},
	keywords = {meta\_Theorizing, t\_Encoding},
}

@article{smith_textual_1999,
	title = {Textual Variation and Version Control in the {TEI}},
	volume = {33},
	url = {http://link.springer.com/article/10.1023%2FA%3A1001795210724},
	doi = {10.1023/A:1001795210724},
	abstract = {Projects that attempt to encode variorum texts with the Text Encoding Initiative's Guidelines for Electronic Text Encoding and Interchange will likely encounter situations where the text varies in its structure, as well as in its content. Although encoding textual variants at a separate level using a version control system may be attractive, the advantages in encoding text and variants in the same format are considerable. This paper proposes solutions to three problems that require more than the standard {TEI} textual critical elements: transposition, variation of meta-data, and insertion of incomplete structures.},
	pages = {103--112},
	number = {1},
	journaltitle = {Computers and the Humanities},
	author = {Smith, David},
	date = {1999},
	langid = {english},
	keywords = {act\_RelationalAnalysis, obj\_Variants, t\_Encoding},
}

@incollection{buzetti_textual_1998,
	location = {Sofia},
	title = {Textual Fluidity and Digital Editions},
	url = {http://www.denkstaette.de/files/Buzzetti-Rehbein.pdf},
	pages = {14--39},
	booktitle = {Text Variety in the Witnesses of Medieval Texts},
	publisher = {Institute of Mathematics and Informatics},
	author = {Buzetti, Dino and Rehbein, Malte},
	editor = {Dobreva, Milena},
	date = {1998},
	langid = {english},
	keywords = {act\_Publishing, goal\_Enrichment, obj\_Variants, t\_Encoding},
}

@article{faulhaber_textual_1991,
	title = {Textual criticism in the 21st century},
	volume = {45},
	url = {http://cat.inist.fr/?aModele=afficheN&cpsidt=4289137},
	abstract = {Un changement décisif entre la pratique actuelle de la critique textuelle et celle du 21e siècle sera le développement de l'édition informatisée. L'A. tente de définir l'évolution que connaîtra l'hyperédition et ses implications théoriques et pratiques},
	pages = {123--148},
	number = {1},
	journaltitle = {Romance Philology},
	author = {Faulhaber, Charles},
	urldate = {2011-04-12},
	date = {1991},
	langid = {english},
	keywords = {goal\_Enrichment, meta\_Theorizing, t\_Encoding},
}

@article{aschenbrenner_textgrid_2006,
	title = {{TextGrid} - a modular platform for collaborative textual editing},
	url = {http://www.textgrid.de/fileadmin/publikationen/aschenbrenner-2006.pdf},
	abstract = {{TextGrid} equips textual criticism with a fundamental infrastructure
and a powerful set of software tools based on the evaluation of existing
solutions and embracing the grid paradigm. Its technology and services
effectively support text scholars in their daily working processes. With its
distinctive features and being a native grid project for the humanities, {TextGrid}
roots in domain sciences including textual scholarship, the Digital Library as
well as the e-Science communities, and it aims to span the continuum between
them. After its launch in early 2006, {TextGrid} has three years to achieve its
goals and make e-Humanities a reality. During all this time the project will be
open and invite participation, to attain a generic software platform that can be
used and re-used in multiple contexts.},
	pages = {27--36},
	journaltitle = {Proceedings of the International Workshop on Digital Library Goes e-Science ({DLSci}06), September 21, 2006, Alicante, Spain},
	author = {Aschenbrenner, Andreas and Gietz, Peter and Küster, Marc Wilhelm and Neuroth, Christoph Ludwig \{and\} Heike},
	date = {2006},
	langid = {english},
	keywords = {goal\_Enrichment, meta\_Collaborating, obj\_VREs, t\_Encoding},
}

@article{sperberg-mcqueen_text_1991,
	title = {Text in the Electronic Age: Textual Study and Text Encoding, with Examples from Medieval Texts},
	volume = {6},
	url = {http://llc.oxfordjournals.org/content/6/1/34.abstract},
	doi = {10.1093/llc/6.1.34},
	shorttitle = {Text in the Electronic Age},
	abstract = {This paper discusses characteristic problems in designing methods of encoding texts in machine-readable form for textual study. Any electronic representation of a text embodies specific ideas of what is important in that text. A well-developed encoding scheme is thus in some sense a theory of the texts it is intended scheme is thus in some sense a theory of the texts it is intended to mark up. The paper describes, with examples, the theory implicit in the work of the Text Encoding Initiative ({TEI}), a project to develop guidelines for the encoding of machine-readable texts.Any machine-readable representation of texts must use markup, but no finite vocabulary of markup items can be complete, since neither the set of textual features worth marking nor the set of texts to be studied is finite. Any useful markup scheme must therefore be extensible.Additionally, a markup scheme must allow several discrete views of texts. Texts are both linguistic and physical objects. They have simultaneously a linear, a hierarchical, and a directed-graph structure. They refer to objects in real or fictive universes Texts, finally, are cultural and thus historical objects a useful encoding scheme must be able to represent textual variation, parallel texts, and the gradual accretion of interpretation and commentary with which human culture adorns venerated texts.},
	pages = {34 --46},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	author = {Sperberg-{McQueen}, C. M.},
	urldate = {2011-05-23},
	date = {1991-01-01},
	langid = {english},
	keywords = {meta\_Theorizing, t\_Encoding},
}

@article{zillig_tei_2011,
	title = {{TEI} Texts that Play Nicely: Lessons from the {MONK} Project},
	volume = {1},
	url = {https://letterpress.uchicago.edu/index.php/jdhcs/article/view/81},
	shorttitle = {{TEI} Texts that Play Nicely},
	abstract = {Text curation, like most human endeavors, requires tools. A technique developed for the {MONK} Project, schema harvesting, provides a useful platform for facilitating the digital conversion and curation of text corpora. The author describes Abbot, an {XSLT}-based application that has had success in converting various Text Creation Partnership collections, and others, during and after {MONK}.},
	number = {3},
	journaltitle = {Journal of the Chicago Colloquium on Digital Humanities and Computer Science},
	shortjournal = {{JDHCS}},
	author = {Zillig, Brian L. Pytlik},
	date = {2011-07-14},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Encoding, t\_Interoperability},
}

@article{pytlik_zillig_tei_2009,
	title = {{TEI} Analytics: converting documents into a {TEI} format for cross-collection text analysis},
	volume = {24},
	url = {http://llc.oxfordjournals.org/content/24/2/187.abstract},
	doi = {10.1093/llc/fqp005},
	shorttitle = {{TEI} Analytics},
	abstract = {For the purposes of large-scale analysis of {XML}/{SGML} files, converting humanities texts into a common form of markup represents a technical challenge. The {MONK} (Metadata Offer New Knowledge) Project has developed both a common format, {TEI} Analytics (a {TEI} subset designed to facilitate interoperability of text archives) and a command-line tool, Abbot, that performs the conversion. Abbot relies upon a new technique, schema harvesting, developed by the author to convert text documents into {TEI}-A. This article has two aims: first, to describe the {TEI}-A format itself and, second, to outline the methods used to convert files. More generally, it is hoped that the techniques described will lead to greater interoperability of text documents for text analysis in a wider context.},
	pages = {187 --192},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	author = {Pytlik Zillig, Brian L.},
	urldate = {2011-06-03},
	date = {2009-06-01},
	langid = {english},
	keywords = {act\_Conversion, obj\_Code, t\_Encoding, t\_Interoperability},
}

@inproceedings{gorz_semantic_2010,
	location = {Shanghai},
	title = {Semantic Annotation for Medieval Cartography: The Example of the Behaim Globe of 1492},
	url = {http://wiss-ki.eu/node/105},
	shorttitle = {Semantic Annotation for Medieval Cartography},
	eventtitle = {22nd General Conference of the International Council of Museums},
	booktitle = {22nd General Conference of the International Council of Museums},
	author = {Görz, Günther},
	date = {2010},
	langid = {english},
}

@article{cohen_scholars_2010,
	title = {Scholars Recruit Public for Project - Humanities 2.0},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2010/12/28/books/28transcribe.html?ref=humanities20},
	abstract = {For Bentham and Others, Scholars Enlist Public to Transcribe Papers},
	journaltitle = {The New York Times},
	author = {Cohen, Patricia},
	urldate = {2011-05-20},
	date = {2010-12-27},
	langid = {english},
	keywords = {goal\_Collaboration, meta\_GiveOverview, obj\_Letters, t\_Encoding},
}

@inproceedings{mikheev_named_1999,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Named Entity recognition without gazetteers},
	url = {http://dx.doi.org/10.3115/977035.977037},
	doi = {10.3115/977035.977037},
	series = {{EACL} '99},
	abstract = {It is often claimed that Named Entity recognition systems need extensive gazetteers---lists of names of people, organisations, locations, and other named entities. Indeed, the compilation of such gazetteers is sometimes mentioned as a bottleneck in the design of Named Entity recognition systems.We report on a Named Entity recognition system which combines rule-based grammars with statistical (maximum entropy) models. We report on the system's performance with gazetteers of different types and different sizes, using test material from the {MUC}-7 competition. We show that, for the text type and task of this competition, it is sufficient to use relatively small gazetteers of well-known names, rather than large gazetteers of low-frequency names. We conclude with observations about the domain independence of the competition and of our experiments.},
	pages = {1--8},
	booktitle = {Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Mikheev, Andrei and Moens, Marc and Grover, Claire},
	urldate = {2013-05-06},
	date = {1999},
	langid = {english},
	keywords = {t\_NamedEntityRecognition},
}

@article{bruning_multiple_2013,
	title = {Multiple Encoding in Genetic Editions: The Case of "Faust"},
	rights = {{TEI} Consortium 2013 (Creative Commons Attribution-{NoDerivs} 3.0 Unported License)},
	issn = {2162-5603},
	url = {http://jtei.revues.org/697},
	shorttitle = {Multiple Encoding in Genetic Editions},
	abstract = {The aim of the present paper is to show how, and to what extent, the standards of critical genetic editions as applied to Goethe's Faust can be attained within a {TEI} framework. It proposes and argues for the introduction of two separate transcripts: documentary and textual. Despite the apparent disadvantages of multiple encoding, this approach recommends itself for practical reasons (e.g., avoidance of overlapping hierarchies), and it conveniently reflects the idea that any written document must be considered a material object on the one hand and a medium of textual transmission on the other. In the course of the paper, some aspects and problems of chapter 11 of version 2.0.0 of {TEI} P5 (the definition and use of the elements {\textless}line{\textgreater} and {\textless}mod{\textgreater} and related issues) will be discussed.},
	issue = {Issue 4},
	journaltitle = {Journal of the Text Encoding Initiative},
	author = {Brüning, Gerrit and Henzel, Katrin and Pravida, Dietmar},
	editora = {Jannidis, Fotis and Rehbein, Malte and Romary, Laurent},
	editoratype = {collaborator},
	urldate = {2013-04-01},
	date = {2013-03-08},
	langid = {english},
	keywords = {obj\_Manuscripts, t\_Encoding},
}

@article{gabler_preeminence_2008,
	title = {La prééminence du document dans l’édition},
	rights = {© Recherches \& Travaux},
	issn = {0151-1874},
	url = {http://recherchestravaux.revues.org/index85.html},
	abstract = {À la question posée jadis par Gunter Martens, spécialiste allemand de critique textuelle1 : « D’un point de vue éditorial, qu’est-ce qu’un texte2 ?», je répondrai aujourd’hui par la question suivante : « D’un point de vue éditorial, qu’est-ce qu’un document ? » Tant que la transmission et l’édition se trouvaient liées dans le même univers matériel d’encre et de papier, le problème ne se posait pas : le document était « transparent » parce qu’il était aussi matériel que tout ce qui l’entourait. M [...]},
	pages = {39--51},
	number = {72},
	journaltitle = {Recherches \& Travaux},
	author = {Gabler, Hans-Walter},
	editor = {Leriche, Françoise and Meynard, Cécile},
	editora = {Coste, Claude and Vibert, Bertrand},
	editoratype = {collaborator},
	urldate = {2010-05-24},
	date = {2008-06-15},
	langid = {french},
	keywords = {act\_Visualizing, meta\_Theorizing, obj\_Documents, obj\_Manuscripts, t\_Encoding},
}

@inproceedings{tjong_kim_sang_introduction_2003,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Introduction to the {CoNLL}-2003 shared task: language-independent named entity recognition},
	url = {http://dx.doi.org/10.3115/1119176.1119195},
	doi = {10.3115/1119176.1119195},
	series = {{CONLL} '03},
	shorttitle = {Introduction to the {CoNLL}-2003 shared task},
	abstract = {We describe the {CoNLL}-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.},
	pages = {142--147},
	booktitle = {Proceedings of the seventh conference on Natural language learning at {HLT}-{NAACL} 2003 - Volume 4},
	publisher = {Association for Computational Linguistics},
	author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
	urldate = {2013-05-06},
	date = {2003},
	langid = {english},
	keywords = {t\_NamedEntityRecognition},
}

@article{ramel_interactive_2013,
	title = {Interactive layout analysis, content extraction, and transcription of historical printed books using Pattern Redundancy Analysis},
	volume = {28},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/28/2/301},
	doi = {10.1093/llc/fqs077},
	abstract = {This article describes the work performed in the Pattern Redundancy Analysis for Document Image Indexing and Transcription research project. The project focused on layout analysis, text/graphics separation, optical character recognition ({OCR}), and text transcription processes dedicated to old and precious books. The originality of this work relies on the analysis and exploitation of pattern redundancy in documents to enable the efficient indexing and quick transcription of books and the identification of typographic materials. For these purposes, we have developed two software packages. The first, {AGORA}, performs page layout analysis, text/graphics separation, and pattern (letterform) extraction simultaneously. These patterns are then processed to group similar patterns together in single clusters so that different letterforms of a book can be extracted and analysed to compute redundancy rates. This process allows a significant reduction of the number of letterforms to be recognized. Once the clustering of letterforms is done, a user may assign a label to each cluster using the second software, {RETRO}. Labels are then automatically assigned to each corresponding character to perform the text transcription of the whole book. Thus, if 90\% of the letterforms are detected as redundant, only one character out of ten must be labelled by the user to transcribe the book. Moreover, this transcription method allows us to deal easily with the special characters that appear frequently in old books. It is also possible to use our clustering approach to extract and create new font packages from specific printing material (e.g. from rare books printed with particular types or woodblocks). These new font packages could be incorporated into the training step of optical fonts recognition methods to improve the recognition results of {OCRs} on rare or specific books. The identification of typographic materials could also be useful for the study of both the aesthetic (such as how the thickness and shape of printing types evolved from the 15th to the mid-16th century) and economic aspects of printing historically. Until the second half of the 16th century, for instance, printing types circulated among workshops, and printers frequently sold or lent types to their fellows.},
	pages = {301--314},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Ramel, Jean-Yves and Sidère, Nicolas and Rayar, Frédéric},
	urldate = {2013-06-12},
	date = {2013-06-01},
	langid = {english},
	keywords = {act\_Annotating, act\_StructuralAnalysis, obj\_Documents, t\_Encoding},
}

@article{mcgann_text_2006,
	title = {From Text to Work: Digital Tools and the Emergence of the Social Text},
	issn = {1467-1255},
	url = {http://id.erudit.org/iderudit/013153ar},
	shorttitle = {From Text to Work},
	abstract = {The essay is a study of how critical editions work, whether in paper-based forms or in electronic forms. The first section – more than half the essay – gives a close examination to J. C. C. Mays’s superb recent (Bollingen) edition of Coleridge’s poetry. This analysis establishes the terms for investigating the opportunities that digital technology supplies for scholars pursuing a close study of the socio-historical character of literary works. This investigation pivots around the seminal work of D. F. {McKenzie}, whose theory of the social-text edition argues for a more comprehensive kind of editorial method. This essay argues that the method can be best realized through digital resources. It concludes with a discussion of The Rossetti Archive as a “proof of concept” experiment to test the social-text approach to editorial method.},
	number = {41},
	journaltitle = {Romanticism on the Net},
	author = {{McGann}, Jerome},
	urldate = {2010-05-21},
	date = {2006},
	langid = {english},
	keywords = {act\_Publishing, act\_Visualizing, goal\_Enrichment, meta\_Theorizing, t\_Encoding},
}

@book{shillingsburg_gutenberg_2006,
	location = {Cambridge},
	title = {From Gutenberg to Google. Electronic Representations of Literary Texts},
	isbn = {9780521864985},
	url = {http://www.cambridge.org/us/academic/subjects/literature/printing-and-publishing-history/gutenberg-google-electronic-representations-literary-texts},
	abstract = {As technologies for electronic texts develop into ever more sophisticated engines for capturing different kinds of information, radical changes are underway in the way we write, transmit and read texts. In this thought-provoking work, Peter Shillingsburg considers the potentials and pitfalls, the enhancements and distortions, the achievements and inadequacies of electronic editions of literary texts. In tracing historical changes in the processes of composition, revision, production, distribution and reception, Shillingsburg reveals what is involved in the task of transferring texts from print to electronic media. He explores the potentials, some yet untapped, for electronic representations of printed works in ways that will make the electronic representation both more accurate and more rich than was ever possible with printed forms. However, he also keeps in mind the possible loss of the book as a material object and the negative consequences of technology.},
	publisher = {Cambridge Univ. Press},
	author = {Shillingsburg, Peter},
	date = {2006},
	langid = {english},
	keywords = {act\_Annotating, act\_Editing, act\_Publishing, goal\_Enrichment, meta\_Theorizing, obj\_Literature, obj\_Text, t\_Encoding},
}
@inproceedings{kazama_exploiting_2007,
	title = {Exploiting Wikipedia as External Knowledge for Named Entity Recognition},
	url = {http://www.aclweb.org/anthology-new/D/D07/D07-1073.pdf},
	abstract = {We explore the use of Wikipedia as external knowledge to improve named entity recognition ({NER}). Our method retrieves the corresponding Wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry, which can be thought of as a definition part. These category labels are used as features in a {CRF}-based {NE} tagger. We demonstrate using the {CoNLL} 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of {NER}.},
	eventtitle = {Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
	pages = {698--707},
	author = {Kazama, Jun'ichi and Torisawa, Kentaro},
	urldate = {2013-05-06},
	date = {2007},
	langid = {english},
	keywords = {t\_NamedEntityRecognition},
}

@collection{burnard_electronic_2006,
	location = {New York},
	title = {Electronic Textual Editing},
	isbn = {9780873529709},
	url = {http://www.tei-c.org/About/Archive_new/ETE/Preview/},
	publisher = {{MLA}},
	editor = {Burnard, Lou and O’Brien O’Keeffe, Katherine and Unsworth, John},
	date = {2006},
	langid = {english},
	keywords = {act\_Publishing, meta\_GiveOverview, t\_Encoding},
}

@collection{gasteiner_digitale_2010,
	location = {Wien},
	title = {Digitale Arbeitstechniken für die Geistes- und Kulturwissenschaften},
	isbn = {9783825231576},
	url = {http://www.utb-shop.de/digitale-arbeitstechniken.html},
	abstract = {Möglichkeiten und Anwendungsgebiete digitaler Arbeitstechniken Das Buch vermittelt Kenntnisse und Kompetenzen für fortgeschrittene Studierende, Dozierende sowie Forschende in den Geisteswissenschaften im Umgang mit Neuen Medien, Open Access und Digitalen Publikationspraktiken. Es orientiert über Rechte und Pflichten im Umgang mit digitalen Texten und Bildern, behandelt die unterschiedlichen Situationen in den deutschsprachigen Ländern und weist weiterführende Literatur und Quellen zu diesen Themen nach.},
	pagetotal = {269},
	publisher = {{UTB}},
	editor = {Gasteiner, Martin and Haber, Peter},
	date = {2010},
	langid = {german},
	keywords = {X-{CHECK}, act\_Publishing, meta\_Assessing, meta\_GiveOverview, obj\_Methods, t\_Encoding},
}

@article{fiormonte_digital_2010,
	title = {Digital Encoding as a Hermeneutic and Semiotic Act: The Case of Valerio Magrelli},
	volume = {4},
	url = {http://www.digitalhumanities.org/dhq/vol/4/1/000082/000082.html},
	shorttitle = {Digital Encoding as a Hermeneutic and Semiotic Act},
	number = {1},
	author = {Fiormonte, Domenico and Martiradonna, Valentina and Schmidt, Desmond},
	urldate = {2010-08-03},
	date = {2010},
	langid = {english},
	keywords = {meta\_Theorizing, t\_Encoding},
}

@inproceedings{ratinov_design_2009,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Design challenges and misconceptions in named entity recognition},
	isbn = {978-1-932432-29-9},
	url = {http://dl.acm.org/citation.cfm?id=1596374.1596399},
	series = {{CoNLL} '09},
	abstract = {We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust {NER} system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local {NER} decisions, the sources of prior knowledge and how to use them within an {NER} system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an {NER} system that achieves 90.8 F1 score on the {CoNLL}-2003 {NER} shared task, the best reported result for this dataset.},
	pages = {147--155},
	booktitle = {Proceedings of the Thirteenth Conference on Computational Natural Language Learning},
	publisher = {Association for Computational Linguistics},
	author = {Ratinov, Lev and Roth, Dan},
	urldate = {2013-05-06},
	date = {2009},
	langid = {english},
	keywords = {*****, t\_NamedEntityRecognition},
}

@book{simsion_data_2004,
	location = {Amsterdam},
	edition = {3. ed.},
	title = {Data modeling essentials},
	isbn = {9780126445510, 0126445516},
	url = {http://shop.oreilly.com/product/9780126445510.do},
	abstract = {Data Modeling Essentials, Third Edition provides expert tutelage for data modelers, business analysts and systems designers at all levels. Beginning with the basics, this book provides a thorough grounding in theory before guiding the reader through the various stages of applied data modeling and database design. Later chapters address advanced subjects, including business rules, data warehousing, enterprise-wide modeling and data management.

The third edition of this popular book retains its distinctive hallmarks of readability and usefulness, but has been given significantly expanded coverage and reorganized for greater reader comprehension. Authored by two leaders in the field, Data Modeling Essentials, Third Edition is the ideal reference for professionals and students looking for a real-world perspective.},
	pagetotal = {560},
	publisher = {Morgan Kaufmann},
	author = {Simsion, Graeme},
	date = {2004},
	langid = {english},
	keywords = {act\_Modeling, goal\_Analysis, goal\_Interpretation, meta\_GiveOverview},
}

@incollection{jannidis_computerphilologie_2007,
	location = {Stuttgart},
	title = {Computerphilologie},
	volume = {2 (Methoden und Theorien)},
	pages = {27--40},
	booktitle = {Handbuch Literaturwissenschaft},
	publisher = {Metzler},
	author = {Jannidis, Fotis},
	editor = {Anz, Thomas},
	date = {2007},
	langid = {german},
	keywords = {*****, {AnalyzeStatistically}, X-{CHECK}, act\_Publishing, goal\_Enrichment, meta\_GiveOverview, obj\_DigitalHumanities, obj\_Tools, t\_Encoding, x\_astree},
}

@thesis{czmiel_adaquate_2003,
	title = {Adäquate Markupsysteme für die digitale Behandlung altägyptischer Texte},
	url = {http://old.hki.uni-koeln.de/studium/MA/MA_czmiel.pdf},
	pagetotal = {94},
	institution = {Köln},
	type = {Thesis ({MA} level)},
	author = {Czmiel, Alexander},
	date = {2003-10-06},
	langid = {german},
	keywords = {goal\_Enrichment, t\_Encoding, t\_XML},
}

@article{nadeau_survey_2007,
	title = {A survey of named entity recognition and classification},
	volume = {30},
	url = {http://www.ingentaconnect.com/content/jbp/li/2007/00000030/00000001/art00002?token=005219458c2514faa7e2a46762c6b635d3b662a2553492b467c673f7b2f267738703375686f497c05b},
	doi = {10.1075/li.30.1.03nad},
	abstract = {This survey covers fifteen years of research in the Named Entity Recognition and Classification ({NERC}) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, {NERC} systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of {NERC} such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.},
	pages = {3--26},
	number = {1},
	journaltitle = {Lingvisticae Investigationes},
	shortjournal = {Lingvisticae Investigationes},
	author = {Nadeau, David and Sekine, Satoshi},
	date = {2007},
	langid = {english},
	keywords = {*****, t\_NamedEntityRecognition},
}

@book{tidwell_xslt_2008,
	location = {Sebstopol, {CA}},
	title = {{XSLT}},
	isbn = {9780596527211  0596527217},
	url = {http://it-ebooks.info/book/119/},
	abstract = {The second edition of {XSLT} incorporates new material for {XSLT} 2.0 and
expounds on the lessons learned over the last six years of {XSLT} 1.0 use. Whether you're looking for the latest and greatest in {XSLT} 1.0 techniques, or moving on to {XSLT} 2.0, this new edition of {XSLT} will address your needs. This book includes plenty of practical,
real-world examples to show you how to apply {XSLT} stylesheets to {XML} data using either version.},
	pagetotal = {990},
	publisher = {O'Reilly Media},
	author = {Tidwell, Doug},
	date = {2008},
	langid = {english},
	keywords = {act\_Modeling, t\_XML},
}

@article{piotrowski_natural_2012,
	title = {Natural Language Processing for Historical Texts},
	volume = {5},
	issn = {1947-4040, 1947-4059},
	url = {http://www.morganclaypool.com/doi/abs/10.2200/S00436ED1V01Y201207HLT017},
	doi = {10.2200/S00436ED1V01Y201207HLT017},
	abstract = {More and more historical texts are becoming available in digital form. Digitization of paper documents is motivated by the aim of preserving cultural heritage and making it more accessible, both to laypeople and scholars. As digital images cannot be searched for text, digitization projects increasingly strive to create digital text, which can be searched and otherwise automatically processed, in addition to facsimiles. Indeed, the emerging field of digital humanities heavily relies on the availability of digital text for its studies.

Together with the increasing availability of historical texts in digital form, there is a growing interest in applying natural language processing ({NLP}) methods and tools to historical texts. However, the specific linguistic properties of historical texts -- the lack of standardized orthography, in particular -- pose special challenges for {NLP}.

This book aims to give an introduction to {NLP} for historical texts and an overview of the state of the art in this field. The book starts with an overview of methods for the acquisition of historical texts (scanning and {OCR}), discusses text encoding and annotation schemes, and presents examples of corpora of historical texts in a variety of languages. The book then discusses specific methods, such as creating part-of-speech taggers for historical languages or handling spelling variation. A final chapter analyzes the relationship between {NLP} and the digital humanities.

Certain recently emerging textual genres, such as {SMS}, social media, and chat messages, or newsgroup and forum postings share a number of properties with historical texts, for example, nonstandard orthography and grammar, and profuse use of abbreviations. The methods and techniques required for the effective processing of historical texts are thus also of interest for research in other domains.},
	pages = {1--157},
	number = {2},
	journaltitle = {Synthesis Lectures on Human Language Technologies},
	author = {Piotrowski, Michael},
	urldate = {2013-01-19},
	date = {2012-09-24},
	langid = {english},
	keywords = {goal\_Capture, goal\_Enrichment, meta\_GiveOverview},
}

@article{haaf_measuring_2013,
	title = {Measuring the Correctness of Double-Keying: Error Classification and Quality Control in a Large Corpus of {TEI}-Annotated Historical Text},
	rights = {{TEI} Consortium 2013 (Creative Commons Attribution-{NoDerivs} 3.0 Unported License)},
	issn = {2162-5603},
	url = {http://jtei.revues.org/739},
	doi = {10.4000/jtei.739},
	shorttitle = {Measuring the Correctness of Double-Keying},
	abstract = {Among mass digitization methods, double-keying is considered to be the one with the lowest error rate. This method requires two independent transcriptions of a text by two different operators. It is particularly well suited to historical texts, which often exhibit deficiencies like poor master copies or other difficulties such as spelling variation or complex text structures.            Providers of data entry services using the double-keying method generally advertise very high accuracy rates (around 99.95\% to 99.98\%). These advertised percentages are generally estimated on the basis of small samples, and little if anything is said about either the actual amount of text or the text genres which have been proofread, about error types, proofreaders, etc. In order to obtain significant data on this problem it is necessary to analyze a large amount of text representing a balanced sample of different text types, to distinguish the structural {XML}/{TEI} level from the typographical level, and to differentiate between various types of errors which may originate from different sources and may not be equally severe.         This paper presents an extensive and complex approach to the analysis and correction of double-keying errors which has been applied by the {DFG}-funded project "Deutsches Textarchiv" (German Text Archive, hereafter {DTA}) in order to evaluate and preferably to increase the transcription and annotation accuracy of double-keyed {DTA} texts. Statistical analyses of the results gained from proofreading a large quantity of text are presented, which verify the common accuracy rates for the double-keying method.},
	issue = {Issue 4},
	journaltitle = {Journal of the Text Encoding Initiative},
	author = {Haaf, Susanne and Wiegand, Frank and Geyken, Alexander},
	editora = {Jannidis, Fotis and Rehbein, Malte and Romary, Laurent},
	editoratype = {collaborator},
	urldate = {2013-04-01},
	date = {2013-03-15},
	langid = {english},
	keywords = {act\_DataRecognition, bigdata{\textasciitilde}, obj\_Text},
}

@inproceedings{irvine_digitizing_2012,
	location = {Montréal, Canada},
	title = {Digitizing 18th-Century French Literature: Comparing transcription methods for a critical edition text},
	url = {http://www.aclweb.org/anthology/W12-2509},
	abstract = {We compare four methods for transcribing early printed texts. Our comparison is through a case-study of digitizing an eighteenth- century French novel for a new critical edition: the 1784 Lettres ta??tiennes by Jose?phine de Monbart. We provide a detailed error analy- sis of transcription by optical character recog- nition ({OCR}), non-expert humans, and expert humans and weigh each technique based on accuracy, speed, cost and the need for schol- arly overhead. Our findings are relevant to 18th-century French scholars as well as the entire community of scholars working to pre- serve, present, and revitalize interest in litera- ture published before the digital age.},
	pages = {64--68},
	booktitle = {Proceedings of the {NAACL}-{HLT} 2012 Workshop on Computational Linguistics for Literature},
	publisher = {Association for Computational Linguistics},
	author = {Irvine, Ann and Marcellesi, Laure and Zomorodian, Afra},
	date = {2012-06},
	langid = {english},
	keywords = {act\_DataRecognition},
}

@article{powell_virtual_2004,
	title = {Virtual teams: a review of current literature and directions for future research},
	volume = {35},
	url = {http://portal.acm.org/citation.cfm?id=968464.968467},
	doi = {10.1145/968464.968467},
	shorttitle = {Virtual teams},
	abstract = {Information technology is providing the infrastructure necessary to support the development of new organizational forms. Virtual teams represent one such organizational form, one that could revolutionize the workplace and provide organizations with unprecedented levels of flexibility and responsiveness. As the technological infrastructure necessary to support virtual teams is now readily available, further research on the range of issues surrounding virtual teams is required if we are to learn how to manage them effectively. While the findings of team research in the traditional environment may provide useful pointers, the idiosyncratic structural and contextual issues surrounding virtual teams call for specific research attention.This article provides a review of previously published work and reports on the findings from early virtual team research in an effort to take stock of the current state of the art. The review is organized around the input - process - output model and categorizes the literature into issues pertaining to inputs, socio-emotional processes, task processes, and outputs. Building on this review we critically evaluate virtual team research and develop research questions that can guide future inquiry in this fertile are of inquiry.},
	pages = {6--36},
	number = {1},
	journaltitle = {{SIGMIS} Database},
	author = {Powell, Anne and Piccoli, Gabriele and Ives, Blake},
	urldate = {2009-05-02},
	date = {2004},
	langid = {english},
	keywords = {act\_Communicating, goal\_Collaboration, obj\_Infrastructures, obj\_People},
}

@article{nolden_verspielt?_2012,
	title = {Verspielt? Konsequenzen eines konstruktivistischen Weltbildes für die (digitale) historische Editorik},
	url = {http://universaar.uni-saarland.de/journals/index.php/zdg/article/view/297},
	abstract = {In recent times advancements of digital historic source editions suffered stagnation. Some hints point to the suspicion that shifting principles of traditional media forms into the digital sphere has led to an obstacle. Those principles seem to block sight to identify intrinsic properties that span the real qualities in web usage. This article accentuates four such properties, relates them to constuctivist's thoughts about generating knowledge and compares the outcome with standard types of digital source editions. This has to take into consideration the astounding finding that the intrinsic properties are realized most consistently in Multiplayer-Videogames. Construction of digital source editions that include technologies, processes and methods of those games may open a more suitable way of dealing with historic sources for the future.},
	number = {1},
	journaltitle = {Zeitschrift für digitale Geschichtswissenschaften},
	author = {Nolden, Nico},
	date = {2012},
	langid = {german},
	keywords = {act\_Crowdsourcing, goal\_Collaboration, goal\_Enrichment, t\_Encoding},
}

@article{nagasaki_towards_2013,
	title = {Towards a digital research environment for Buddhist studies},
	volume = {28},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/28/2/296},
	doi = {10.1093/llc/fqs076},
	abstract = {This article will explain the structure and function of a digital research environment for Buddhist studies entitled Research Base for Indian and Buddhist Studies ({RBIB}). In the field of Buddhist studies (and especially in the area of Indian Buddhism), scholars face various obstacles in their efforts to share materials—in both paper and digital media. In establishing the {RBIB}, we have considered it to be of paramount importance to preserve the continuity of the prior tradition of studies within the paper medium, while at the same time developing a methodology that takes best advantage of the digital medium. After identifying six basic conditions that will satisfy the demands of both approaches, we have implemented a Web collaboration system that fulfills these conditions. Our system also makes functional parts of the {RBIB} that enable the presentation of the relationships between arbitrary fragments of related texts in various canonical languages through a user-friendly interface, which in turn allows users to learn how to edit and browse relationship data. Our system has thus far been highly evaluated by testers, and it has been further facilitated by recent developments in Information Communications Technology. Humanities digitization has a deep mutual relationship with this new technology.},
	pages = {296--300},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Nagasaki, Kiyonori and Tomabechi, Toru and Shimoda, Masahiro},
	urldate = {2013-06-12},
	date = {2013-06-01},
	langid = {english},
	keywords = {obj\_Manuscripts, obj\_VREs},
}

@article{zielinski_tei_2009,
	title = {{TEI} documents in the grid},
	volume = {24},
	url = {http://llc.oxfordjournals.org/content/24/3/267.abstract},
	doi = {10.1093/llc/fqp016},
	abstract = {This article describes the life cycle of a {TEI} Document within {TextGrid}, an {eHumanities} platform for scholarly text processing, in which structured search is based on the {TEI} framework and metadata with restricted values. A workbench is provided that offers tools for handling {TEI} documents, {TextGridLab}, making it easier to annotate, process, search, and persistently store new digitized texts. The digitization and annotation of the Campe dictionary1 serves as a first test bed. The overall framework of {TextGrid} is very generic and can handle different types of text (literary editions, linguistic corpora, lexica) as well as heterogeneous data formats (plain text, {XML}/{TEI}, images). In fact, the {TextGrid} repository, {TextGridRep}, is designed as a digital virtual library over federated archives, where humanities projects are invited to participate. Sharing of data is enabled by means of a grid-based architecture. Specifically the middleware includes most of the treatment of authorization, search, and file management. {TextGrid} is entirely based on open source software including Eclipse2 and Globus Toolkit.},
	pages = {267 --279},
	number = {3},
	journaltitle = {Literary and Linguistic Computing},
	author = {Zielinski, Andrea and Pempe, Wolfgang and Gietz, Peter and Haase, Martin and Funk, Stefan and Simon, Christian},
	urldate = {2012-01-05},
	date = {2009},
	langid = {english},
	keywords = {obj\_Infrastructures},
}

@article{arthur_reading_2009,
	title = {Reading tools : the enhancement of an online scholarly research environment},
	url = {https://circle.ubc.ca/handle/2429/5733},
	shorttitle = {Reading tools},
	abstract = {The purpose of this study was to determine whether in the context of an article-critiquing assignment in a teacher education course, access to Open Journal Systems’ Reading Tools, significantly enhances university students’ 1) comprehension, 2) ability to evaluate the quality of the article, and 3) level of confidence in using the article, critically or supportively, as part of their present work as students at university or future work as teachers in a school. The sample for this experimental design consisted of 75 pre-service teacher education students who comprised two sections of the compulsory course entitled “Information Communication Technology for Secondary Teachers” were randomly assigned to either have access to the Reading Tools or no access to complete an authentic assignment as part of the course. As part of the assignment students were asked to create a list of 10-12 “talking points” for each of four articles prior to the class in which they would use the points as a basis of discussing the article with their group. These “talking points” were then marked by two independent markers using a rubric to arrive at a “comprehension” and “critique score.” An article usability score was based on survey questions that all students answered after completing each article assignment. Students in the treatment groups were compared to students who didn’t have access to the Reading Tools. Results of the mixed design {ANOVA} used to analyze the data indicated there were no significant differences found between the two groups of students in the areas of comprehension, ability to critique, or article utilization. However, student survey feedback indicated a positive perception for the Reading Tools ability to provide value to the online research reading environment. Reasons are considered for why the tools offered no advantage to search-wise students, while further studies are proposed for the development of reading tools.},
	author = {Arthur, Peter J},
	urldate = {2012-04-25},
	date = {2009},
	langid = {english},
	keywords = {act\_RelationalAnalysis, goal\_Interpretation, obj\_VREs},
}

@report{unsworth_our_2006,
	title = {Our Cultural Commonwealth Report},
	url = {http://www3.isrl.illinois.edu/~unsworth/ sdl.html},
	abstract = {Our cultural commonwealth: the report of the American Council of Learned Societies Commission on cyberinfrastructure for the humanities and so-
cial sciences.},
	institution = {American Council of Learned Societies ({ACLS})},
	author = {Unsworth, John},
	date = {2006},
	langid = {english},
	keywords = {*****, act\_Conceptualizing, obj\_Methods},
}

@article{ge_information-seeking_2010,
	title = {Information-Seeking Behavior in the Digital Age: A Multidisciplinary Study of Academic Researchers},
	volume = {71},
	issn = {0010-0870, 2150-6701},
	url = {http://crl.acrl.org/content/71/5/435},
	shorttitle = {Information-Seeking Behavior in the Digital Age},
	abstract = {This article focuses on how electronic information resources influence the information-seeking process in the social sciences and humanities. It examines the information-seeking behavior of scholars in these fields, and extends the David Ellis model of information-seeking behavior for social scientists, which includes six characteristics: starting, chaining, browsing, differentiating, monitoring, and extracting. The study was conducted at Tennessee State University ({TSU}). Thirty active social sciences and humanities faculty, as well as doctoral students, were interviewed about their use of electronic information resources for research purposes, their perception of electronic and print materials, their opinions concerning the Ellis model, and ways the model might apply to them. Based on the interview results, the researcher provides suggestions on how current information services and products can be improved to better serve social sciences and humanities researchers. The author makes recommendations for improving library services and technologies to better meet the needs of social sciences and humanities scholars.},
	pages = {435--455},
	number = {5},
	journaltitle = {College \& Research Libraries},
	shortjournal = {Coll. res. libr.},
	author = {Ge, Xuemei},
	urldate = {2012-04-25},
	date = {2010-09-01},
	langid = {english},
	keywords = {act\_Archiving, act\_Query/Retrieve, obj\_Data/Databases, obj\_Documents},
}

@unpublished{grimme_blick_2011,
	title = {Ein Blick auf die Grid-Architektur},
	url = {http://www.wissgrid.de/workgroups/ap2/workshop-2011-01.html},
	note = {Workshop: Virtuelle Forschungsumgebungen aufbauen –mit D-Grid},
	author = {Grimme, Christian},
	date = {2011-01-19},
	langid = {german},
	keywords = {obj\_Infrastructures, obj\_VREs},
}

@article{gleim_ehumanities_2009,
	title = {{eHumanities} Desktop – eine webbasierte Arbeitsumgebung für die geisteswissenschaftliche Fachinformatik},
	author = {Gleim, Rüdiger and Waltinger, Ulli and Ernst, Alexandra and Esch, Dietmar and Feith, Tobias and Mehler, Alexander},
	date = {2009},
	langid = {german},
	keywords = {goal\_Collaboration, obj\_Infrastructures, obj\_VREs},
}

@collection{schomburg_digitale_2011,
	location = {Köln},
	edition = {2., ergänzte Fassung},
	title = {Digitale Wissenschaft. Stand und Entwicklung digital vernetzter Forschung in Deutschland [Tagung, 20./21. September 2010, Köln]},
	url = {http://www.digitalewissenschaft.},
	publisher = {hbz},
	editor = {Schomburg, Silke and Puschmann, Cornelius and Lobin, Henning},
	date = {2011},
	langid = {german},
	keywords = {X-{CHECK}, meta\_Assessing, obj\_DigitalHumanities, obj\_Research},
}

@inproceedings{van_zundert_alfalab:_2009,
	title = {Alfalab: Construction and Deconstruction of a Digital Humanities Experiment},
	isbn = {978-0-7695-3877-8},
	doi = {10.1109/e-Science.2009.8},
	shorttitle = {Alfalab},
	abstract = {This paper presents project 'Alfalab'. Alfalab is a collaborative frame work project of the Royal Netherlands Academy of Arts and Sciences ({KNAW}). It explores the success and fail factors for virtual research collaboration and supporting digital infrastructure in the Humanities. It does so by delivering a virtual research environment engineered through a virtual R\&D collaborative and by drawing in use cases and feedback from Humanities researchers from two research fields: textual historical text research and historical {GIS}-application. The motivation for the project is found in a number of commonly stated factors that seem to be inhibiting general application of virtualized research infrastructure in the Humanities. The paper outlines the project's motivation, key characteristics and implementation. One of the pilot applications is described in greater detail.},
	eventtitle = {Fifth {IEEE} International Conference on e-Science, 2009. e-Science '09},
	pages = {1--5},
	booktitle = {Fifth {IEEE} International Conference on e-Science, 2009. e-Science '09},
	publisher = {{IEEE}},
	author = {van Zundert, J. and Zeldenrust, D. and Beaulieu, A.},
	date = {2009-12-09},
	langid = {english},
	keywords = {obj\_VREs},
}

@inproceedings{barga_virtual_2007,
	title = {A Virtual Research Environment ({VRE}) for Bioscience Researchers},
	isbn = {0-7695-2992-5, 978-0-7695-2992-9},
	url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04401895},
	doi = {10.1109/ADVCOMP.2007.14},
	abstract = {In this paper we present the Research Information Centre ({RIC}), a virtual research environment being jointly developed by the Technical Computing Group at Microsoft and The British Library. We view researchers as extreme information workers and the purpose of the {RIC} is to support researchers in managing the increasingly complex range of tasks involved in carrying out research. Our first implementation of the {RIC} is focused on the biomedical researcher, leveraging commercial off-the-shelf software to the extent possible. However, the base architecture of {RIC} is designed so that it can be re-used for research in other domains.},
	pages = {31--38},
	publisher = {{IEEE}},
	author = {Barga, Roger and Andrews, Stephen and Parastatidis, Savas},
	urldate = {2011-08-03},
	date = {2007},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_Infrastructures, obj\_VREs},
}
@incollection{mounier_ledition_2010,
	title = {L'édition électronique : un nouvel eldorado pour les sciences humaines ?},
	rights = {Licence Creative Commons Attribution - Pas d’Utilisation Commerciale - Pas de Modification 3.0 France},
	isbn = {978-2-9536419-0-5},
	url = {http://press.openedition.org/169},
	shorttitle = {L'édition électronique},
	abstract = {En guise d’introduction, il me semble nécessaire de faire le point sur un certain nombre de questions qui ont structuré les débats sur l’édition électronique depuis dix ans. À mon sens, ces questions sont aujourd’hui largement obsolètes.1. La désintermédiation. C’est le terme par lequel on a pu dire que la mise en ligne des publications scientifiques revenait à la suppression des intermédiaires (c’est-à-dire les éditeurs). On passerait donc à un modèle de distribution directe du producteur au consommateur. On sait aujourd’hui que cette approche n’est pas pertinente, parce que toute forme de communication est marquée par la présence d’intermédiaires, aussi discrets soient-ils. Aujourd’hui, les plates-formes où les utilisateurs sont invités à déposer leurs propres productions (les archives ouvertes par exemple) sont des intermédiaires. De par sa seule existence, la plate-forme a un impact éditorial sur les contenus qu’elle héberge ; impact dont il faut tenir compte.2. L’électronique versus le papier. Faut-il publier au format électronique ou papier ? Cette question ne vaut plus la peine d’être posée ; en ces termes en tout cas. La réalité est qu’aujourd’hui, l’édition est forcément électronique ; les auteurs, les éditeurs, les chefs de fabrication et même les imprimeurs travaillent en numérique. La question du papier est donc réduite à celle du support de diffusion. Et même dans ce cas, elle ne concerne qu’un certain type de documents. Typiquement, les livres. Pour les revues, [...]},
	pages = {149--156},
	booktitle = {{OpenEdition} Press},
	publisher = {{OpenEdition} Press},
	author = {Mounier, Pierre},
	urldate = {2012-05-30},
	date = {2010-03-25},
	langid = {french},
}

@article{merriam_application_2003,
	title = {An Application of Authorship Attribution by Intertextual Distance in English},
	rights = {© Tous droits réservés},
	issn = {1638-9808},
	url = {http://corpus.revues.org/35?&id=35},
	abstract = {Une application d’attribution d’auteur au moyen de la distance intertextuelle en anglais Le calcul de distance intertextuelle que C. et D. Labbé appliquent aux textes français peut être utilisé pour différencier les œuvres d’au moins deux auteurs dramatiques contemporains de l’époque élisabéthaine, William Shakespeare et Thomas Middleton. Bien que les 46 textes sous étude, transcrits avec une orthographe moderne, ne soient pas lemmatisés et que seuls des échantillons de textes de même longueur aient été utilisés, les indices de distance intertextuelle qu’on a pu ainsi établir empiriquement sont du même ordre de grandeur que ceux qu’ont établis C. et D. Labbé pour le français. Timon of Athens considéré comme étant pour deux-tiers de Shakespeare et pour un tiers de Middleton se place entre le groupe des œuvres de Shakespeare et celui des œuvres de Middleton dans une analyse multidimensionnelle de 1035 distances intertextuelles.},
	number = {2},
	journaltitle = {Corpus},
	author = {Merriam, Thomas},
	editora = {Luong, Xuan},
	editoratype = {collaborator},
	urldate = {2013-07-06},
	date = {2003-12-15},
	langid = {french},
	keywords = {{AnalyzeStatistically}},
}

@article{zanganeh_not_2003,
	location = {New York},
	title = {Not Molière! Ah, Nothing Is Sacred - New York Times},
	url = {http://www.nytimes.com/2003/09/06/books/not-moliere-ah-nothing-is-sacred.html},
	abstract = {To be or not to be Molière: that is the latest question wreaking havoc among French academics.In ''Corneille in the Shadow of Molière,'' a book recently published in France, Dominique Labbé, a},
	journaltitle = {New York Times},
	author = {Zanganeh, Lila Azam},
	urldate = {2013-07-06},
	date = {2003-09-06},
	langid = {english},
}

@collection{baker_guide_2013,
	title = {Guide to Creative Commons for Humanities and Social Sciences Monograph Authors},
	url = {http://oapen-uk.jiscebooks.org/ccguide/},
	abstract = {An output of the {OAPEN}-{UK} project, this guide explores concerns expressed in public evidence given by researchers, learned societies and publishers to inquiries in the House of Commons and the House of Lords, and also concerns expressed by researchers working with the {OAPEN}-{UK} project. We have also identified a number of common questions and have drafted answers, which have been checked by experts including Creative Commons. The guide has been edited by active researchers, to make sure that it is relevant and useful to academics faced with making decisions about publishing.},
	publisher = {{OpenUK} Project},
	editor = {Baker, James and Eve, Martin Paul and Priego, Ernesto},
	date = {2013},
	langid = {english},
}

@online{open_access_publishing_literature_2013,
	title = {Literature Overview [about Open Access]},
	url = {http://project.oapen.org/index.php/literature-overview},
	author = {{Open Access Publishing}},
	date = {2013},
	note = {Den Haag: {OAPen}},
	keywords = {goal\_Dissemination, obj\_ResearchResults},
}

@report{dfg-projekt_radieschen_synthese_2013,
	title = {Synthese},
	url = {http://dx.doi.org/10.2312/RADIESCHEN_007},
	abstract = {Die Reports Technologie, Organisation, Kosten stellen die Ergebnisse einzelner Arbeitspakete des {DFG}-Projekts „Rahmenbedingungen einer disziplinübergreifenden Forschungsdateninfrastruktur“ (Radieschen) dar. Der Report „Synthese“ gibt einen Überblick über die Gesamt-Ergebnisse des Projekts, zeigt Handlungsempfehlungen auf und gibt einen Ausblick auf eine mögliche Zukunft der Forschungsdaten-Infrastrukturen in Deutschland.},
	institution = {{RADIESCHEN}},
	author = {{DFG-Projekt RADIESCHEN}},
	date = {2013},
	langid = {german},
	keywords = {act\_Organizing, goal\_Collaboration, goal\_Storage, obj\_Data},
}
@online{ciula_bibliography_2013,
	title = {A Bibliography of Publications Related to the Text Encoding Initiative},
	url = {http://www.tei-c.org/Support/Learn/tei_bibliography.xml},
	titleaddon = {Text Encoding Inititative},
	editora = {Ciula, Arianna},
	editoratype = {collaborator},
	date = {2013},
}

@online{wieviorka_mettre_2013,
	title = {Mettre le numérique  au service des humanités},
	url = {http://www.liberation.fr/sciences/2013/05/10/mettre-le-numerique-au-service-des-humanites_902086},
	shorttitle = {Michel Wieviorka},
	abstract = {Sociologue et directeur de la Fondation Maison des sciences de l’homme, Michel Wieviorka retrace les moments forts de l’institution, ses figures déclinantes ou montantes, de l’intellectuel sartrien à l’expert des plateaux télé, et prône une approche globale de la recherche enrichie par les nouvelles technologies.},
	titleaddon = {Libération},
	author = {Wieviorka, Michel},
	urldate = {2013-05-14},
	date = {2013-05},
	langid = {french},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}
@online{mimno_topic_nodate,
	title = {Topic Modeling Bibliography},
	url = {http://www.cs.princeton.edu/~mimno/topics.html},
	author = {Mimno, David},
	langid = {english},
	keywords = {t\_TopicModeling},
}

@online{ramsay_dh_2013,
	title = {{DH} Types One and Two},
	url = {http://stephenramsay.us/2013/05/03/dh-one-and-two/},
	abstract = {I just got done with a good twenty-four hours of arguing with people online about digital humanities. It’s not the first time I’ve done this. I’ve been caught up in what some call “the dh meta-discussion” before, and since my own work is mostly devoted to that discussion, I fully expect to be caught up in it again.

But I never feel good about it afterward, and when I reflect on why there is so much rancor (on both sides, a good deal of it coming from me), I sense that we are all talking past one another.

The reason for this, I think, is that there are really two definitions of dh being bandied about.},
	titleaddon = {stehenramsay.us},
	author = {Ramsay, Stephen},
	date = {2013-05-03},
	langid = {english},
	keywords = {meta\_GiveOverview, meta\_Theorizing, obj\_DigitalHumanities},
}

@online{ramsay_dh_2013-1,
	title = {{DH} and {CS}},
	url = {http://stephenramsay.us/2013/04/30/dh-and-cs/},
	abstract = {One often hears, though, that digital humanities is in some sort of fraught relationship with computer science. The main contention is that digital humanists lack humility when they claim to be technologists themselves. They fail to recognize that computer science is an entire field – a complex and multifaceted discipline that requires at least some degree of immersion both in its intellectual content and in its communities of practice. Anyone on the far side of a humanities {PhD} has no hope of catching up, and is therefore doomed to partial (if not complete) misunderstanding of how to build software systems.

One hears, too, a corollary to these dispiriting observations: namely, that since all of this is true, it is absurd and counterproductive to suggest that humanists engaged in certain kinds of intellectual pursuits might profit from learning to program, studying inferential statistics, or gaining some basic facility with design.

I think all of this is based on a serious misunderstanding of both computer science and software development.},
	titleaddon = {Stephen Ramsay},
	author = {Ramsay, Stephen},
	date = {2013-04-30},
	langid = {english},
	keywords = {meta\_Theorizing, obj\_DigitalHumanities},
}

@article{dunne_weblogs:_2004,
	title = {Weblogs: Verdichtung durch Kommentar},
	url = {http://web.fu-berlin.de/phin/beiheft2/b2t04.htm},
	abstract = {Some recent approches to digital media stress the fact that the use of the computer as a medium is based on a symbolic practice, which can be analyzed by philological means. In this context, the praxis of commentary is becoming ever more important: Commentary is not only a means for interpretation of canonical texts of a given culture, but it serves also to draw the basic distinction, what kind of information is worth being dealt with at all and what will be left back as 'waste'. During the last years, weblogs, which can be defined as frequently updated websites consisting of dated entries arranged in reverse chronological order (Walker), have become a popular form of personal web-publishing. After a short introduction to this rather new phenomenon, this contribution focuses on the importance of commentary in weblogs. While, on a collective level, weblog commentaries lead to a 'condensation' (Verdichtung) of information, they also generate individual self-distinction for those who write them. Weblogs can thus serve as an interesting historical starting point for a comparison of changes that reading and writing practices undergo from book culture to electronic media.},
	pages = {35--65},
	number = {2},
	journaltitle = {Beihefte zu Philologie im Netz ({PhiN})},
	author = {Dünne, Jörg},
	date = {2004},
	langid = {german},
	keywords = {act\_Annotating, obj\_AnyObject},
}

@article{kjellberg_i_2010,
	title = {I am a blogging researcher. Motivations for blogging in a scholarly context},
	volume = {15},
	url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/viewArticle/2962/2580},
	abstract = {The number of scholarly blogs on the Web is increasing. In this article, a group of researchers are asked to describe the functions that their blogs serve for them as researchers. The results show that their blogging is motivated by the possibility to share knowledge, that the blog aids creativity, and that it provides a feeling of being connected in their work as researchers. In particular, the blog serves as a creative catalyst in the work of the researchers, where writing forms a large part, which is not as prominent as a motivation in other professional blogs. In addition, the analysis brings out the blog’s combination of functions and the possibility it offers to reach multiple audiences as a motivating factor that makes the blog different from other kinds of communication in scholarly contexts.},
	number = {8},
	journaltitle = {First Monday},
	author = {Kjellberg, Sara},
	date = {2010},
	langid = {english},
	keywords = {act\_Publishing, obj\_ResearchResults, t\_Blogging},
}
@incollection{mccarty_modeling:_2004,
	location = {Oxford},
	edition = {Online Edition},
	title = {Modeling: A Study in Words and Meanings},
	url = {http://www.digitalhumanities.org/companion/},
	abstract = {The question of modeling arises naturally for humanities computing from the prior question of what its practitioners across the disciplines have in common. What are they all doing with their computers that we might find in their diverse activities indications of a coherent or cohesible practice? How do we make the best, most productive sense of what we observe? There are, of course, many answers: practice varies from person to person, from project to project, and ways of construing it perhaps vary even more. In this chapter I argue for modeling as a model of such a practice. I have three confluent goals: to identify humanities computing with an intellectual ground shared by the older disciplines, so that we may say how and to what extent our field is of as well as in the humanities, how it draws from and adds to them; at the same time to reflect experience with computers "in the wild"; and to aim at the most challenging problems, and so the most intellectually rewarding future now imaginable.},
	pages = {(chapter 19)},
	booktitle = {A Companion to Digital Humanities},
	publisher = {Blackwell},
	author = {{McCarty}, Willard},
	editor = {Schreibman, Susan and Siemens, Ray and Unsworth, John},
	date = {2004},
	langid = {english},
	keywords = {act\_Modeling, goal\_Analysis, goal\_Interpretation, meta\_Theorizing, obj\_AnyObject},
}

@online{cohen_digital_2009,
	title = {Digital Humanities [Bibliography]},
	url = {https://www.zotero.org/groups/digital_humanities/items},
	abstract = {A place for all of those interested in how digital media and technology are changing the humanities to discuss and create the future together},
	titleaddon = {Zotero.org},
	editora = {Cohen, Dan},
	editoratype = {collaborator},
	date = {2009},
	keywords = {X-{CHECK}, obj\_AnyObject},
}

@online{spiro_bamboo_2012,
	title = {Bamboo {DiRT} (Digital Research Tools)},
	url = {https://digitalresearchtools.pbworks.com/w/page/17801672/FrontPage},
	abstract = {I’m pleased to announce that a richer, more powerful version of the Digital Research Tools ({DiRT}) wiki has now been released, Bamboo {DiRT}. Please visit http://dirt.projectbamboo.org/ to explore this excellent collection of research tools. With Bamboo {DiRT}, you can browse by tags and by categories such as “new and updated” and “recommended,” see what tools have been mentioned by sources such as {ProfHacker} and {DH} Questions and Answers, and view cleaner, often more detailed resource records. Now a steering committee (of which I am a member) will oversee {DiRT}, ensuring the quality of metadata and adding new tools. If you’re looking for a tool that will help you analyze data, organize research materials, or publish and share information (and much more), Bamboo {DiRT} is now an even better place to make such discoveries.

 

Much of the data in {DiRT} 1.0 was migrated over to Bamboo {DiRT}. Since {DiRT} 1.0 has been superseded, it will no longer be updated, and will remain online only for historical purposes. This means that all {DiRT} accounts will be closed by July 25, 2012.

I encourage you to add new tools to Bamboo {DiRT}, recommend resources, and submit reviews; the more people contribute, the more useful Bamboo {DiRT} becomes.  And please help spread the word!

Thanks to Quinn Dombrowski,Project Bamboo, the Project Team, and the Steering Committee for their excellent work on Bamboo {DiRT}. Thanks also to everyone who contributed to {DiRT} 1.0.},
	author = {Spiro, Lisa and Dombrowski, Quinn},
	urldate = {2011-04-26},
	date = {2012-09-01},
	langid = {english},
	keywords = {X-{CHECK}, obj\_Tools},
}

@online{munson_german_2013,
	title = {German {DH} Theses and Dissertations - a {DARIAH}-{DE} Bibliography},
	url = {https://www.zotero.org/groups/german_dh_theses_and_dissertations_-_a_dariah-de_bibliography},
	abstract = {This group contains a bibliography of completed masters´ theses, Ph.D. dissertations, and habilitations in Germany that have a relationship to Digital Humanities.  It has been put together by the {DARIAH}-{DE} project, the German contribution to {DARIAH}-{EU}.  The bibliography will be regularly expanded with new offerings.},
	titleaddon = {Zotero.org},
	author = {Munson, Matt},
	date = {2013},
	langid = {english},
	keywords = {obj\_Research},
}

@online{siemens_resources_2013,
	title = {Resources for Starting and Sustaining {DH} Centers},
	url = {http://digitalhumanities.org/centernet/resources-for-starting-and-sustaining-dh-centers/},
	abstract = {This resources page lists talks, articles, sample {DH} proposals and other sources of information about ways to start and sustain {DH} centers.  It will be updated as new materials become available.},
	titleaddon = {{CenterNet}},
	author = {Siemens, Lynne},
	date = {2013},
	langid = {english},
	keywords = {meta\_GiveOverview},
}

@article{berry_computational_2011,
	title = {The Computational Turn: Thinking About the Digital Humanities},
	volume = {12},
	url = {http://www.culturemachine.net/index.php/cm/article/viewArticle/440},
	journaltitle = {Culture Machine},
	author = {Berry, David M.},
	date = {2011},
	langid = {english},
	keywords = {act\_Conceptualizing, meta\_Assessing, obj\_DigitalHumanities},
}
@article{aahc_aahc_2000,
	title = {{AAHC} Suggested Guidelines for Evaluating Digital Media Activities in Tenure, Review, and Promotion - Neue Version},
	volume = {3},
	url = {http://hdl.handle.net/2027/spo.3310410.0003.311},
	abstract = {Note: These recommendations were developed in consultation with the Modern Language Association and the American Political Science Association.},
	number = {3},
	journaltitle = {Journal of the Association for History and Computing},
	author = {{AAHC}},
	date = {2000},
	langid = {english},
	keywords = {meta\_Assessing, meta\_DefinePolicy},
}

@collection{price_literary_2012,
	location = {New York},
	title = {Literary Studies in the Digital Age: A Methodological Primer},
	publisher = {{MLA} Publications},
	editor = {Price, Ken and Siemens, Ray},
	date = {2012},
	langid = {english},
	keywords = {X-{CHECK}, meta\_GiveOverview, obj\_Literature},
}

@online{mla_short_2009,
	title = {Short Guide to Evaluation of Digital Work},
	url = {http://wiki.mla.org/index.php/Short_Guide_to_Evaluation_of_Digital_Work},
	titleaddon = {{MLA} Wiki},
	author = {{MLA}},
	urldate = {2010-05-21},
	date = {2009},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Research},
}

@report{atkins_cyberinfrastructure_2003,
	title = {Cyberinfrastructure Report},
	author = {Atkins, Dan},
	date = {2003},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Infrastructures},
}

@online{underwood_why_2012,
	title = {Why digital humanities isn’t actually “the next thing in literary studies.”},
	url = {http://tedunderwood.wordpress.com/2011/12/27/why-we-dont-actually-want-to-be-the-next-thing-in-literary-studies/},
	abstract = {It's flattering for digital humanists to be interpellated by Stanley Fish as the next thing in literary studies. It's especially pleasant since the field is old enough now to be tickled by depictio...},
	titleaddon = {The Stone and the Shell},
	author = {Underwood, Ted},
	urldate = {2012-01-05},
	date = {2012-01},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@online{liu_where_2011,
	title = {Where is Cultural Criticism in the Digital Humanities},
	url = {http://liu.english.ucsb.edu/where-is-cultural-criticism-in-the-digital-humanities/},
	titleaddon = {Alan Liu},
	author = {Liu, Alan},
	date = {2011},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@online{mandell_what_2009,
	title = {What Is/Are/Isn’t the Digital Humanities?},
	url = {http://aims.muohio.edu/?p=1931},
	titleaddon = {Armstrong Institute for Interactive Media Studies},
	author = {Mandell, Laura},
	urldate = {2010-02-24},
	date = {2009-03-02},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@report{knowledge_exchange_virtual_2012,
	title = {Virtual Research Environments – Catalysts of Change. Report on Knowledge Exchange workshop in Birmingham, 17-18 November 2011},
	url = {http://www.knowledge-exchange.info/Default.aspx?ID=451},
	abstract = {{\textgreater} The report on the Knowledge Exchange workshop on Virtual Research
{\textgreater} Environments is now available. The workshop brought together experts to
{\textgreater} discuss how these environments can best be developed and supported. The
{\textgreater} most important findings were:
{\textgreater}
{\textgreater}
{\textgreater} Insight is required into the wide band of what Virtual Research Environments
{\textgreater} ({VREs}) are to best support researchers and their demands. {VRE}’s are not ‘one
{\textgreater} thing’: they can vary in shape, size, goal and use. They can range from
{\textgreater} communication platforms to tools to manage and collaborate on large
{\textgreater} datasets. To exploit the full potential of the phenomenon we need to bring
{\textgreater} together a variety of perspectives (e.g. researchers, {IT} staff, librarians, policy
{\textgreater} makers and research funders) and solutions. Better ways are required to reach
{\textgreater} particularly young researchers, who tend to rely on their network of peers
{\textgreater} rather than on institutional support. A joint approach is also important to
{\textgreater} support the development of {VREs} and sustaining the tools that have been
{\textgreater} developed after the project funding ceases.
{\textgreater}
{\textgreater}
{\textgreater}
{\textgreater} One of the suggestions was to collect case studies – of successful and failed
{\textgreater} projects –to gain better understanding of researcher’s behaviour, and the way
{\textgreater} it is affected by the use of {VRE}’s. Through the case studies the value and impact
{\textgreater} of {VRE}’s can be explored, and this should help identify the factors that influence
{\textgreater} use and success. Insights from such case studies will help to support community
{\textgreater} networks in the development and implementation of {VRE}’s and resolve
{\textgreater} sustainability issues more easily. The insights can also be used to inform policy
{\textgreater} makers and funders about effective policy and financial support. There was a
{\textgreater} strong agreement t {\textless}{\textless}image001.jpg{\textgreater}{\textgreater} hat Knowledge Exchange is well-situated to play an
{\textgreater} important role in these knowledge developing and sharing activities to
{\textgreater} integrate {VRE}’s in the research infrastructure.
{\textgreater}
{\textgreater}
{\textgreater}
{\textgreater} The workshop brought together 34 experts in Birmingham on 17 and 18
{\textgreater} November 2011 to explore the potential of Virtual Research Environments
{\textgreater} ({VREs}). Drawing from experiences of the last seven years in a variety of
{\textgreater} projects, the discussions provided valuable insights into how {VREs} can integrate
{\textgreater} more fully in the research infrastructure, both organisationally and socially.
{\textgreater} Such questions were addressed as what changes do they bring about and what
{\textgreater} changes do they presume.},
	institution = {Knowledge Exchange},
	author = {{Knowledge Exchange}},
	date = {2012},
	langid = {english},
	keywords = {meta\_Advocating, obj\_VREs},
}

@online{cinquin_utiliser_2011,
	title = {Utiliser la lexicométrie en histoire (1) : panorama historiographique},
	url = {http://devhist.hypotheses.org/898},
	shorttitle = {Utiliser la lexicométrie en histoire (1)},
	titleaddon = {Devenir historien-ne},
	author = {Cinquin, Sophie},
	urldate = {2011-12-15},
	date = {2011-12-05},
	langid = {french},
	keywords = {X-{CHECK}, goal\_Analysis},
}

@report{brown_university_2007,
	title = {University Publishing In A Digital Age},
	url = {http://scholarlypublishing.org/ithakareport/},
	institution = {Ithaka},
	author = {Brown, Laura and Griffiths, Rebecca and Rascoff, Matthew},
	date = {2007},
	langid = {english},
}

@report{european_commission_towards_2012,
	title = {Towards better access to scientific information: Boosting the benefits of public investments in research},
	url = {http://eur-lex.europa.eu/Notice.do?val=681636:cs&lang=en&list=681636:cs,628868:cs,627302:cs,615192:cs,620116:cs,554973:cs,521632:cs,505454:cs,503256:cs,500987:cs,&pos=1&page=1&nbl=126&pgs=10&hwords=communication~access~&checktexte=checkbox&visu=},
	abstract = {{COMMUNICATION} {FROM} {THE} {COMMISSION} {TO} {THE} {EUROPEAN} {PARLIAMENT}, {THE} {COUNCIL}, {THE} {EUROPEAN} {ECONOMIC} {AND} {SOCIAL} {COMMITTEE} {AND} {THE} {COMMITTEE} {OF} {THE} {REGIONS}},
	author = {{European Commission}},
	date = {2012-07},
	langid = {english},
	keywords = {goal\_Dissemination, obj\_ResearchResults},
}

@article{wisbey_three_1992,
	title = {Three Decades of Literary and Linguistic Computing},
	pages = {247--268},
	journaltitle = {London German Studies 4 (1992)},
	author = {Wisbey, Roy},
	date = {1992},
	langid = {english},
	note = {Wisbey, Roy: Three Decades of Literary and Linguistic Computing. In: London German Studies 4 (1992), S. 247-268.},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{hope_very_2004,
	title = {The Very Large Textual Object: A Prosthetic Reading of Shakespeare},
	volume = {9},
	url = {http://purl.oclc.org/emls/09-3/hopewhit.htm},
	pages = {6.1--36},
	number = {3},
	journaltitle = {Early Modern Literary Studies},
	author = {Hope, Jonathan and Witmore, Michael},
	date = {2004},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, obj\_Literature},
}

@report{mcdonald_value_2012,
	title = {The Value and Benefit of Text Mining to {UK} Further and Higher Education},
	url = {http://bit.ly/jisc-textm},
	shorttitle = {The Value and Benefit of Text Mining},
	institution = {{JISC}},
	author = {{McDonald}, Diane and Kelly, Ursula},
	date = {2012},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@online{heiden_txm_2010,
	title = {The {TXM} Platform: Building Open-Source Textual Analysis Software Compatible with the {TEI} Encoding Scheme},
	url = {http://halshs.archives-ouvertes.fr/halshs-00549764/en},
	shorttitle = {The {TXM} Platform},
	author = {Heiden, Serge},
	urldate = {2011-08-03},
	date = {2010-11-04},
	langid = {english},
	note = {This paper describes the rationale and design of an {XML}-{TEI} encoded corpora compatible analysis platform for text mining called {TXM}. The design of this platform is based on a synthesis of the best available algorithms in existing textometry software. It also relies on identifying the most relevant open-source technologies for processing textual resources encoded in {XML} and Unicode, for efficient full-text search on annotated corpora and for statistical data analysis. The architecture is based on a Java toolbox articulating a full-text search engine component with a statistical computing environment and with an original import environment able to process a large variety of data sources, including {XML}-{TEI}, and to apply embedded {NLP} tools to them. The platform is distributed as an open-source Eclipse project for developers and in the form of two demonstrator applications for end users: a standard application to install on a workstation and an online web application framework.},
	keywords = {{AnalyzeStatistically}, obj\_Tools},
}

@article{hoover_tutors_2012,
	title = {The Tutor's Story: A Case Study of Mixed Authorship},
	url = {http://dh2011abstracts.stanford.edu/xtf/view?docId=tei/ab-110.xml},
	journaltitle = {English Studies},
	author = {Hoover, David L.},
	date = {2012},
	langid = {english},
}

@article{schreibman_text_2010,
	title = {The Text Encoding Initiative. An Interchange Format Once Again},
	volume = {10},
	url = {http://computerphilologie.tu-darmstadt.de/jg08/schreibman.html},
	pages = {11--24},
	journaltitle = {Jahrbuch für Computerphilologie},
	author = {Schreibman, Susan},
	urldate = {2010-08-26},
	date = {2010},
	langid = {english},
	keywords = {meta\_GiveOverview, t\_Encoding},
}

@article{anderson_long_2004,
	title = {The Long Tail},
	url = {http://www.wired.com/wired/archive/12.10/tail.html},
	journaltitle = {Wired},
	author = {Anderson, Chris},
	urldate = {2011-06-05},
	date = {2004-10},
	langid = {english},
	keywords = {act\_Conceptualizing},
}

@online{terras_impact_2012,
	title = {The Impact of Social Media on the Dissemination of Research: Results of an Experiment},
	url = {http://journalofdigitalhumanities.org/1-3/the-impact-of-social-media-on-the-dissemination-of-research-by-melissa-terras/},
	shorttitle = {The Impact of Social Media on the Dissemination of Research},
	titleaddon = {Journal of Digital Humanities},
	author = {Terras, Melissa},
	urldate = {2012-11-25},
	date = {2012-10-02},
	langid = {english},
}
@article{shaw_humanities_2012,
	title = {The Humanities, Digitized. Reconceiving the study of culture},
	url = {http://harvardmagazine.com/2012/05/the-humanities-digitized},
	journaltitle = {Hardvard Magazine},
	author = {Shaw, Jonathan},
	date = {2012},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@incollection{siemens_history_2004,
	location = {Oxford},
	title = {The History of Humanities Computing},
	isbn = {1405103213},
	url = {http://www.digitalhumanities.org/companion/},
	series = {Blackwell Companions to Literature and Culture},
	booktitle = {A Companion to Digital Humanities},
	publisher = {Blackwell},
	author = {Hockey, Susan},
	editor = {Siemens, Ray and Unsworth, John and Schreibman, Susan},
	urldate = {2010-05-17},
	date = {2004},
	langid = {english},
	keywords = {X-{CHECK}, act\_Publishing, meta\_GiveOverview, meta\_Theorizing, obj\_DigitalHumanities, obj\_Methods},
}

@online{mla_committee_on_information_technology_evaluation_2012,
	title = {The Evaluation of Digital Work},
	url = {http://wiki.mla.org/index.php/Evaluation_Wiki},
	abstract = {This wiki is an ongoing project initiated by the {MLA} Committee on Information Technology ({CIT}) as a way for the academic community to develop, gather, and share materials about the evaluation of work in digital media for purposes of tenure and promotion. These materials were initially conceived, written, and hosted by Geoffrey Rockwell when he was a member of the {CIT} (2005-08). The wiki is now maintained as a collaborative project of the {MLA} and other interested parties, such as {HASTAC} (Humanities, Arts, Science and Technology Advanced Collaboratory).},
	titleaddon = {Evaluation Wiki},
	type = {Wiki},
	author = {{MLA Committee on Information Technology} and Rockwell, Geoffrey},
	date = {2012},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, obj\_Research},
}

@article{porsdam_enlightenment_2012,
	title = {The Enlightenment Revisited: Digitization and the Right to Read},
	url = {http://web.fu-berlin.de/phin/phin62/p62t2.htm},
	abstract = {So far, scholarly analyses on digitization have been characterized by the same skepticism and/or awe that modernism and, later, the launch of the internet were able to unfold. This article is an attempt to modify some of these reactions, discourses and conclusions. Instead, we call upon the European and American Enlightenment ideals to explain and discuss current transatlantic digitization library projects: A new digital "Republic of Letters" in the {US} and "Europeana" as its counterpart in Europe. It is argued that classic and more radical notions of Bildung and Enlightenment are useful when trying to understand the premises, visions and ideological battles that determine and challenge current efforts to digitize national libraries and cultural heritage institutions and the {UNESCO} charter of 1972 on the right to read.},
	pages = {21--37},
	number = {62},
	journaltitle = {Philologie im Netz ({PhiN})},
	author = {Porsdam, Helle and Rendix, Mia},
	date = {2012},
	langid = {english},
	keywords = {goal\_Dissemination},
}

@online{underwood_emergence_2012,
	title = {The Emergence of Literary Diction},
	url = {http://journalofdigitalhumanities.org/1-2/the-emergence-of-literary-diction-by-ted-underwood-and-jordan-sellers/},
	titleaddon = {Journal of Digital Humanities},
	author = {Underwood, Ted},
	urldate = {2012-11-25},
	date = {2012-06-26},
	langid = {english},
}

@article{luyckx_effect_2011,
	title = {The effect of author set size and data size in authorship attribution},
	volume = {26},
	url = {http://llc.oxfordjournals.org/content/26/1/35.abstract},
	doi = {10.1093/llc/fqq013},
	abstract = {Applications of authorship attribution `in the wild’ [Koppel, M., Schler, J., and Argamon, S. (2010). Authorship attribution in the wild. Language Resources and Evaluation. Advanced Access published January 12, 2010:10.1007/s10579-009-9111-2], for instance in social networks, will likely involve large sets of candidate authors and only limited data per author. In this article, we present the results of a systematic study of two important parameters in supervised machine learning that significantly affect performance in computational authorship attribution: (1) the number of candidate authors (i.e. the number of classes to be learned), and (2) the amount of training data available per candidate author (i.e. the size of the training data). We also investigate the robustness of different types of lexical and linguistic features to the effects of author set size and data size. The approach we take is an operationalization of the standard text categorization model, using memory-based learning for discriminating between the candidate authors. We performed authorship attribution experiments on a set of three benchmark corpora in which the influence of topic could be controlled. The short text fragments of e-mail length present the approach with a true challenge. Results show that, as expected, authorship attribution accuracy deteriorates as the number of candidate authors increases and size of training data decreases, although the machine learning approach continues performing significantly above chance. Some feature types (most notably character n-grams) are robust to changes in author set size and data size, but no robust individual features emerge.},
	pages = {35 --55},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	author = {Luyckx, Kim and Daelemans, Walter},
	urldate = {2011-12-14},
	date = {2011-04},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Theorizing, t\_Stylometry},
}

@article{fish_digital_2012,
	title = {The Digital Humanities and the Transcending of Mortality},
	url = {http://opinionator.blogs.nytimes.com/2012/01/09/the-digital-humanities-and-the-transcending-of-mortality/?hp},
	journaltitle = {The New York Times "Opinionator"},
	author = {Fish, Stanley},
	date = {2012-01-09},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@report{siemens_credibility_2001,
	title = {The Credibility of Electronic Publishing: A Report to the Humanities and Social Sciences Federation of Canada},
	url = {http://web.viu.ca/hssfc/final/credibility.htm},
	author = {Siemens, Raymond},
	date = {2001},
	langid = {english},
}

@article{clarke_cost_2007,
	title = {The cost profiles of alternative approaches to journal publishing},
	volume = {12},
	url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/2048/1906},
	abstract = {The digital era is having substantial impacts on journal publishing. In order to assist in analysing these impacts, a model is developed of the costs incurred in operating a refereed journal. Published information and estimates are used to apply the model to a computation of the total costs and per-article costs of various forms of journal-publishing. Particular attention is paid to the differences between print and electronic forms of journals, to the various forms of open access, and to the differences between not-for-profit and for-profit publishing undertakings. Insight is provided into why for-profit publishing is considerably more expensive than equivalent activities undertaken by unincorporated mutuals and not-for-profit associations. Conclusions are drawn concerning the current debates among conventional approaches and the various open alternatives.},
	number = {12},
	journaltitle = {First Monday},
	author = {Clarke, Roger},
	date = {2007},
	langid = {english},
}

@inproceedings{hoover_teasing_2010,
	location = {London},
	title = {Teasing out Authorship and Style with t-tests and Zeta},
	url = {http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/html/ab-658.html},
	eventtitle = {Digital Humanities Conference},
	booktitle = {Digital Humanities Conference},
	author = {Hoover, David L.},
	date = {2010},
	langid = {english},
}

@unpublished{verhaar_sustaining_2011,
	location = {Copenhagen},
	title = {Sustaining virtual research environments},
	url = {http://www.knowledge-exchange.info/Admin/Public/DWSDownload.aspx?File=%2fFiles%2fFiler%2fdownloads%2fVRE+Roundtable+May+2011%2fKE_VRE_Roundtable_12_May_2011_Peter_Verhaar.pdf},
	abstract = {This presentations proposes that there are 6 essential elements in a successful business plan for sustaining a {VRE}:
1. Products and services
2. Users / Customers
3. Activities / Key resources
4. Communication
5. Financial model (costs and funding)
6. Partner organisations
The presentation then goes on to expand on each of these elements.},
	note = {Knowledge Exchange roundtable on Virtual Research Environments},
	author = {Verhaar, Peter},
	date = {2011-05-12},
	langid = {english},
	keywords = {obj\_VREs},
}

@unpublished{christensen-dalsgaard_sustainability_2011,
	location = {Copenhagen},
	title = {Sustainability},
	url = {http://www.knowledge-exchange.info/Admin/Public/DWSDownload.aspx?File=%2fFiles%2fFiler%2fdownloads%2fVRE+Roundtable+May+2011%2fKE_VRE_Roundtable_12_May_2011_Birte_Christensen-Dalsgaard.pdf},
	note = {Knowledge Exchange roundtable on Virtual Research Environments},
	author = {Christensen-Dalsgaard, Birte},
	date = {2011-05-12},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_Infrastructures, obj\_VREs},
}

@online{suber_survey_2009,
	title = {Survey on financing books in the humanities and social sciences},
	rights = {Copyright 2002-2004 by Peter Suber, All Rights Reserved},
	url = {http://www.earlham.edu/~peters/fos/2009/10/survey-on-financing-books-in-humanities.html},
	titleaddon = {Open Access News},
	type = {Text},
	author = {Suber, Peter},
	urldate = {2009-10-15},
	date = {2009-10-14},
	langid = {english},
	note = {News from the open access movement},
	keywords = {act\_Publishing, obj\_ResearchResults},
}

@article{chambers_supporting_2002,
	title = {Supporting Teaching and Research in an Online Environment: Developing the University of London Library Model},
	issn = {1435-5205},
	url = {http://liber.library.uu.nl/publish/articles/000292/article.pdf},
	pages = {381--392},
	journaltitle = {Liber Quarterly},
	author = {Chambers, Sally},
	date = {2002},
	langid = {english},
	keywords = {obj\_Infrastructures, obj\_VREs},
}

@inproceedings{eder_stylometry_2011,
	location = {Stanford},
	title = {Stylometry with R (Poster)},
	url = {http://dh2011abstracts.stanford.edu/xtf/view?docId=tei/ab-168.xml},
	eventtitle = {Digital Humanities 2011},
	author = {Eder, Maciej and Rybicki, Jan},
	date = {2011-06},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@report{nines_evaluating_digital_scholarship_group_statement_2011,
	title = {Statement on Authorship},
	url = {http://institutes.nines.org/docs/2011-documents/statement-on-authorship/},
	institution = {{NINES}},
	author = {{NINES Evaluating Digital Scholarship Group} and Ardis, A. and Brake, L. and Della Coletta, C. and Ehnenn, J. and Gagnier, R. and Radcliffe, D. and Workman, N.},
	date = {2011},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Research},
}

@online{ramsay_stanley_2012,
	title = {Stanley and Me},
	url = {http://stephenramsay.us/text/2012/11/08/stanley-and-me.html},
	titleaddon = {stehenramsay.us},
	author = {Ramsay, Stephen},
	date = {2012-11-08},
	langid = {english},
	keywords = {{AnalyzeStatistically}, goal\_Interpretation, meta\_Advocating, meta\_Assessing, meta\_Theorizing, obj\_DigitalHumanities},
}

@unpublished{doove_showcase_2011,
	location = {Copenhagen},
	title = {Showcase of virtual research environment},
	url = {http://www.knowledge-exchange.info/Admin/Public/DWSDownload.aspx?File=%2fFiles%2fFiler%2fdownloads%2fVRE+Roundtable+May+2011%2fKE_VRE_Roundtable_12_May_2011_John_Doove.pdf},
	abstract = {This was a presentation given by John Doove of {SURFfoundation} in the Netherlands at the Knowledge Exchange Roundtable on {VREs} (12 May, 2011, Copenhagen).
It gives several examples of the collaboratories that have been developed and tested by {SURF} in the last several years.
It also summarizes some conclusions from their experiences with collaboratories and outlines the next steps to be taken.  Of these, especially their "Extension Report" sounds very interesting for our purposes in that it has a "Checklist" for starting a {VRE}.  Their definition of a {VRE} and ours in {DARIAH} may differ somewhat (theirs weighted almost entirely toward collaboration with few or no resource finding aids or analysis tools), but this checklist would still be interesting to consider as we develop our {VRE} blueprint.},
	note = {Knowledge Exchange roundtable on Virtual Research Environments},
	author = {Doove, John},
	date = {2011-12-05},
	langid = {english},
	keywords = {goal\_Collaboration, goal\_Dissemination, obj\_Infrastructures, obj\_VREs},
}

@online{sample_serial_2012,
	title = {Serial Concentration is Deep Concentration},
	url = {http://www.samplereality.com/2012/02/12/serial-concentration/},
	titleaddon = {Samplereality},
	author = {Sample, Mark},
	date = {2012-02-12},
	langid = {english},
}

@misc{knowledge_exchange_report_2011,
	title = {Report on the Knowledge Exchange Virtual Research Environments strategic roundtable, 12 May 2011, Copenhagen},
	url = {http://www.knowledge-exchange.info/Admin/Public/DWSDownload.aspx?File=%2fFiles%2fFiler%2fdownloads%2fVRE+Roundtable+May+2011%2fReport_VRE_Roundtable_Copenhagen_12_May_2011.pdf},
	abstract = {The report gives a quick overview of the current state, possibilities, and difficulties for {VREs}.  Though short, it gives a good overview of the situation and some interesting ideas on how to move forward.},
	publisher = {Knowledge Exchange},
	author = {{Knowledge Exchange}},
	date = {2011},
	langid = {english},
	keywords = {obj\_VREs},
}

@online{mounier_quapportent_2011,
	title = {Qu’apportent les digital humanities ? Quelques exemples (1/2)},
	url = {http://leo.hypotheses.org/7598},
	shorttitle = {Qu’apportent les digital humanities ?},
	titleaddon = {L\&\#039;édition électronique ouverte},
	author = {Mounier, Pierre},
	urldate = {2011-10-09},
	date = {2011-10-07},
	langid = {french},
}

@online{swan_open_2009,
	title = {Open Access Scholarly Information Sourcebook ({OASIS})},
	url = {http://www.openoasis.org/},
	abstract = {{OASIS} aims to provide an authoritative ‘sourcebook’ on Open Access, covering the concept, principles, advantages, approaches and means to achieving it. The site highlights developments and initiatives from around the world, with links to diverse additional resources and case studies. As such, it is a community-building as much as a resource-building exercise. Users are encouraged to share and download the resources provided, and to modify and customize them for local use. Open Access is evolving, and we invite the growing world-wide community to take part in this exciting global movement.},
	titleaddon = {Open Access Scholarly Information Sourcebook ({OASIS})},
	author = {Swan, Alma and Chan, Leslie},
	date = {2009},
	langid = {english},
}

@article{moretti_network_2011,
	title = {Network Theory, Plot Analysis},
	url = {http://www.newleftreview.org/?page=article&view=2887},
	abstract = {In the last few years, literary studies have experienced what we could call the rise of quantitative evidence. This had happened before of course, without producing lasting effects, but this time it is probably going to be different, because this time we have digital databases and automated data retrieval. As a recent article in Science on ‘Culturomics’ made clear, the width of the corpus and the speed of the search have increased beyond all expectations: today, we can replicate in a few minutes investigations that took a giant like Leo Spitzer months and years of work. [1] When it comes to phenomena of language and style, we can do things that previous generations could only dream of.},
	pages = {80--102},
	number = {68},
	journaltitle = {The New Left Review},
	author = {Moretti, Franco},
	date = {2011},
	langid = {english},
	keywords = {act\_ContentAnalysis, act\_StructuralAnalysis, obj\_Literature},
}

@online{french_make_2009,
	title = {Make “10″ louder, or, the amplification of scholarly communication},
	url = {http://amandafrench.net/2009/12/30/make-10-louder/},
	abstract = {'Here’s a little spreadsheet I put together about Twitter use at three conferences: Digital Humanities 2009, {THATcamp} 2009, and the (just-ended) Modern Language Association convention of 2009:  

As you can probably see, what I did was to divide the total number of tweets during the date range of the conference by the number of [...]'},
	titleaddon = {amandafrench.net},
	author = {French, Amanda},
	urldate = {2010-01-13},
	date = {2009-12-31},
	langid = {english},
}

@incollection{siemens_literary_2004,
	location = {Oxford},
	edition = {Hardcover},
	title = {Literary Analysis},
	isbn = {1405103213},
	url = {http://www.digitalhumanities.org/companion/},
	series = {Blackwell Companions to Literature and Culture},
	booktitle = {A Companion to Digital Humanities},
	publisher = {Blackwell Publishing Professional},
	author = {Rommel, Thomas},
	editor = {Siemens, Ray and Unsworth, John and Schreibman, Susan},
	urldate = {2010-05-17},
	date = {2004},
	langid = {english},
	keywords = {X-{CHECK}, act\_Publishing, meta\_GiveOverview, meta\_Theorizing, t\_Encoding},
}
@online{solon_linguistics_2012,
	title = {Linguistics tool simultaneously compares multiple translations of Othello (Wired {UK})},
	url = {http://www.wired.co.uk/news/archive/2012-09/13/shakespeare-translation-comparison},
	abstract = {A team at Swansea University has developed an online tool that allows researchers to compare multiple translations of Shakespeare at the same time to see how much they vary},
	titleaddon = {Wired {UK}},
	author = {Solon, Olivia},
	urldate = {2012-09-14},
	date = {2012-09-13},
	langid = {english},
	keywords = {act\_RelationalAnalysis, t\_Collation},
}

@online{hacker_is_2012,
	title = {Is Open Access a Moral or a Business Issue? A Conversation with The Pennsylvania State University Press},
	url = {http://chronicle.com/blogs/profhacker/is-open-access-a-moral-or-a-business-issue-a-conversation-with-the-pennsylvania-state-university-press/41267},
	shorttitle = {Is Open Access a Moral or a Business Issue?},
	titleaddon = {{ProfHacker}},
	type = {The Chronicle of Higher Education},
	author = {Hacker, Prof},
	urldate = {2012-07-11},
	date = {2012-07-10},
	langid = {english},
}

@online{burnard_is_1999,
	title = {Is Humanities Computing an Academic Discipline? or, Why Humanities Computing Matters},
	url = {http://www.iath.virginia.edu/hcs/burnard.html},
	titleaddon = {Institute for Advanced Technology in the Humanities},
	author = {Burnard, Lou},
	urldate = {2011-04-20},
	date = {1999},
	langid = {english},
	keywords = {X-{CHECK}, act\_Conceptualizing, meta\_Assessing, obj\_DigitalHumanities},
}

@article{orlandi_is_2002,
	title = {Is Humanities Computing a Discipline?},
	url = {http://computerphilologie.digital-humanities.de/jg02/orlandi.html},
	abstract = {At the beginning, the article mentions the results of an enquiry on how humanities computing is being introduced into the curricula of the European universities; and the most important topics recently discussed around the theme of a theory of humanities computing. It appears that most experts agree on the opinion that humanities computing is an independent discipline, and as such it should be introduced into the faculties of humanities. The article then explains how the foundation of the discipline should be understood, on the basis of computing theory and the methodology of the different humanities disciplines. Source of abstract: http://computerphilologie.tu-darmstadt.de/jg02/orlandi.html},
	pages = {51--58},
	number = {4},
	journaltitle = {Jahrbuch für Computerphilologie},
	author = {Orlandi, Tito},
	date = {2002},
	langid = {english},
	note = {Orlandi, Tito: Is Humanities Computing a Discipline? In: Jahrbuch für Computerphilologie 4 (2002), S. 51-58.},
	keywords = {act\_Conceptualizing, meta\_Assessing, obj\_DigitalHumanities},
}

@article{blei_introduction_2011,
	title = {Introduction to probabilistic topic models},
	abstract = {Abstract: Probabilistic topic models are a suite of algorithms whose aim is to discover the hidden thematic structure in large archives of documents. In this article, we review the main ideas of this ⬚eld, survey the current state-of-the-art, and describe some promising future directions. We ⬚rst describe latent Dirichlet allocation ({LDA}) [8], which is the simplest kind of topic model. We discuss its connections to probabilistic modeling,
and describe two kinds of algorithms for topic discovery. We then survey the growing
body of research that extends and applies topic models in interesting ways. These
extensions have been developed by relaxing some of the statistical assumptions of {LDA},
incorporating meta-data into the analysis of the documents, and using similar kinds
of models on a diversity of data types such as social networks, images and genetics.
Finally, we give our thoughts as to some of the important unexplored directions for
topic modeling. These include rigorous methods for checking models built for data
exploration, new approaches to visualizing text and other high dimensional data, and
moving beyond traditional information engineering applications towards using topic models for more scienti⬚c ends.},
	journaltitle = {Communication of the {ACM}},
	author = {Blei, David M.},
	date = {2011},
	langid = {english},
	keywords = {*****, act\_ContentAnalysis, goal\_Analysis, meta\_GiveOverview, t\_TopicModeling},
}

@article{schreibman_introduction_2011,
	title = {Introduction [to "Evaluating Digital Scholarship"]},
	volume = {2011},
	url = {http://www.mlajournals.org/doi/abs/10.1632/prof.2011.2011.1.123},
	doi = {10.1632/prof.2011.2011.1.123},
	shorttitle = {{MLA} Journals},
	abstract = {This special section of Profession addresses the evaluation of digital scholarship
in the humanities, an issue that has been discussed both within the
digital humanities community and at symposia on the future of scholarly
publishing, in university departments’ and deans’ offices, at professional
conferences, and in scholarly journals as well as mainstream media.1 There
is a growing consensus that humanities disciplines must find ways not simply
of evaluating but also of valuing digital scholarship as part of hiring,
promotion, and tenure decisions. National scholarly organizations such as
the Modern Language Association and the American Council of Learned
Societies have called for departments and institutions to “recognize the
legitimacy of scholarship produced in new media, whether by individuals
or in collaboration, and create procedures for evaluating these forms of
scholarship” (Report of the {MLA} Task Force). We have solicited the following articles to contribute to this continuing dialogue about recognizing and
appropriately rewarding new types of scholarly investigation and communication
made possible by digital media.},
	pages = {123--135},
	number = {1},
	journaltitle = {Profession},
	author = {Schreibman, Susan and Mandell, Laura and Olsen, Stephen},
	urldate = {2011-12-09},
	date = {2011},
	langid = {english},
	keywords = {meta\_GiveOverview},
}

@article{davis_institutional_2007,
	title = {Institutional Repositories: Evaluating the Reasons for Non-use of Cornell University's Installation of {DSpace}},
	volume = {13},
	url = {http://works.bepress.com/ir_research/8/},
	abstract = {Problem: While there has been considerable attention dedicated to the development and implementation of institutional repositories, there has been little done to evaluate them, especially with regards to faculty participation.

Purpose: This article reports on a three-part evaluative study of institutional repositories. We describe the contents and participation in Cornell's {DSpace} and compare these results with seven university {DSpace} installations. Through in-depth interviews with eleven faculty members in the sciences, social sciences and humanities, we explore their attitudes, motivations, and behaviors for non-participation in institutional repositories.

Results: Cornell's {DSpace} is largely underpopulated and underused by its faculty. Many of its collections are empty, and most collections contain few items. Those collections that experience steady growth are collections in which the university has made an administrative investment, such are requiring deposits of theses and dissertations into {DSpace}. Cornell faculty have little knowledge of and little motivation to use {DSpace}. Many faculty use alternatives to institutional repositories, such as their personal Web pages and disciplinary repositories, which are perceived to have higher community salience than one's affiliate institution. Faculty gave many reasons for not using repositories: redundancy with other modes of disseminating information, the learning curve, confusion with copyright, fear of plagiarism and having one's work scooped, associating one's work with inconsistent quality, and concerns about whether posting a manuscript constitutes "publishing".

Conclusion: While some librarians perceive a crisis in scholarly communication as a crisis in access to the literature, Cornell faculty perceive this essentially as a non-issue. Each discipline has a normative culture, largely defined by their reward system and traditions. If the goal of institutional repositories is to capture and preserve the scholarship of one's faculty, institutional repositories will need to address this cultural diversity.},
	number = {3},
	journaltitle = {D-Lib Magazine},
	author = {Davis,, Philip M. and Connolly, Matthew J. L.},
	urldate = {2010-03-06},
	date = {2007},
	langid = {english},
}

@inproceedings{jardino_identification_2006,
	location = {Besançon},
	title = {Identification des auteurs de textes courts avec des n-grammes de caractères},
	eventtitle = {Journées internationales d’Analyse statistique des Données Textuelles},
	booktitle = {Actes des Journées internationales d’Analyse statistique des Données Textuelles},
	publisher = {Presses Universitaires de Franche-Comté},
	author = {Jardino, M.},
	date = {2006},
	langid = {french},
}

@online{priego_i_2011,
	title = {"I Smell Smoke": Blogging as an Endangered Species},
	url = {http://hastac.org/blogs/ernesto-priego/2011/11/23/i-smell-smoke-blogging-endangered-species},
	titleaddon = {{HASTAC}},
	author = {Priego, Ernesto},
	date = {2011-11-23},
	langid = {english},
}

@inproceedings{mccarty_humanities_1999,
	title = {Humanities computing as interdiscipline},
	url = {http://www.iath.virginia.edu/hcs/mccarty.html},
	abstract = {In its title, "Is humanities computing an academic discipline?", this Seminar poses a question that when asked is all too often answered with little or no awareness of the assumptions behind it. Because they have remained largely unexamined, these assumptions have tended to misdirect our thinking about what we do as computing humanists and to interfere with our ability to speak cogently about it. My efforts here will concentrate on taking this question apart and examining its major assumptions to see if we cannot make better questions. In passing I will also flag several areas of disciplinary research that we need to bring within our scope in order to address those better questions.},
	eventtitle = {Seminar "Is Humanities Computing an Academic Discipline?"},
	booktitle = {Seminar "Is Humanities Computing an Academic Discipline?"},
	publisher = {Institute for Advanced Technology in the Humanities ({IATH}), University of Virginia},
	author = {{McCarty}, Willard},
	date = {1999},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@online{konnikova_humanities_2012,
	title = {Humanities aren’t a science. Stop treating them like one},
	url = {http://blogs.scientificamerican.com/literally-psyched/2012/08/10/humanities-arent-a-science-stop-treating-them-like-one/},
	titleaddon = {Literally Psyched},
	author = {Konnikova, Maria},
	date = {2012-08-10},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, obj\_Methods},
}

@online{guillaud_humanites_2012,
	title = {Humanités numériques : so what ?},
	url = {http://lafeuille.blog.lemonde.fr/2012/01/31/humanites-numeriques-so-what/},
	shorttitle = {Humanités numériques},
	titleaddon = {La Feuille},
	author = {Guillaud, Hubert},
	urldate = {2012-02-21},
	date = {2012-01-31},
	langid = {french},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@online{unsworth_how_2008,
	title = {How Not To Read A Million Books},
	url = {http://www3.isrl.illinois.edu/~unsworth/hownot2read.html},
	author = {Unsworth, John},
	editora = {Clement, Tanya and Steger, Sara},
	editoratype = {collaborator},
	urldate = {2011-09-20},
	date = {2008},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@online{horn_how_2012,
	title = {How Journals Put Us Behind the Times},
	url = {http://www.insidehighered.com/blogs/how-journals-put-us-behind-times},
	titleaddon = {Inside Higher Ed},
	author = {Horn, Denise},
	date = {2012-02-05},
	langid = {english},
}

@misc{modern_language_association_committee_on_information_technology_guidelines_2002,
	title = {Guidelines for Evaluating Work with Digital Media in the Modern Languages},
	url = {http://www.mla.org/guidelines_evaluation_digital},
	publisher = {{MLA}},
	author = {{Modern Language Association Committee on Information Technology}},
	date = {2002},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Literature, obj\_Research},
}

@online{mla_committee_on_scholarly_editions_guidelines_2007,
	title = {Guidelines for Editors of Scholarly Editions},
	url = {http://www.mla.org/cse_guidelines},
	titleaddon = {Modern Language Association},
	author = {{MLA Committee on Scholarly Editions}},
	urldate = {2010-05-17},
	date = {2007-09-25},
	langid = {english},
	keywords = {act\_Visualizing, goal\_Enrichment, meta\_Assessing, obj\_Tools, t\_Encoding},
}

@misc{nines_general_nodate,
	title = {General Guidelines and Peer Review Criteria for {NINES} Content},
	url = {http://www.nines.org/about/scholarship/9s-guidelines.doc},
	author = {{NINES}},
	langid = {english},
	keywords = {X-{CHECK}, meta\_Assessing, obj\_Research},
}

@article{rowe_editor:_2010,
	title = {From the Editor: Gentle Numbers},
	volume = {61},
	url = {http://mediacommons.futureofthebook.org/mcpress/ShakespeareQuarterly_NewMedia/from-the-editor-gentle-numbers/},
	number = {3},
	journaltitle = {Shakespeare Quarterly},
	author = {Rowe, Katherine},
	urldate = {2011-11-27},
	date = {2010},
	langid = {english},
}

@online{crocker_forum_2012,
	title = {{FORUM} {II}: The State(s) of Review},
	url = {http://postmedieval-forum.com/forums/forum-ii-states-of-review/},
	abstract = {{FORUM} {II} is in part prompted by postmedieval‘s experiment in crowd review, which took place July-September 2011. All posts in this Forum are open to public comment.

Introduction: How Open, or, Can Vulnerability Go Digital?
Holly A. Crocker

In the Shape of a Crowd
Jen Boyle

Building Community
Sarah Werner

“Yeah, but good luck getting it peer reviewed.”
Bonnie Wheeler

Not Far From, but Close to, the Madding Crowd Review 
Eileen A. Joy

Experiment and Replication in the Humanities
Katherine Rowe

Saving Tenure, or Helping to Kill it?:  A Few Words about “Publish, then Filter”
Sharon O’Dair

Eight Crowded Paragraphs Collaborating Openly
Martin K. Foys},
	titleaddon = {postmedieval {FORUM}},
	editora = {Crocker, Holly A.},
	editoratype = {collaborator},
	date = {2012-03},
	langid = {english},
	keywords = {X-{CHECK}},
}

@report{cohen_final_2008,
	title = {Final Report: Tools for Data-Driven Scholarship: Past, Present, Future.},
	url = {http://mith.umd.edu/tools/final-report.html},
	institution = {Center for History and New Media, George Mason University / Maryland Institute for Technology and the Humanities},
	author = {Cohen, Dan and Fraistat, Neil and Kirschenbaum, Matthew G. and Scheinfeldt, Tom},
	date = {2008},
	langid = {english},
	keywords = {goal\_Analysis, meta\_Assessing, obj\_AnyObject, obj\_Tools},
}

@article{beauvisage_exploiter_2001,
	title = {Exploiter des données morphosyntaxiques pour l'étude statistique des genres. Application au roman policier},
	url = {http://www.revue-texto.net/index.php?id=629},
	journaltitle = {Texto!},
	author = {Beauvisage, Thomas},
	date = {2001},
	langid = {french},
	keywords = {{AnalyzeStatistically}, obj\_Language},
}

@inproceedings{byerly_evaluating_2012,
	title = {Evaluating Digital Work for Tenure and Promotion: A Workshop for Evaluators and Candidates},
	url = {http://www.mla.org/resources/documents/rep_it/dig_eval},
	abstract = {The workshop will provide materials and facilitated discussion about evaluating work in digital media (e.g., scholarly editions, databases, digital mapping projects, born-digital creative or scholarly work). Designed for both creators of digital materials (candidates for tenure and promotion) and administrators or colleagues who evaluate those materials, the workshop will propose strategies for documenting, presenting, and evaluating such work.},
	eventtitle = {{MLA} Convention 2012},
	publisher = {{MLA}},
	author = {Byerly, Alison and Rowe, Katherine A. and Schreibman, Susan},
	date = {2012-01},
	langid = {english},
	keywords = {meta\_DefinePolicy},
}

@online{coble_evaluating_2012,
	title = {Evaluating {DH} Work: Guidelines for Librarians},
	url = {http://acrl.ala.org/dh/2012/12/03/evaluating-dh-work-guidelines-for-librarians/},
	shorttitle = {Evaluating {DH} Work},
	abstract = {In this post, Zach Coble explores the benefits of creating guidelines for the evaluation of librarians’ digital humanities work for the purposes of hiring, appointment, tenure, and promotion,...},
	titleaddon = {dh+lib},
	author = {Coble, Zach},
	urldate = {2012-12-06},
	date = {2012-12-03},
	langid = {english},
	keywords = {X-{CHECK}, meta\_Assessing, obj\_ResearchResults},
}

@report{[esf]_european_2011,
	title = {European Peer Review Guide. Integrating Policies and Practices into Coherent Procedures},
	url = {http://www.esf.org/activities/mo-fora/publications.html},
	institution = {European Science Foundation},
	author = {{[ESF]}},
	date = {2011},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Research, obj\_ResearchResults},
}

@article{liu_essay_2012,
	title = {Essay on opportunities for humanities programs in digital era},
	url = {http://www.insidehighered.com/views/2012/10/01/essay-opportunities-humanities-programs-digital-era},
	journaltitle = {Inside Higher Ed},
	author = {Liu, Alan and Thomas, William G.},
	urldate = {2012-10-01},
	date = {2012-10-01},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}
@article{svensson_envisioning_2012,
	title = {Envisioning the Digital Humanities},
	volume = {6},
	url = {http://digitalhumanities.org/dhq/vol/6/1/000112/000112.html},
	abstract = {Over the last couple of years, it has become increasingly clear that the digital humanities is associated with a visionary and forward-looking sentiment, and that the field has come to constitute a site for far-reaching discussions about the future of the field itself as well as the humanities at large. Based on a rich set of materials closely associated with the formation of the digital humanities, this article explores the visions and expectations associated with the digital humanities and how the digital humanities often becomes a laboratory and means for thinking about the state and future of the humanities. It is argued that this forward-looking sentiment comes both from inside and outside the field, and is arguably an important reason for the attraction and importance of the field. Furthermore, the author outlines a visionary scope for the digital humanities and offers a personal visionary statement as the endpoint to the article series.},
	number = {1},
	journaltitle = {Digital Humanities Quarterly},
	author = {Svensson, Patrik},
	date = {2012},
	langid = {english},
	keywords = {meta\_Assessing, meta\_ProjectManagement, obj\_DigitalHumanities},
}

@incollection{brown_dont_2011,
	location = {Edmonton},
	title = {Don't Mind the Gap: Evolving Digital Modes of Scholarly Production across the Digital-Humanities Divide},
	pages = {203--231},
	booktitle = {The Culture of the Humanities.},
	publisher = {University of Alberta},
	author = {Brown, Susan},
	editor = {Coleman, Daniel and Kamboureli, Smaro},
	date = {2011},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities, obj\_Methods},
}

@online{fitzpatrick_[risky_2011,
	title = {[Do the risky thing]},
	author = {Fitzpatrick, Kathleen},
	date = {2011},
	langid = {english},
}

@online{mandell_digital_2010,
	title = {Digital Humanities: The Bright Spot},
	url = {http://aims.muohio.edu/?p},
	shorttitle = {Digital Humanities},
	titleaddon = {{AIMS}},
	author = {Mandell, Laura},
	urldate = {2010-01-13},
	date = {2010-01-13},
	langid = {english},
}

@misc{guiliano_digital_2012,
	title = {Digital Humanities Topic Modeling (Bibliography)},
	url = {https://www.zotero.org/groups/digital_humanities_topic_modeling/items},
	author = {Guiliano, Jennifer},
	date = {2012},
	langid = {english},
	keywords = {X-{CHECK}},
}

@report{nines/neh_summer_institute_group_digital_2011,
	title = {Digital Humanities Scholarship: Recommendations for Chairs in Language and Literature Departments},
	url = {http://institutes.nines.org/docs/2011-documents/recommendations-for-chairs/},
	institution = {{NINES}},
	author = {{NINES/NEH Summer Institute Group} and Booth, Alison and Gilbert, Pamela K. and Olsen, Steve and Pasanek, Bradley M.},
	date = {2011},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Literature, obj\_Research},
}

@online{presner_digital_2010,
	title = {Digital Humanities 2.0: A Report on Knowledge},
	url = {http://cnx.org/content/m34246/latest/},
	titleaddon = {Connexions},
	author = {Presner, Todd},
	urldate = {2011-07-06},
	date = {2010},
	langid = {english},
	keywords = {meta\_Advocating, meta\_Assessing, obj\_DigitalHumanities},
}

@article{schlieder_digital_2011,
	title = {Digital Humanities - Technologien für die Geisteswissenschaften},
	url = {http://www.uni-bamberg.de/fileadmin/uni/verwaltung/presse/Publikationen/uni.vers/univers_forschung_2011/02_Digital_Humanities-Einfuehrung.pdf},
	pages = {4--5},
	issue = {Mai 2011},
	journaltitle = {uni.vers, Magazin der Otto-Friedrich-Universität Bamberg},
	author = {Schlieder, Christoph},
	date = {2011},
	langid = {german},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{lohr_dickens_2013,
	title = {Dickens, Austen and Twain, Through a Digital Lens},
	url = {http://www.nytimes.com/2013/01/27/technology/literary-history-seen-through-big-datas-lens.html?pagewanted=all},
	journaltitle = {The New York Times},
	author = {Lohr, Steve},
	date = {2013-01-26},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Literature},
}

@online{mattern_dh:_2011,
	title = {{DH}: The Name That Does No Favors},
	url = {http://www.wordsinspace.net/wordpress/2011/02/17/dh-the-name-that-does-no-favors/},
	titleaddon = {Words in Space},
	author = {Mattern, Shannon Christine},
	urldate = {2011-08-30},
	date = {2011-02-17},
	langid = {english},
}

@online{owens_defining_2012,
	title = {Defining Data for Humanists: Text, Artifact, Information or Evidence?},
	url = {http://journalofdigitalhumanities.org/1-1/defining-data-for-humanists-by-trevor-owens/},
	shorttitle = {Defining Data for Humanists},
	titleaddon = {Journal of Digital Humanities},
	author = {Owens, Trevor},
	urldate = {2012-11-25},
	date = {2012-03-16},
	langid = {english},
}

@online{goodman_curious_2012,
	title = {Curious Rainbows: the humanities in the digital age},
	url = {http://cardiffbookhistory.wordpress.com/2012/11/06/curious-rainbows/},
	titleaddon = {Cardiff Book History},
	author = {Goodman, Michael},
	date = {2012-11},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@online{mcgann_culture_2004,
	title = {Culture and Technology: The Way We Live Now, What Is To Be Done?},
	url = {http://www2.iath.virginia.edu/jjm2f/nlh04web.htm},
	abstract = {Paper delivered at the University of Chicago, April 23, 2004. Dec. 27, 2004.},
	titleaddon = {Jerome {McGann}},
	author = {{McGann}, Jerome},
	date = {2004},
	langid = {english},
	keywords = {act\_Conceptualizing, meta\_Assessing, obj\_DigitalHumanities},
}

@online{ball_cultural_2010,
	title = {Cultural goldmine lurks in digitized books : Nature News},
	url = {http://www.nature.com/news/2010/101216/full/news.2010.677.html},
	titleaddon = {Nature},
	author = {Ball, Philip},
	urldate = {2011-07-07},
	date = {2010-12-16},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata{\textasciitilde}, meta\_GiveOverview},
}

@article{bower_crimes_2011,
	title = {Crime’s digital past. Computer science makes history in a Victorian-era courthouse},
	volume = {180},
	url = {http://www.sciencenews.org/view/feature/id/332393/description/Crime%E2%80%99s_digital_past},
	number = {3},
	journaltitle = {{ScienceNews}},
	author = {Bower, Bruce},
	date = {2011-07-30},
	langid = {english},
	keywords = {X-{CHECK}},
}

@unpublished{unsworth_creating_1997,
	location = {Oxford, England},
	title = {Creating Digital Resources: the Work of Many Hands},
	url = {http://www3.isrl.uiuc.edu/%7Eunsworth/drh97.html},
	note = {Digital Resources for the Humanities},
	author = {Unsworth, John M.},
	urldate = {2009-03-10},
	date = {1997-09},
	langid = {english},
	keywords = {act\_Communicating, goal\_Collaboration, obj\_Infrastructures},
}

@incollection{mahlberg_corpus_2007,
	location = {London},
	title = {Corpus stylistics: bridging the gap between linguistic and literary studies},
	pages = {219--246},
	booktitle = {Text, Discourse and Corpora. Theory and Analysis},
	publisher = {Continuum},
	author = {Mahlberg, Michaela and Stubbs, M. and Teubert, W.},
	editor = {Hoey, M. and Mahlberg, Michaela},
	date = {2007},
	langid = {english},
	keywords = {{AnalyzeStatistically}, act\_StylisticAnalysis},
}

@article{hoover_corpus_2007,
	title = {Corpus Stylistics, Stylometry, and the Styles of Henry James},
	volume = {47},
	pages = {174--203},
	journaltitle = {Style},
	author = {Hoover, David L.},
	date = {2007},
	langid = {english},
}

@article{meister_consensus_1995,
	title = {Consensus ex machina? Consensus qua machina!},
	url = {http://llc.oxfordjournals.org/content/10/4/263.full.pdf},
	pages = {263--270},
	journaltitle = {Literary and Linguistic Computing 10 (1995)},
	author = {Meister, Jan C.},
	date = {1995},
	langid = {english},
	note = {Meister, Jan C.: Consensus ex machina? Consensus qua machina! In: Literary and Linguistic Computing 10 (1995), S. 263-270.},
	keywords = {act\_Conceptualizing, goal\_Analysis},
}

@article{moretti_conjectures_2000,
	title = {Conjectures on World Literature},
	url = {http://newleftreview.org/A2094},
	number = {1},
	journaltitle = {The New Left Review},
	author = {Moretti, Franco},
	date = {2000},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Literature},
}

@article{cowgill_computer_1967,
	title = {Computer Applications in Archaeology},
	volume = {2},
	pages = {17--23},
	number = {1},
	journaltitle = {Computers and the Humanities},
	author = {Cowgill, George L},
	date = {1967-09},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Artefacts, obj\_Tools},
}

@article{benzon_computational_1976,
	title = {Computational linguistics and the humanist},
	volume = {10},
	url = {http://www.springerlink.com/content/27v1017j277h1146/},
	number = {5},
	journaltitle = {Computers and the Humanities},
	author = {Benzon, William and Hays, David G.},
	urldate = {2011-05-18},
	date = {1976},
	langid = {english},
	keywords = {{AnalyzeStatistically}, meta\_Theorizing, obj\_Language},
}

@online{meeks_comprehending_2011,
	title = {Comprehending the Digital Humanities},
	url = {https://dhs.stanford.edu/comprehending-the-digital-humanities/},
	titleaddon = {Digital Humanities Specialist},
	author = {Meeks, Elijah},
	urldate = {2011-05-12},
	date = {2011-02},
	langid = {english},
	keywords = {act\_Conceptualizing, act\_Visualizing, obj\_DigitalHumanities},
}

@article{fitzpatrick_commentpress:_2007,
	title = {{CommentPress}: New (Social) Structures for New (Networked) Texts},
	volume = {10},
	url = {http://quod.lib.umich.edu/cgi/t/text/text-idx?c=jep;view=text;rgn=main;idno=3336451.0010.305},
	doi = {http://dx.doi.org/10.3998/3336451.0010.305},
	number = {3},
	journaltitle = {Journal of Electronic Publishing},
	author = {Fitzpatrick, Kathleen},
	date = {2007},
	langid = {english},
}

@report{van_der_vaart_collaboratories:_2010,
	title = {Collaboratories: Connecting Researchers -  How to facilitate choice, design and uptake of online research collaboratories},
	url = {http://www.surffoundation.nl/nl/publicaties/Documents/Collaboratories%20Connecting%20Researchers9april.pdf},
	abstract = {A report from the {SURFfoundation} concerning best practices in setting up collaboratories.  It came about through research into collaborative environments and interviews with people involved in those projects.
Of great interest for us is the "Checklist questions for setting up a collaboratory," on pages 73-76.},
	institution = {{SURF}},
	author = {van der Vaart, Lilian},
	date = {2010-04},
	langid = {english},
	keywords = {meta\_Assessing, obj\_Infrastructures, obj\_VREs},
}
@online{black_clustering_2012,
	title = {Clustering with Compression for the Historian},
	url = {http://journalofdigitalhumanities.org/1-1/clustering-with-compression-for-the-historian-by-chad-black/},
	titleaddon = {Journal of Digital Humanities},
	author = {Black, Chad},
	urldate = {2012-11-25},
	date = {2012-03-14},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata},
}

@online{flood_charles_2012,
	title = {Charles Dickens identified as author of mystery article},
	url = {http://www.guardian.co.uk/books/2012/jun/25/charles-dickens-identified-author-article},
	abstract = {An article championing the rights of the working classes, published in one of the journals edited by Dickens, has been attributed to the author himself},
	titleaddon = {The Guardian},
	author = {Flood, Alison},
	urldate = {2012-07-09},
	date = {2012-06-25},
	langid = {english},
	keywords = {{AnalyzeStatistically}, t\_Stylometry},
}

@report{national_science_foundation_nsf_changing_2011,
	title = {Changing the Conduct of Science in the Information Age. Summary Report of Workshop Held on November 12, 2010},
	url = {http://www.nsf.gov/pubs/2011/oise11003/},
	institution = {National Science Foundation ({NSF})},
	author = {{National Science Foundation (NSF)}},
	date = {2011-06},
	langid = {english},
}

@online{mohamed_can_2011,
	title = {Can There Be a Digital Humanism?},
	url = {http://www.huffingtonpost.com/feisal-g-mohamed/can-there-be-a-digital-humanism_b_1173188.html},
	abstract = {The sense that technology is inherently a form of progress, rather than a platform for consumerism, is one of the most insidious ideologies of our time, and one that distracts us from meditating on the true sources of human flourishing.},
	titleaddon = {Huffington Post},
	author = {Mohamed, Feisal G.},
	urldate = {2012-01-05},
	date = {2011-12-28},
	langid = {english},
	keywords = {act\_Conceptualizing, obj\_DigitalHumanities},
}

@article{nolin_boundaries_2011,
	title = {Boundaries of research disciplines are paper constructs: Digital Web-based information as a challenge to disciplinary research},
	volume = {16},
	url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3669/3080},
	abstract = {Boundaries of research disciplines are paper constructs: Digital Web-based information as a challenge to disciplinary research Jan Michael Nolin Abstract Modern disciplinary research is partly constructed, and limited, by the medium of paper. It is possible to bypass the restraints imposed by paper in modern Web publication. Still, the research sector keeps publishing as if the qualities of the hard copy should be forced on the Web. This article discusses the role of paper in the construction of the boundaries of disciplines and the challenges from digital Web-based publication.},
	number = {11},
	journaltitle = {First Monday},
	author = {Nolin, Jan Michael},
	date = {2011},
	langid = {english},
}

@article{post_bits_2011,
	title = {Bits And The Bard},
	url = {http://www.forbes.com/forbes/2011/0627/technology-michael-witmore-shakespeare-literature-bits-bard.html},
	abstract = {Michael Witmore has been digitizing Shakespeare. His surprising insights may have far-reaching impact outside literature.},
	journaltitle = {Forbes.com},
	author = {Post, Tom},
	date = {2011-06-27},
	langid = {english},
	keywords = {{AnalyzeStatistically}, bigdata{\textasciitilde}},
}

@article{hayes_bit_2011,
	title = {Bit Lit. With digitized text from five million books, one is never at a loss for words},
	url = {http://www.americanscientist.org/issues/issue.aspx?id=12418&y=2011&no=3&content=true&page=1&},
	issue = {May-June},
	journaltitle = {American Scientist},
	author = {Hayes, Brian},
	urldate = {2011-05-04},
	date = {2011},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@online{[collective]_berlin_2003,
	title = {Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities},
	url = {http://oa.mpg.de/lang/en-uk/berlin-prozess/berliner-erklarung/},
	titleaddon = {{OA} {MPS}},
	editora = {{[Collective]}},
	editoratype = {collaborator},
	urldate = {2011-07-14},
	date = {2003},
	langid = {english},
	keywords = {meta\_Advocating, obj\_DigitalHumanities},
}

@article{juola_authorship_2006,
	title = {Authorship attribution},
	volume = {1},
	pages = {233--334},
	number = {3},
	journaltitle = {Foundations and Trends in Information Retrieval},
	author = {Juola, Patrick},
	date = {2006},
	langid = {english},
	keywords = {*****, {AnalyzeStatistically}, meta\_GiveOverview, t\_Stylometry},
}

@incollection{friedlander_asking_2009,
	location = {Washington},
	title = {Asking Questions and Building a Research Agenda for Digital Scholarship},
	url = {http://www.clir.org/activities/digitalscholar2/friedlander.pdf},
	pages = {1--15},
	booktitle = {Working Together or Apart: Promoting the Next Generation of Digital Scholarship},
	publisher = {Council on Library and Information Resources ({CLIR})},
	author = {Friedlander, Amy},
	editor = {{CLIR}},
	date = {2009},
	langid = {english},
	keywords = {meta\_Advocating, meta\_GiveOverview, obj\_DigitalHumanities},
}

@article{bush_as_1945,
	title = {As We May Think},
	url = {http://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/3881/?single_page=true},
	journaltitle = {The Atlantic},
	author = {Bush, Vannevar},
	date = {1945-07},
	langid = {english},
	note = {doi: 10.3998/3336451.0001.101},
	keywords = {*****, act\_Conceptualizing},
}

@online{rockwell_as_2010,
	title = {As Transparent as Infrastructure: On the research of cyberinfrastructure in the humanities},
	url = {http://cnx.org/content/m34315/latest/},
	titleaddon = {Connexions Web site},
	author = {Rockwell, Geoffrey},
	date = {2010-05-14},
	langid = {english},
	keywords = {act\_Conceptualizing, meta\_Assessing, meta\_GiveOverview, obj\_Infrastructures},
}

@book{rommel_and_1995,
	title = {And Trace it in this Poem Every Line. Methoden und Verfahren computerunterstuetzter Textanalyse am Beispiel von Lord Byrons Don Juan},
	publisher = {Tübingen: Gunter Narr 1995},
	author = {Rommel, Thomas},
	date = {1995},
	langid = {english},
	note = {Rommel, Thomas: And Trace it in this Poem Every Line. Methoden und Verfahren computerunterstuetzter Textanalyse am Beispiel von Lord Byrons Don Juan. Tübingen: Gunter Narr 1995.},
	keywords = {{AnalyzeQualitatively}, meta\_Theorizing, obj\_Literature},
}

@article{guerrini_analyzing_2011,
	title = {Analyzing Culture with Google Books: Is It Social Science?},
	url = {http://www.miller-mccune.com/media/culturomics-an-idea-whose-time-has-come-34742/},
	journaltitle = {Miller-{McCune} Magazine},
	author = {Guerrini, Anita},
	date = {2011-08-07},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}

@online{prescott_electric_2012,
	title = {An Electric Current of the Imagination: What the Digital Humanities Are and What They Might Become},
	url = {http://journalofdigitalhumanities.org/1-2/an-electric-current-of-the-imagination-by-andrew-prescott/},
	shorttitle = {An Electric Current of the Imagination},
	titleaddon = {Journal of Digital Humanities},
	author = {Prescott, Andrew},
	urldate = {2012-11-25},
	date = {2012-06-26},
	langid = {english},
	keywords = {act\_Conceptualizing, meta\_Assessing, obj\_DigitalHumanities},
}

@article{jessen_aggregated_2012,
	title = {Aggregated trustworthiness: Redefining online credibility through social validation},
	volume = {17},
	url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3731/3132},
	abstract = {This article investigates the impact of social dynamics on online credibility. Empirical studies by Pettingill (2006) and Hargittai, et al. (2010) suggest that social validation and online trustees play increasingly important roles when evaluating credibility online. This dynamic puts pressure on the dominant theory of online credibility presented by Fogg and Tseng (1999). To remedy this problem we present a new theory we call “aggregated trustworthiness” based on social dynamics and online navigational practices.},
	number = {1},
	journaltitle = {First Monday},
	author = {Jessen, Johan and Jørgensen, Anker Helms},
	date = {2012},
	langid = {english},
	keywords = {meta\_Assessing, obj\_ResearchResults},
}

@article{anderson-wilk_achieving_2011,
	title = {Achieving rigor and relevance in online multimedia scholarly publishing},
	volume = {16},
	url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3762/3119},
	abstract = {This paper discusses the importance of relevance and rigor in scholarly publishing in a new media–rich world. We defend that scholarship should be useful and engaging to audiences through the use of new media, and at the same time scholarly publishers must develop and maintain methods of ensuring content accuracy and providing quality controls in the production of scholarly multimedia products. We review examples and a case study of existing scholarly publishing venues that attempt to maintain quality control standards while embracing innovative multimedia formats. We also present lessons learned from the case experience and challenges that face us in the scholarly publication of multimedia.},
	number = {12},
	journaltitle = {First Monday},
	author = {Anderson-Wilk, Mark and Hino, Jeff},
	date = {2011},
	langid = {english},
}

@online{hanson_academic_2008,
	title = {Academic Publishing in the Digital Age},
	url = {http://hastac.org/forums/hastac-scholars-discussions/academic-publishing-digital-age},
	titleaddon = {{HASTAC} Scholars Forum},
	author = {Hanson, Christopher},
	date = {2008},
	langid = {english},
}

@online{carrigan_academic_2013,
	title = {Academic blogging – both/and rather than either/or},
	url = {http://markcarrigan.net/2013/01/10/academic-blogging-bothand-rather-than-eitheror/},
	titleaddon = {mikecarrigan.net},
	author = {Carrigan, Mike},
	date = {2013-01-10},
	langid = {english},
	keywords = {act\_Publishing, meta\_Assessing, obj\_ResearchResults},
}

@online{equipe_vos_2011,
	title = {A vos marques, prêts, bloguez !},
	url = {http://bublog.upmf-grenoble.fr/2011/11/17/bloguez/},
	titleaddon = {Le temps d'un blog},
	author = {{Équipe}},
	urldate = {2011-11-26},
	date = {2011-11-17},
	langid = {french},
}

@video{lieberman_aiden_picture_2011,
	title = {A Picture is Worth 500 Billion Words},
	url = {http://tedxtalks.ted.com/video/TEDxBoston-Erez-Lieberman-Aid-2},
	editora = {Lieberman Aiden, Erez and Michel, Jean-Baptiste},
	editoratype = {collaborator},
	date = {2011-09-20},
	langid = {english},
	keywords = {{AnalyzeStatistically}, obj\_Tools},
}

@article{youmans_new_1991,
	title = {A New Tool for Discourse Analysis: The Vocabulary-Management Profile},
	volume = {67},
	rights = {Copyright © 1991 Linguistic Society of America},
	issn = {00978507},
	url = {http://www.jstor.org/stable/415076},
	abstract = {A computer is used to count the number of new vocabulary words introduced into a text over a moving interval thirty-five words long. The number of new words in each successive interval is plotted at the midpoint of the interval, generating a curve called the Vocabulary-Management Profile ({VMP}). Analysis of {VMPs} for passages from James Joyce and George Orwell illustrates that clearcut peaks and valleys on {VMPs} correlate closely with constituent boundaries and information flow in discourse. {VMPs} for these authors show surprisingly regular alternations between peaks and valleys (that is, between new and repeated vocabulary), reflecting two competing principles that necessarily underlie all normal discourse: innovation and coherence.},
	pages = {pp. 763--789},
	number = {4},
	journaltitle = {Language},
	author = {Youmans, Gilbert},
	date = {1991},
	langid = {english},
	keywords = {{AnalyzeQualitatively}, {AnalyzeStatistically}, t\_DiscourseAnalysis},
}

@online{underwood_brief_2012,
	title = {A brief outburst about numbers.},
	url = {http://tedunderwood.wordpress.com/2012/01/03/a-brief-outburst-about-numbers/},
	abstract = {In responding to Stanley Fish last week, I tried to acknowledge that the "digital humanities," in spite of their name, are not centrally about numbers. The movement is very broad, and at the broade...},
	titleaddon = {The Stone and the Shell},
	author = {Underwood, Ted},
	urldate = {2012-01-05},
	date = {2012-01},
	langid = {english},
}

@online{scheinfeldt_3_2009,
	title = {3 Innovation Killers in Digital Humanities},
	url = {http://www.foundhistory.org/2009/10/16/3-innovation-killers-in-digital-humanities/},
	titleaddon = {Found History},
	author = {Scheinfeldt, Tom},
	urldate = {2009-10-16},
	date = {2009-10-16},
	langid = {english},
	keywords = {meta\_Assessing, obj\_DigitalHumanities},
}

@audio{gulliver_100_2012,
	title = {100 Commandments of Twitter for Academics},
	url = {http://chronicle.com/article/10-Commandments-of-Twitter-for/131813/},
	editora = {Gulliver, Katrina},
	editoratype = {collaborator},
	date = {2012-05-09},
	langid = {english},
}
@book{turkel_programming_2007,
	location = {London, Ontario},
	edition = {Online Edition},
	title = {The Programming Historian},
	url = {http://niche-canada.org/member-projects/programming-historian/ch1.html},
	publisher = {Network in Canadian History \& Environment ({NICHE})},
	author = {Turkel, William J. and {MacEachern}, Alan},
	date = {2007},
	langid = {english},
}

@collection{kansa_escholarship:_2011,
	location = {Los Angeles},
	edition = {E-Book Edition},
	title = {{eScholarship}: Archaeology 2.0: New Approaches to Communication and Collaboration},
	url = {http://escholarship.org/uc/item/1r6137tb},
	series = {Cotsen Digital Archaeology series},
	pagetotal = {296},
	publisher = {Cotsen Institute of Archaeology, {UC} Los Angeles},
	editor = {Kansa, Eric C. and Kansa, Sarah Whitcher and Watrall, Ethan},
	urldate = {2011-08-19},
	date = {2011},
	langid = {english},
	keywords = {meta\_GiveOverview, obj\_Artefacts},
}

@article{feinerer_text_2008,
	title = {Text Mining Infrastructure in R},
	volume = {25},
	url = {http://www.jstatsoft.org/v25/i05},
	abstract = {During the last decade text mining has become a widely used discipline utilizing statistical and machine learning methods. We present the tm package which provides a framework for text mining applications within R. We give a survey on text mining facilities in R and explain how typical application tasks can be carried out using our framework. We present techniques for count-based analysis methods, text clustering, text classification and string kernels.},
	pages = {1--54},
	number = {5},
	journaltitle = {Journal of Statistical Software},
	author = {Feinerer, Ingo and Hornik, Kurt and Meyer, David},
	date = {2008},
	langid = {english},
	keywords = {{AnalyzeStatistically}},
}





