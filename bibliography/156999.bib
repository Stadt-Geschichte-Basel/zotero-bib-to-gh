
@thesis{fabinski_design_2020,
	location = {Darmstadt},
	title = {Design und Konzeption von Suchfunktionalitäten in digitalen historisch-kritischen Editionen},
	url = {https://fbi.h-da.de/fileadmin/Personen/fbi1057/Masterthesis_Fabinski_Matthias.pdf},
	abstract = {Z U S A M M E N {FA} S S U N G
Ziel dieser Masterarbeit ist es, die Anforderungen und Problemstellungen zu
ermitteln, die Anwender bei der Verwendung einer Suchfunktion in digita-
len (historisch-kritischen) Editionen haben, hierfür nutzerzentriert Lösungs-
ansätze zu entwickeln und diese zu evaluieren.
Die vorliegende Arbeit orientiert sich am Double-Diamond Design Pro-
zess (British Design Council 2005). Zur Definition der Problemstellungen
wurde eine Marktforschung in Form einer Konkurrenzanalyse von 42 digi-
talen (historisch-kritischen) Editionen durchgeführt. Grundlage bildet dabei
der Kriterienkatalog für die Besprechung digitaler Editionen von Patrick Sah-
le (2014), welcher um Attribute mit Suchbezug ergänzt wurde. Außerdem
wurde eine Nutzerrecherche mittels einer Onlineumfrage durchgeführt, um
die Bedürfnisse der Benutzer zu identifizieren. Basierend auf den konsoli-
dierten Anforderungen wurden konkrete Problemstellungen abgeleitet.
Anschließend wurden für diese identifizierten Problemstellungen mittels
Sketching und Protoyping Lösungsansätze entwickelt. Am Beispiel der Brief-
edition Wedekind wurden diese in einem High-Fidelity-Prototypen als Web-
Applikation umgesetzt und unter anderem mit Guerilla Testing und einem
Remote-Usability-Test iterativ weiterentwickelt und evaluiert.
Ergebnisse der Arbeit sind sowohl die identifizierten Problemstellungen
als auch die dafür entwickelten Lösungsansätze, die in einem performan-
ten High-Fidelity-Prototypen umgesetzt und evaluiert wurden. Die Kombi-
nation der unterschiedlichen Problemstellungen zu einem funktionierenden
Prototyp stellte eine der zentralen Herausforderungen dieser Arbeit dar. Es
wurde eine konfigurierbare Volltextsuche mit Entitäts- und Metadatenfiltern
und einer benutzerfreundlichen Ergebnisdarstellung implementiert. Die Ar-
chitektur des Prototyps erlaubt die schnelle Auslieferung neuer Lösungsan-
sätze und steht unter Open-Source Lizenz zur weiteren Verwendung zur
Verfügung.},
	pagetotal = {149},
	institution = {Hochschule Darmstadt},
	type = {phdthesis},
	author = {Fabinski, Matthias},
	date = {2020-01-12},
}

@book{weber_mehrdeutige_1974,
	location = {Tübingen},
	title = {Mehrdeutige Wortformen im heutigen Deutsch : Studien zu ihrer grammatischen Beschreibung und lexikographischen Erfassung},
	isbn = {3-484-10216-0},
	series = {Linguistische Arbeiten  :  {LA}, {ISSN} 0344-6727},
	publisher = {Niemeyer},
	author = {Weber, Heinz Josef},
	date = {1974},
}

@thesis{saez-godoy_lexico_1967,
	location = {Bonn},
	title = {El lexico de Lope de Rueda: Clasificaciones conceptual y estadistica},
	url = {http://d-nb.info/482182822},
	pagetotal = {415},
	institution = {Univ. Bonn},
	type = {phdthesis},
	author = {Saéz-Godoy, Leopoldo},
	date = {1967},
	note = {https://twitter.com/espejolento/status/1314701155436355584},
}

@thesis{thomas_verbesserung_2012,
	location = {Leipzig},
	title = {Verbesserung einer Erkennungs- und Normalisierungsmaschine für natürlichsprachige Zeitausdrücke},
	url = {urn:nbn:de:bsz:15-qucosa2-172396},
	abstract = {Digital gespeicherte Daten erfreuen sich einer stetig steigenden Verwendung. Insbesondere die computerbasierte Kommunikation über E-Mail, {SMS}, Messenger usw. hat klassische Kommunikationsmittel nahezu vollständig verdrängt. Einen Mehrwert aus diesen Daten zu generieren, ist sowohl im geschäftlichen als auch im privaten Bereich von entscheidender Bedeutung. Eine Möglichkeit den Nutzer zu unterstützen ist es, seine textuellen Daten umfassend zu analysieren und bestimmte Elemente hervorzuheben und ihm die Erstellung von Einträgen für Kalender, Adressbuch und dergleichen abzunehmen bzw. zumindest vorzubereiten. Eine weitere Möglichkeit stellt die semantische Suche in den Daten des Nutzers dar. Selbst mit Volltextsuche muss man bisher den genauen Wortlaut kennen, wenn man eine bestimmte Information sucht. Durch ein tiefgreifendes Verständnis für Zeit ist es nun aber möglich, über einen Zeitstrahl alle mit einem bestimmten Zeitpunkt oder einer Zeitspanne verknüpften Daten zu finden. Es existieren bereits viele Ansätze um Named Entity Recognition voll- bzw. semi-automatisch durchzuführen, aber insbesondere Verfahren, welche weitgehend sprachunabhängig arbeiten und sich somit leicht auf viele Sprachen skalieren lassen, sind kaum publiziert. Um ein solches Verfahren für natürlichsprachige Zeitausdrücke zu verbessern, werden in dieser Arbeit, basierend auf umfangreichen Analysen, Möglichkeiten vorgestellt. Es wird speziell eine Strategie entwickelt, die auf einem Verfahren des maschinellen Lernens beruht und so den manuellen Aufwand für die Unterstützung neuer Sprachen reduziert. Diese und weitere Strategien wurden implementiert und in die bestehende Architektur der Zeiterkennungsmaschine der {ExB}-Gruppe integriert.},
	institution = {Universität Leipzig},
	type = {phdthesis},
	author = {Thomas, Stefan},
	date = {2012-01-12},
	langid = {german},
}

@thesis{vorbach_analysen_2015,
	location = {Würzburg},
	title = {Analysen und Heuristiken zur Verbesserung von {OCR}-Ergebnissen bei Frakturtexten},
	url = {https://opus.bibliothek.uni-wuerzburg.de/opus4-wuerzburg/frontdoor/deliver/index/docId/10652/file/Vorbach_Paul_Fraktur-OCR.pdf},
	abstract = {Zahlreiche Digitalisierungsprojekte machen das Wissen vergangener Jahrhunderte jederzeit verfügbar. Das volle Potenzial der Digitalisierung von Dokumenten entfaltet sich jedoch erst, wenn diese als durchsuchbare Volltexte verfügbar gemacht werden. Mithilfe von {OCR}-Software kann die Erfassung weitestgehend automatisiert werden. Fraktur war ab dem 16. Jahrhundert bis zur Mitte des 20. Jahrhunderts die verbreitete Schrift des deutschen Sprachraums. Durch einige Besonderheiten von Fraktur bleiben die Erkennungsraten bei Frakturtexten aber meist deutlich hinter den Erkennungsergebnissen bei Antiquatexten zurück.
Diese Arbeit konzentriert sich auf die Verbesserung der Erkennungsergebnisse der {OCR}-Software Tesseract bei Frakturtexten. Dazu wurden die Software und bestehende Sprachpakete gesondert auf die Eigenschaften von Fraktur hin analysiert. Durch spezielles Training und Anpassungen an der Software wurde anschließend versucht, die Ergebnisse zu verbessern und Erkenntnisse über die Effektivität verschiedener Ansätze zu gewinnen.
Die Zeichenfehlerraten konnten durch verschiedene Experimente von zuvor 2,5 Prozent auf 1,85 Prozent gesenkt werden. Außerdem werden Werkzeuge vorgestellt, die das Training neuer Schriftarten für Tesseract erleichtern und eine Evaluation der erzielten Verbesserungen ermöglichen.},
	pagetotal = {68},
	institution = {Julius-Maximilians-Universität Würzburg},
	type = {Masterthesis},
	author = {Vorbach, Paul},
	date = {2015-09-17},
	langid = {german},
	keywords = {{FineReader}, {OCR}, Tesseract, Tool},
}

@thesis{wuttke_infrastrukturelle_2019,
	location = {Köln},
	title = {Infrastrukturelle Erfolgsfaktoren für einen Digital Humanities-Schwerpunkt an deutschen Universitäten},
	abstract = {Die zunehmende digitale Transformation der Geisteswissenschaften und die Ausdifferenzierung der sogenannten Digital Humanities ({DH}) haben in breiteren Kreisen der Geisteswissenschaften einen Einfluss auf das Selbstverständnis, den theoretischen Rahmen und die angewandten Methoden. Um das Innovations- und Transformationspotenzial der Digital Humanities für die universitäre Forschung und Lehre in ihrer Gesamtheit fruchtbar zu machen, müssen dafür an universitären Standorten geeignete Rahmenbedingungen geschaffen werden, die sowohl »genuinen« {DH}-Forschungsaktivitäten als auch der breiteren Digitalisierung geisteswissenschaftlicher Forschungsprozesse dienlich sind.

Nach einem Überblick über die wichtigsten Aspekte der digitalen Transformation der geisteswissenschaftlichen Forschung geht diese Arbeit der Frage nach, welche infrastrukturellen Rahmenbedingungen sich an deutschen universitären Standorten aus der Sicht der Forschenden als besonders Erfolg versprechend erwiesen haben. Auf dieser Grundlage werden infrastrukturelle Erfolgsfaktoren und Handlungsempfehlungen erarbeitet, die als Grundlage und Anregung für den Auf- und Ausbau von {DH}-Schwerpunkten an deutschen Universitäten dienen können und gegebenenfalls auf andere institutionelle Kontexte übertragbar sind.
    
Thee ongoing digital transformation of the Humanities and the emergence of the so-called Digital Humanities ({DH}) are having an impact on the self-image, the theoretical framework and the methods applied in broader areas of the Humanities. In order to fully harness the innovation and transformation potential of the Digital Humanities for university-level research and teaching activities, suitable framework conditions must be created at campuses that serve both »genuine« {DH} research activities and the broader digitisation of research processes in the Humanities.

After an overview of the most important aspects of the digital transformation of the Humanities, the thesis addresses the question which infrastructural framework conditions at German campuses have proven to be particularly advantageous from the researchersʼ point of view. Based on this, several infrastructural success factors and recommendations are formulated, which can serve as a starting point and inspiration for the establishment and expansion of {DH} focus points at German universities which can also be adapted to other institutional contexts.},
	institution = {{TH} Köln},
	type = {phdthesis},
	author = {Wuttke, Ulrike},
	date = {2019},
	langid = {german},
}

@thesis{meyer_datenmodellierung_2019,
	location = {Berlin},
	title = {Datenmodellierung für digitale Editionen},
	url = {https://edoc.hu-berlin.de/bitstream/handle/18452/21564/master_meyer_till.pdf?sequence=3&isAllowed=y},
	abstract = {Im Zentrum dieser Masterarbeit steht die Frage, welchen Mehrwert Semantic-Web-Technologien für digitale Editionen bieten können. Um diese Frage zu beantworten, wird in einem ersten Schritt die Verbreitung von Semantic-Web-Technologien im Kulturerbebereich dargestellt. In den folgenden beiden Teilen (Kapitel 3 \& 4) werden die Potenziale von Semantic-Web-Technologien für digitale Editionen mithilfe einer für diese Arbeit entwickelten Matrix genauer in den Blick genommen. In Kapitel 3 steht dabei die Erörterung der theoretischen Potenziale in der einschlägigen Fachliteratur im Vordergrund, während in Kapitel 4 52 Datenmodelle digitaler Editionen daraufhin untersucht werden, ob und wie Semantic-Web-Technologien hier bereits praktisch angewandt werden.
 
This master’s thesis focusses on the question, which added values semantic web technologies can offer for digital scholarly editions. To answer this question, the dissemination of semantic web technologies in the sector of cultural heritage will be depicted in a first step. Based on a matrix, that was developed for this thesis, the potentials of semantic web technologies for digital editions will be explored in the following two parts (chapters 3 \& 4). In chapter 3 the emphasis lies on discussing the theoretical potentials in relevant literature, whereas chapter 4 reviews data models of 52 digital scholarly editions in order to ascertain, if and how semantic web technologies are being used in practice.},
	institution = {Humbold University},
	type = {phdthesis},
	author = {Meyer, Till},
	date = {2019},
}

@thesis{kellendonk_bedarfsorientierte_2018,
	title = {Bedarfsorientierte Flexibilisierung der virtuellen Forschungsumgebung {FuD} für die digitale Editionswissenschaft},
	abstract = {Im Rahmen dieser Masterarbeit wurde eine Umfrage zur Nutzung der virtuellen Forschungsumgebung {FuD} durchgeführt, um Bedarfe der Nutzer zu eruieren. Anhand der Ergebnisse wurde entschieden, einen Dokumenttypeditor für {FuD} zu konzipieren und diesen soweit wie möglich zu implementieren. Mit dem Dokumenttypeditor sollen neue Dokumenttypen erstellt und vorhandene Dokumenttypen bearbeitet werden können, um die Arbeit der {FuD}-Mitarbeiter zu erleichtern und Nutzern die Möglichkeit zu geben, selbst Änderungen an ihren Dokumenttypen vorzunehmen. Dokumenttypen bilden in {FuD} Datenmodelle ab und dienen in erster Linie zur Eingabe und Speicherung von Metadaten.
Der Dokumenttypeditor wurde als Modul innerhalb von {FuD} in der Programmiersprache tcl/tk implementiert und von Mitarbeitern des {TCDH} und des Servicezentrum {eSciences} getestet.},
	institution = {Trier},
	type = {phdthesis},
	author = {Kellendonk, Stefan},
	date = {2018},
	langid = {german},
}

@thesis{luschow_vom_2018,
	location = {Trier},
	title = {Vom verlegten Buch zu verlinkten Daten - Automatische Extraktion und semantische Modellierung der Einträge einer Bibliographie französischsprachiger Romane},
	abstract = {In der 1977 erschienenen Bibliographie du genre romanesque français 1751-1800 haben die Herausgeber sämtliche bekannten französischsprachigen Romane der zweiten Hälfte des 18. Jahrhunderts mit ihren wesentlichen Metadaten erfasst. Insbesondere die zusätzliche Angabe inhaltlicher Charakteristika - wie z.B. zum Schauplatz, zu den Figuren oder der Erzählperspektive des jeweiligen Romans - macht diese Bibliographie zu einer umfangreichen und bedeutsamen Quelle romanistischer Forschung. In dieser Masterarbeit werden die einzelnen Einträge durch automatische Verfahren erschlossen und die Metadaten daraufhin als Linked Data semantisch modelliert, um sie an das Netzwerk global vorhandener semantischer Daten anbinden zu können. Diese neue Repräsentation der Bibliographie ermöglicht darüber hinaus die einfache Suche in den Daten und die individuelle Zusammenstellung einzelner Einträge sowie die Beantwortung weitergehender Fragestellungen.},
	institution = {Trier},
	type = {phdthesis},
	author = {Lüschow, Andreas},
	date = {2018},
	langid = {german},
	note = {Siehe auch: https://doi.org/10.5281/zenodo.3401428 (Datensatz)},
}

@thesis{hartmann_automatische_2018,
	location = {Trier},
	title = {Automatische Klassifikation semistrukturierter Texte mit neuronalen Netzen},
	abstract = {In dieser Masterarbeit wird der Frage nachgegangen, ob neuronale Netze lernen können, deutsche Blogbeiträge der geistes- und sozialwissenschaftlichen Blogplattform Hypotheses.org in die von den Wissenschaftlern angegebenen Themen und Disziplinen zu klassifizieren und diese Kategorien für neue Blogbeiträge vorherzusagen. Umgesetzt wird dies mit der in diesem Bereich beliebten Programmiersprache Python und den hochentwickelten Softwarebibliotheken Gensim, Keras und {TensorFlow}.},
	institution = {Trier},
	type = {phdthesis},
	author = {Hartmann, Maria},
	date = {2018},
	langid = {german},
}

@thesis{radlinger_statistische_2019,
	location = {Trier},
	title = {Statistische Netzwerkanalyse von Figuren und Ensembles aus der Operngeschichte},
	abstract = {Die Extraktion von Netzwerken aus literarischen Texten stellt ein quantitatives Verfahren dar, das dem Bereich des distant reading zuzuordnen ist. Der Forschungsgegenstand der Masterarbeit besteht im Vergleich von Figuren- und Ensemblenetzwerken, die aus Opernlibretti generiert werden. Das Ziel ist es, den Wandel der dramatischen Struktur in der Operngeschichte einerseits und die Entwicklung des Ensemblegesangs andererseits durch Methoden der sozialen Netzwerkanalyse aufzuzeigen. Als Datengrundlage dient ein 200 Libretti zählendes Textkorpus, das von 1673-1942 bedeutende Werke der Operngeschichte umfasst. Die in italienisch, französisch oder deutsch vorliegenden Texte werden mit Python automatisiert in {TEI}-Dateien umgewandelt, aus denen die Datenstruktur der Netzwerke extrahiert wird. Mit der Programmiersprache R werden die Netzwerke modelliert und verschiedene Metriken berechnet, anhand derer eine auf  musikwissenschaftliche Thesen gestützte Netzwerkanalyse durchgeführt wird.},
	institution = {Trier},
	type = {phdthesis},
	author = {Radlinger, David},
	date = {2019},
	langid = {german},
}

@thesis{kalb_automatische_2019,
	location = {Trier},
	title = {Automatische Klassifikation von Filmgenres auf Grundlage von Topic Models und syntaktischer Komplexität},
	institution = {Trier},
	type = {phdthesis},
	author = {Kalb, Florian},
	date = {2019},
	langid = {german},
}

@thesis{feineis_wortgenaue_2008,
	location = {Würzburg},
	title = {Wortgenaue Annotation digitalisierter mittelalterlicher Handschriften},
	url = {https://opus.bibliothek.uni-wuerzburg.de/opus4-wuerzburg/frontdoor/deliver/index/docId/2613/file/Diplomarb_Feineis.pdf},
	pagetotal = {71},
	institution = {Bayerische Julius-Maximilians-Universität Würzburg},
	type = {Diplomarbeit},
	author = {Feineis, Markus},
	date = {2008-04-15},
	langid = {german},
	keywords = {{HCR}, Layoutanalyse},
}

@thesis{hohn_mustererkennung_2016,
	location = {Würzburg},
	title = {Mustererkennung in Frühdrucken},
	url = {https://opus.bibliothek.uni-wuerzburg.de/opus4-wuerzburg/frontdoor/deliver/index/docId/2604/file/Diplomarb_Hoehn.pdf},
	pagetotal = {56},
	institution = {Bayerische Julius-Maximilians Universität Würzburg},
	type = {Diplomarbeit},
	author = {Höhn, Winfried},
	date = {2016-09-14},
	langid = {german},
	keywords = {{FineReader}, Inkunabel, {OCR}},
}

@thesis{sirajzade_luxemburgischsprachige_2015,
	location = {Trier},
	title = {Das luxemburgischsprachige Oeuvre von Michel Rodange (1827-1876). Editionsphilologische und korpuslinguistische Analyse},
	url = {http://ubt.opus.hbz-nrw.de/volltexte/2015/914/},
	abstract = {Die Arbeit setzt sich mit den theoretischen und praktischen Aspekten der Analyse und Edition eines literarisch wichtigen Textes mit Methoden der Digital Humanities sowie der Korpus- und Computerlinguistik auseinander. Als Materialgrundlage dient das luxemburgischsprachige Werk des Michel Rodange. Hierzu gehören die Werke ‚Renert oder de Fuuss am Frack an a Maansgréisst‘ - ca. 35.000 Tokens, ‚Dem Léiweckerche säi Lidd‘ – ca. 5.000 Tokens, ‚Dem Grof Sigfrid seng Goldkuemer‘ – ca. 10.000 Tokens und zwei Gedichte – ca. 500 Tokens. Auf der empirischen Seite handelt es sich um die Erstellung eines elektronischen Korpus mit historisch-kritischen und linguistischen Annotationen und dessen Darstellung im Internet als Webportal. Dabei entsteht eine Wechselwirkung zwischen Theorie und Praxis, so werden die erstellten Annotationen verwendet, um das Werk aus sprach- und literaturwissenschaftlicher Perspektive zu untersuchen; diese Erkenntnisse können dann wiederum bei der Implementierung der Tools eingesetzt werden, um den Korrektheitsgrad der automatischen Annotation zu erhöhen. Die historisch-kritischen Annotationen beinhalten beispielsweise Lesarten, Korrekturen sowie Worterklärungen, wohingegen die linguistischen Annotationen die Orthographie, Morphologie (Wortklassen und Lemmata) und Phraseologie betreffen. Die Annotationen werden in der Markup-Sprache {XML} kodiert. Der erste Schritt der Erstellung eines elektronischen Korpus ist die Digitalisierung der Texte. Bei den Handschriften geschah dies mithilfe einer manuellen Transkription, bei den Drucken wurde auf eine {OCR}-Software zurückgegriffen. Es empfiehlt sich, bereits in dieser Phase den Text gut zu strukturieren und mit Annotationen zu versehen. Dabei wurden zunächst Metadaten festgehalten. Anschließend wurden Informationen wie Seitenzahl, Zeilenumbrüche etc. als Annotationen hinzugefügt. Von besonderer Bedeutung für die Erstellung eines Korpus aus einem historisch und literarisch wichtigen Text ist jedoch seine Anreicherung mit historisch-kritischen Kommentaren. Die Untersuchung und Berücksichtigung der literarischen bzw. wissenschaftlichen Gattung historisch-kritische Edition stellt die theoretische Grundlage für solche Annotationen dar. Alle für die Editionswissenschaft relevanten Texthinweise, -bruchstücke und vom Autor durchgestrichene und gelöschte Stellen wurden dokumentiert. Bei schlecht lesbaren Stellen wurden Lesemöglichkeiten vorgeschlagen und die anderer Editionen diskutiert. Die Text Incoding Initiative ({TEI}) bietet eine Fülle von {XML}-Elementen, um solche Annotationen zu speichern. Um diese Arbeit nicht manuell ausführen zu müssen, wurde auf Tools wie {TUSTEP}, {oXygen} oder Skriptsprachen wie beispielsweise Perl zurückgegriffen. Diese können u. a. die Such- und Ersetzen-Arbeiten mithilfe der Regulären Ausdrücke bedeutend erleichtern. Den nächsten Schritt der Korpus-Erstellung stellt die Tokenisierung dar. Hierbei gehen die historisch-kritischen Annotationen in linguistische Annotationen über. Die Grenzen eines Wortes werden festgelegt und jedes Wort mit seinem eigenen Element versehen. Aus der digitalen Verarbeitung nicht wegzudenken ist dabei die Berücksichtigung und Untersuchung der Sprache des Autors. In diesem Fall wurde auf Aspekte wie die Dichtungsstile des Luxemburgischen im 19. Jahrhundert, die literarischen Gattungen der Texte sowie die Schreibung des Autors geachtet. Der empirische Anteil der Analyse mit {EDV}-technischen Methoden und die Speicherung der Ergebnisse als Annotationen stellt die wissenschaftliche Basis für die spätere digitale Präsentation dar. In der Arbeit werden die Ergebnisse der quantitativen und qualitativen Analyse der Sprache des Werks mithilfe von selbstimplementierten Programmen diskutiert. Dabei werden die vorhandenen Theorien sowohl der klassischen Linguistik z. B. aus der Morphologie oder der Phraseologieforschung, als auch der Korpuslinguistik besprochen und evaluiert. Die Implementierung und Ergebnisse folgender Programme für das Michel Rodange Korpus werden thematisiert: Tokeniser, {FreqList}, {POS}-Trainer, {POS}-Tagger, Lemmatisierer und Programme zur morphologischen und phraseologischen Analyse des Korpus. Der {POS}-Tagger kann die Wortarten im Korpus bestimmen. Grundlage dafür sind die sogenannten Hidden Markov Modelle, die auf der Wahrscheinlichkeitstheorie basieren. Der Lemmatisierer und das Programm zur morphologischen Analyse arbeiten hauptsächlich regelbasiert, wohingegen das Programm für die phraseologische Analyse anhand statistischer Verfahren wie dem Z-Test, dem Chi-Quadrat-Test und dem Exakten Test von Fisher implementiert wurde. So widmet sich beispielsweise Kapitel 3.4 dem Output der morphologischen Analyse und diskutiert die Wortbildung. Kapitel 3.6 beschäftigt sich mit der Interpretation der Phraseologismen. Hierbei zeigte sich, dass viele der automatisch identifizierten Phraseologismen aus Michel Rodanges Werken in der Tat ein fester Bestandteil nicht nur der luxemburgischen Sprache und Kultur sind, sondern sich auch in der gesamten westlichen Kultur wiederfinden.},
	pagetotal = {319},
	institution = {Universität Trier},
	type = {Dissertation},
	author = {Sirajzade, Joshgun},
	date = {2015-04-03},
	langid = {german},
	keywords = {Digitale Edition, Editionswissenschaften},
}

@thesis{wilhelm_werkzeug_2015,
	location = {Regensburg},
	title = {Ein Werkzeug zur datengetriebenen Visualisierung von Shakespeare-Dramen. Konzeption und Implementierung von To See or Not to See},
	url = {https://epub.uni-regensburg.de/32474/7/_final_Masterarbeit.pdf},
	abstract = {In dieser Arbeit stelle ich das webbasierte, interaktive Textanalysewerkzeug „To See or Not to See“ zur quantitativen Dramenanalyse vor. Damit werden vierzehn Shakespeare-Dramen auf nicht-lineare Weise visualisiert und dem Nutzer datengetriebene Auswertungen angezeigt. Die Datengrundlage bilden die digital annotierten Texte der „Folger Digital Library“ in einem {TEI}-konformen Format. Diese statische Repräsentation der Dramen wird mithilfe von {XSLT} automatisch in ein {HTML}-Dokument transformiert. Auf diesem Weg wird eine neue Darstellungsform eines anderweitig linearen Texts erzeugt. Durch Anwendung weiterer Webtechniken wie {JavaScript} wird aus dieser neuen Darstellungsform ein interaktives Werkzeug. Mit diesem können bestimmte Aspekte der Texte genauer betrachtet werden. Dazu gehören dessen Abschnitte (Akte und Szenen), Figuren und Bühnenanweisungen. Der Fokus liegt dabei auf quantitativen Aspekten, weshalb Graphen und Diagramme benutzt werden, um diese darzustellen. Die Fokussierung auf quantitative Aspekte von Dramen ist im „Distant Reading“ begründet. Diese Methode nutzt Daten als Mittel zum Erkenntnisgewinn, wobei dieser nicht auf Einzeltexte sondern größere Textgruppen abzielt. Die Erstellung von Werkzeugen zählt in den Digital Humanities seit jeher zu den Kernaktivitäten. Herausforderungen, Probleme und Lösungsansätze, die im Zusammenhang mit der Erstellung einer Software wie „To See or Not to See“ zum „Distant Reading“ auftreten, werden hier diskutiert.},
	pagetotal = {83},
	institution = {Institut für Information und Medien, Sprache und Kultur; Universität Regensburg,},
	type = {Masterthesis},
	author = {Wilhelm, Thomas},
	date = {2015-09-04},
	langid = {german},
	note = {http://www.thomaswilhelm.eu/},
	keywords = {Distant Reading, Tool},
}

@thesis{richts_frbr_2013,
	location = {Köln},
	title = {Die {FRBR} customization im Datenformat der Music Encoding Initiative ({MEI})},
	rights = {{CC} {BY}-{SA}},
	url = {https://publiscologne.th-koeln.de/frontdoor/index/index/docId/144},
	abstract = {Vor dem Hintergrund des digitalen Wandels und der Entwicklung virtueller Forschungsumgebungen wird eine stärkere Kooperation von Bibliotheken und Forschungsinstitutionen künftig unabdingbar sein. Die zunehmende Internationalisierung in Bereichen der Datenaufbereitung stellt wachsende Anforderungen an Bibliotheken wie auch Forschungsinstitutionen. Die vorliegende Arbeit thematisiert die Implementierung des Modells der Functional Requirements for Bibliographic Records ({FRBR})im Datenformat der Music Encoding Initiative ({MEI}), welches sich in den letzten Jahren als Standard zur Codierung von Musiknotation etabliert hat und sehr detaillierte Möglichkeiten auch zur Erfassung von Metadaten bietet. Ziel dieser Kombination ist es ausdrücklich, größtmögliche Kompatibilität zwischen den in musikwissenschaftlichen Projekten erarbeiteten, {MEI}-basierten Forschungsdaten und bibliothekarischen Erschließungstechniken herzustellen. Die Vorteile einer solchen Erweiterung erscheinen gerade vor dem Hintergrund der bevorstehenden Umstellung auf {RDA} als überaus lohnenswert.},
	pagetotal = {81},
	institution = {Fakultät für Informations- und Kommunikationswissenschaften Fachhochschule Köln},
	type = {Masterthesis},
	author = {Richts, Kristina},
	date = {2013-04-13},
	langid = {german},
	keywords = {{FRBR}, {MEI}},
}

@thesis{hoserle_geschichten_2017,
	location = {Graz},
	title = {Geschichte(n) wahren - Zukunft gestalten. Digitale Erhaltungsstrategien der Langzeitarchivierung am Beispiel des Hans-Gross-Kriminalmuseums},
	url = {http://resolver.obvsg.at/urn:nbn:at:at-ubg:1-119947},
	abstract = {Der Informations- und Datenverlust vergangener Generationen ist erwiesene Realität. Dies führt dazu, dass Wissensbestände aus der Vergangenheit im Wandel der Zeit spurlos verschwinden. Die Herausforderung besteht demnach darin, Daten auf lange Sicht vollständig zu erhalten und jegliche Änderungen zu dokumentieren. Diese Masterarbeit beschäftigt sich mit der Frage, wie die fortschreitende Digitalisierung genützt werden kann, um Informationen aus Daten zukünftigen Generationen zur Verfügung zu stellen.Wesentlich dabei ist, dass neben der Wahl des Speichermediums auch eine Archivierungsstrategie festgelegt wird. Dahingehend ist eine Analyse von bereits existierenden Lösungen valider Langzeitarchivierungsstrategien notwendig, wozu die Zertifizierung der Datenspeicher zählt. Diese „Trusted Repositories“ müssen flexibel genug sein, um zukünftige Erneuerungsprozesse konsequent umzusetzen zu können, da sich jedes digitale Objekt im Verlauf seiner Existenz verändert.Ein Metadatenstandard, welcher die Dokumentation der Lebenszyklen von Daten ermöglicht, ist {PREMIS}. Die laufende Weiterentwicklung ermöglicht den physischen Erhalt von Objekten auf Datenebene, welcher auch als Bitstream Preservation bezeichnet wird.Der technologische Fortschritt führt zu einer Diversität an Möglichkeiten. Damit gehen Herausforderungen einher, welche oftmals die Handlungsspielräume einzelner Institutionen übersteigen. Forschungsinfrastrukturen wie {DARIAH} oder {CLARIN} bieten Plattformen, anhand derer technische Lösungsansätze analysiert werden. Die notwendige Standardisierung von Daten und der Einsatz von Auszeichnungssprachen stellen sicher, dass Informationen über Webservices transportiert werden können.Eine praxisnahe Umsetzung der Thematik, wurde im Hans-Gross-Kriminalmuseums der Universität Graz durchgeführt. Die Transformation analoger Karteikarten zu digitalen Objekten wird im finalen Teil dieser Arbeit beschrieben und dokumentiert.},
	pagetotal = {165},
	institution = {Karl-Franzens-Universität Graz; Austrian Centre for Digital Humanities},
	type = {Masterthesis},
	author = {Höserle, Christian},
	date = {2017},
	langid = {german},
	keywords = {Langzeitarchivierung, Speichermedien, Trusted Repositories},
}

@thesis{steiner_cultural_2016,
	location = {Graz},
	title = {Cultural heritage and the semantic web : opportunities and practical feasibility exemplified by the project "Virtual Museum of the University of Graz"},
	url = {http://resolver.obvsg.at/urn:nbn:at:at-ubg:1-112520},
	abstract = {Die vorliegende Arbeit zeigt die vielfältigen Möglichkeiten des Einsatzes von Semantic Web Technologien im Bereich von Kulturerbeprojekten auf. Der Fokus wurde dabei auf digitalen Editionen gesetzt. Das Projekt „Virtuelles Museum der Universität Graz“ dient hierbei als ein Beispiel für die Nutzung von Semantic Web Technologien in einem Editionsprojekt mit dem Ziel der digitalen Langzeitarchivierung. Es konnte gezeigt werden, dass {GAMS} (Geisteswissenschaftliches Asset Management System) als zugrundeliegende Infrastruktur am Zentrum für Informationsmodellierung an der Universität Graz diesem Anspruch folgen kann. Die Veröffentlichung der „Content Layers“ also jeglicher inhaltlicher Informationen von Kulturerbeprojekten wie diesem kann mit Hilfe von Transformationen gesteuert werden. Das bedeutet, der Kern der Edition bleibt immer bestehen und kann dennoch etwa nach {RDF}, der „Sprache“ des Semantic Web übersetzt werden. Dieses Prinzip der Modularität wird auch von {GAMS} hochgehalten. Somit können in der Infrastruktur selbst verschiedene Module gegen andere ausgetauscht werden, ohne das Kernsystem verändern zu müssen und selbiges gilt für die in {GAMS} verfügbaren Daten. Diese können menschenlesbar in Form eines {PDFs} oder einer Webseite aufbereitet werden oder aber maschinenlesbar in Form von {RDF} oder anderen maschinenlesbaren Formaten. An dieser Stelle beginnt sich das Potential von hochstrukturierten Daten zu entfalten. Digitale Editionen könnten von einer Veröffentlichung ihrer Daten in {RDF} sowie der Nutzung von kontrollierten Vokabularien oder sogar Ontologien stark profitieren. Während in anderen Bereichen wie etwa der Medizin oder Regierungsdaten Semantic Web Technologien wie {RDF} Vokabularien bereits weit verbreitet sind, würde der Einsatz letzterer bei digitalen Editionen in Kulturerbeprojekten einen großen Schritt vorwärts bedeuten.},
	pagetotal = {146},
	institution = {Karl-Franzens-Universität Graz, Austrian Centre for Digital Humanities},
	type = {Masterthesis},
	author = {Steiner, Christian},
	date = {2016},
	langid = {english},
	keywords = {Digitale Edition, Semantic Web},
}

@thesis{petra_wissenschaftliche_2015,
	location = {Köln},
	title = {Wissenschaftliche Bibliotheken als Kooperationspartner der Digital Humanities : Faktoren einer aktiven Unterstützung im Bereich der Metadaten},
	rights = {{CC} {BY}-{SA}},
	url = {https://publiscologne.th-koeln.de/frontdoor/index/index/docId/787},
	abstract = {Wissenschaftliche Bibliotheken haben traditionell die Aufgabe, die Wissenschaft hinsichtlich der Informationsversorgung und -beschaffung zu unterstützen. Durch die digitalen Entwicklungen und der Ausdifferenzierung der sogenannten Digital Humanities ({DH}) hat sich das Verständnis dessen, was Information ist, gewandelt: Das Arbeiten mit digitalen Daten in der Wissenschaft gehört heute zum Alltag. Hierdurch sind Bibliotheken gefordert, ihr Selbstverständnis und das Aufgabenprofil anzupassen. In der bibliothekarischen Fachwelt werden seit längerem genau dieses Selbstverständnis sowie das eigene Berufsbild stark und vor allem kontrovers diskutiert. An einem praktischen Beispiel wird gezeigt, wie eine Bibliothek ihre Kompetenz im Bereich der Metadaten als Kooperationspartner in einem {DH}-Projekt gezielt einbringen kann. Aufbauend auf dieser Ausgangsbasis wird die vielschichtige Diskussion um die Rollenverteilung in der sich verändernden Informationsinfrastruktur aufgegriffen und hierdurch werden Faktoren erarbeitet, die als Grundlage für die praktische Unterstützung der {DH} durch wissenschaftliche Bibliotheken gesehen werden.},
	pagetotal = {79},
	institution = {Institut für Informationswissenschaft der Fachhochschule Köln},
	type = {Master Thesis},
	author = {Petra, Maier},
	date = {2015-09-16},
	langid = {german},
	keywords = {Forschungsdatenmanagement, Metadaten},
}

@thesis{chunze_korpus-basierte_2013,
	location = {Gießen},
	title = {Korpus-basierte effiziente Informationsextraktion und Grammatikinduktion der natürlichen Sprachen},
	rights = {http://geb.uni-giessen.de/geb/doku/lic\_ohne\_pod.php?la=de},
	url = {http://geb.uni-giessen.de/geb/volltexte/2013/9925/pdf/ShenChunze_2013_07_11.pdf},
	abstract = {Grammatik-Induktion gewinnt aufgrund ihrer vielseitigen Anwendungen seit einigen Jahren immer mehr Aufmerksamkeit. In der Computerlinguistik ist die automatische Grammatik-Induktion heute ein hochinteressantes Forschungsthema geworden. In der vorliegenden Arbeit wurde ein effizienter lernbasierter Ansatz ({EDSI}) entwickelt, kontextfreie Grammatiken automatisch zu extrahieren und darüber hinaus lokale Grammatiken zu induzieren. Der {EDSI}-Ansatz (Effiziente Distillation syntaktischer Informationen) wurde auf englischen und deutschen Korpora evaluiert und mit den klassischen Ansatz {ABL} verglichen. Die Verarbeitungseffizienz wird wesentlich erhöht, während die Extraktionsresultate auch verbessert werden. Am Ende werden lokale Grammatiken beispielsweise angewendet, um spezielle Informationen zu extrahieren.},
	pagetotal = {275},
	institution = {Justus-Liebig-Universität Gießen},
	type = {Dissertation},
	author = {Chunze, Shen},
	date = {2013-07-22},
	langid = {german},
	keywords = {Computerlinguistik, Korpus {\textless}Linguistik{\textgreater}},
}

@thesis{hornschemeyer_textgenetische_2017,
	location = {Köln},
	title = {Textgenetische Prozesse in Digitalen Editionen},
	url = {http://kups.ub.uni-koeln.de/7544/},
	abstract = {Begünstigt durch den digitalen Medienwandel entstehen zur Zeit neue Editionsformen, deren Nutzen sich vor allem aus der Überwindung medienbedingter Limitierungen gedruckter Editionen ergibt. Der Wegfall dieser einschränkenden Faktoren, wie ein fest vorgegebenes Seitenformat oder die Begrenzung auf eine bestimmte Seitenanzahl und die daraus resultierenden Möglichkeiten, im Prinzip unbegrenzte Mengen an Transkriptionen, Abbildungen und Kontextmaterialien in die Digitale Edition integrieren zu können, sind ein wesentliches Merkmal dieser neueren Ansätze. Trotz dieser Entwicklungen fehlt es im Bereich Digitaler Textgenetischer Editionen bis heute aber weiterhin an geeigneten Methoden und Werkzeugen, um den Entstehungsprozess eines Textes intuitiv nachvollziehen und in geeigneter Weise visualisieren zu können. Besonders zwei Aspekte werden in diesem Zusammenhang bislang vernachlässigt. Zum einen die räumliche Anordnung textgenetischer Prozesse, die vor allem bei der Transkription von Manuskripten nicht in ausreichendem Maße berücksichtigt werden und zum anderen die zeitliche Abfolge dieser Prozesse. Die Vorliegende Arbeit soll diese Bereiche gezielt in den Blick nehmen. Dabei geht es zum einen um eine kritische Analyse der bisherigen Kodierungspraxis solcher Phänomene und zum anderen um die Möglichkeiten, wie solche Prozesse durch browserbasierte Lösungen visualisiert werden können. Die Arbeit ist in einen theoretischen und einen praktischen Teil untergliedert. Der erste Teil gibt einen Überblick über die Entwicklungen im Bereich Digitaler Editionen vor allem seit der Etablierung des {WWW} als generisch digitales Editionsmedium. Darauf folgt ein kurzer historischer Exkurs über Editionspraktiken zu Zeiten der Druckkultur. Anschließend werden zentrale Problembereiche digitaler textgenetischer Editionen thematisiert und modellhafte Lösungsansätze entwickelt. Abschließend werden diese in einem praxisnahen Teil in Form von beispielhaft implementierten Softwarekomponenten veranschaulicht.},
	institution = {Universität zu Köln},
	type = {Dissertation},
	author = {Hörnschemeyer, Jörg},
	date = {2017-07-22},
	langid = {german},
	keywords = {Digitale Edition, Editionswissenschafte, Textgenese},
}

@thesis{gradl_concept_2014,
	location = {Bamberg},
	title = {Concept and implementation of a rule framework to dynamically transform data and queries for heterogeneous collections},
	rights = {{CC}-{BY}},
	url = {https://zenodo.org/record/804890#.Wi_CzVXibIU},
	abstract = {The presented thesis provides a new approach for the integration of heterogeneous data in contexts, which (1) require data to remain in their original structural and semantic form in order to allow a dynamic ad-hoc integration based on individual use-cases and (2) need a particular focus on lexical and syntactical patterns, which are only bound to the instance-level of data and thus not covered by schema level integration.

Based on preliminary work, the concept of a data transformation framework is developed on the contextual base of the formal interpretation of semi-structured schemata as regular tree grammars. As the content of the terminal nodes has been identified to be often not in an atomic form, the rule framework is applied on contained data in order to describe implicitly existing patterns and translate the data into semantically enriched forms.},
	pagetotal = {108},
	institution = {Otto-Friedrich-Universität Bamberg},
	type = {phdthesis},
	author = {Gradl, Tobias},
	date = {2014-02-07},
	langid = {german},
	keywords = {data federation, data integration, domain specific languages, language theory},
}

@thesis{coll_ardanuy_entity-centric_2017,
	location = {Göttingen},
	title = {Entity-Centric Text Mining for
Historical Documents},
	rights = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {http://ediss.uni-goettingen.de/handle/11858/00-1735-0000-0023-3F65-3},
	abstract = {Recent years have seen an important increase of digitization projects in the cultural heritage domain. As a result, growing efforts have been directed towards the study of natural language processing technologies that support research in the humanities. This thesis is a contribution to the study and development of new text mining strategies that allow a better exploration of contemporary history collections from an entity-centric perspective. In particular, this thesis focuses on the challenging problems of disambiguating two specific kinds of named entities: toponyms and person names. They are approached as two clearly differentiated tasks, each of which exploiting the inherent characteristics that are associated to each kind of named entity. Finding the correct referent of a toponym is a challenging task, and this difficulty is even more pronounced in the historical domain, as it is not uncommon that places change their names over time. The method proposed in this thesis to disambiguate toponyms, {GeoSem}, is especially suited to work with collections of historical texts. It is a weakly-supervised model that combines the strengths of both toponym resolution and entity linking approaches by exploiting both geographic and semantic features. In order to do so, the method makes use of a knowledge base built using Wikipedia as a basis and complemented with additional knowledge from {GeoNames}. The method has been tested on a historical toponym resolution benchmark dataset in English and improved on the state of the art. Furthermore, five datasets of historical news in German and Dutch have been created from scratch and annotated. The method proposed in this thesis performs significantly better on them than two out-of-the-box state-of-the-art entity linking methods when only locations are considered for evaluation. Person names are likewise highly ambiguous. This thesis introduces a novel method for disambiguating person names from news articles. The method, {SNcomp}, exploits the relation between the ambiguity of a person name and the number of entities referred to by it. Modeled as a clustering problem in which the number of target entities is unknown, the method dynamically adapts its clustering strategy to the most suitable configuration for each person name depending on how common this name is. {SNcomp} has a strong focus on social relations and returns sets of automatically created social networks of disambiguated person entities extracted from the texts. The performance of the method has been tested on three person name disambiguation benchmark datasets in two different languages and is on par with the state of the art reported for one of the datasets, while using less specific resources. This thesis contributes to the fields of natural language processing and digital humanities. Information about entities and their relations is often crucial for historical research. Both methods introduced in this thesis have been designed and developed with the goal of assisting historians in delving into large collections of unstructured text and exploring them through the locations and the people that are mentioned in them.},
	pagetotal = {155},
	institution = {Georg-August-Universit¨at G¨ottingen},
	type = {Masterthesis},
	author = {Coll Ardanuy, Maria},
	date = {2017-11-14},
	langid = {english},
	note = {digital humanities; text mining; toponym disambiguation; person name disambiguation; historical text mining},
	keywords = {Text Mining, person name disambiguation, toponym disambiguation},
}

@thesis{efer_graphdatenbanken_2017,
	location = {Leipzig},
	title = {Graphdatenbanken für die textorientierten e-Humanities},
	url = {http://ul.qucosa.de/landing-page/?tx_dlf[id]=http%3A%2F%2Fubl.qucosa.de%2Fapi%2Fqucosa%253A15329%2Fmets},
	abstract = {Vor dem Hintergrund zahlreicher Digitalisierungsinitiativen befinden sich weite Teile der Geistes- und Sozialwissenschaften derzeit in einer Transition hin zur großflächigen Anwendung digitaler Methoden. Zwischen den Fachdisziplinen und der Informatik zeigen sich große Differenzen in der Methodik und bei der gemeinsamen Kommunikation. Diese durch interdisziplinäre Projektarbeit zu überbrücken, ist das zentrale Anliegen der sogenannten e-Humanities. Da Text der häufigste Untersuchungsgegenstand in diesem Feld ist, wurden bereits viele Verfahren des Text Mining auf Problemstellungen der Fächer angepasst und angewendet. Während sich langsam generelle Arbeitsabläufe und Best Practices etablieren, zeigt sich, dass generische Lösungen für spezifische Teilprobleme oftmals nicht geeignet sind. Um für diese Anwendungsfälle maßgeschneiderte digitale Werkzeuge erstellen zu können, ist eines der Kernprobleme die adäquate digitale Repräsentation von Text sowie seinen vielen Kontexten und Bezügen. In dieser Arbeit wird eine neue Form der Textrepräsentation vorgestellt, die auf Property-Graph-Datenbanken beruht – einer aktuellen Technologie für die Speicherung und Abfrage hochverknüpfter Daten. Darauf aufbauend wird das Textrecherchesystem „Kadmos“ vorgestellt, mit welchem nutzerdefinierte asynchrone Webservices erstellt werden können. Es bietet flexible Möglichkeiten zur Erweiterung des Datenmodells und der Programmfunktionalität und kann Textsammlungen mit mehreren hundert Millionen Wörtern auf einzelnen Rechnern und weitaus größere in Rechnerclustern speichern. Es wird gezeigt, wie verschiedene Text-Mining-Verfahren über diese Graphrepräsentation realisiert und an sie angepasst werden können. Die feine Granularität der Zugriffsebene erlaubt die Erstellung passender Werkzeuge für spezifische fachwissenschaftliche Anwendungen. Zusätzlich wird demonstriert, wie die graphbasierte Modellierung auch über die rein textorientierte Forschung hinaus gewinnbringend eingesetzt werden kann.},
	pagetotal = {267},
	institution = {Universität Leipzig},
	type = {Doktorarbeit},
	author = {Efer, Thomas},
	date = {2017-02-15},
	langid = {german},
	note = {Graphdatenbanken, Datenmodellierung, Recherchesysteme, e-Humanities, Text Mining, Korpusexploration, Information Retrieval},
	keywords = {Datenmodellierung, Graphdatenbanken, Information Retrieval, Recherchesysteme, Text Mining},
}
@thesis{hanusch_modell_2016,
	location = {Wuerzburg},
	title = {Modell einer Briefedition am Beispiel eines Briefes von Grabbe},
	pagetotal = {125},
	institution = {University of Wuerzburg, Department of Literary Computing},
	type = {Master-Thesis},
	author = {Hanusch, Christian},
	date = {2016-06-06},
}

@thesis{du_topic_2016,
	location = {Wuerzburg},
	title = {Topic Modeling deutscher Romane und die manuelle Evaluation des Ergebnisses},
	pagetotal = {82},
	institution = {University of Wuerzburg, Department of Literary Computing},
	type = {Master-Thesis},
	author = {Du, Keli},
	date = {2016-03-19},
}

@thesis{gutierrez_de_la_torre_literarische_2016,
	location = {Wuerzburg},
	title = {Literarische Konstellationen: eine Annäherung an die Salonforschung Mexikos im 19. Jahrhundert},
	pagetotal = {87},
	institution = {University of Wuerzburg, Department of Literary Computing},
	type = {Master-Thesis},
	author = {Gutiérrez De la Torre, Silvia Eunice},
	date = {2016-01-11},
}

@thesis{reger_figurennetzwerke_2016,
	location = {Wuerzburg},
	title = {Figurennetzwerke als Ähnlichkeitsmaß},
	pagetotal = {79},
	institution = {University of Wuerzburg, Department for Literary Computing},
	type = {Master-Thesis},
	author = {Reger, Isabella},
	date = {2016-08-08},
}

@thesis{draxler_computerunterstutzte_1988,
	title = {Computerunterstützte Dramenanalyse},
	url = {https://epub.ub.uni-muenchen.de/29404/},
	pagetotal = {124},
	institution = {{LMU} München, Institut für romanische Philologie},
	type = {Master Thesis},
	author = {Draxler, Christoph},
	urldate = {2016-11-04},
	date = {1988},
}

@thesis{reiter_discovering_2014,
	location = {Heidelberg},
	title = {Discovering Structural Similarities in Narrative Texts using Event Alignment Algorithms},
	rights = {info:eu-repo/semantics/{openAccess}},
	url = {http://www.ub.uni-heidelberg.de/archiv/17042},
	abstract = {This thesis is about the discovery of structural similarities across narrative texts. We will describe a method that is based on event alignments created automatically on automatically preprocessed texts. This opens up a path to large-scale empirical research on structural similarities across texts.
Structural similarities are of interest for many areas in the humanities and social sciences. We will focus on folkloristics and research of rituals as application scenarios. Folkloristics researches folktales, i.e., tales that have been passed down orally for a long time. Similarities across different folktales have been observed, both at the level of individual events (being abandoned in the woods) or participants (the gingerbread house) and structurally: Events do not happen at random, but in a certain order. Rituals are an omnipresent part of human behavior and are studied in ethnology, social sciences and history. Similarities across types of rituals have been observed and sparked a discussion about structural principles that govern the combination of individual ritual elements to rituals.
As descriptions of rituals feature a lot of uncommon language constructions, we will also discuss methods of domain adaptation in order to adapt existing {NLP} components to the domain of rituals. We will mainly use supervised methods and employ retraining as a means for adaptation. This presupposes annotating small amounts of domain data. We will be discussing the following linguistic levels: Part of speech, chunking, dependency parsing, word sense disambiguation, semantic role labeling and coreference resolution. On all levels, we have achieved improvements. We will also describe how these annotation levels are brought together in a single, integrated discourse representation that is the basis for further experiments.

In order to discover structural similarities, we employ three different alignment algorithms and use them to align semantically similar events. Sequence alignment (Needleman-Wunsch) is a classic algorithm with limited capabilities. A graph-based event alignment system that has been developed for newspaper texts will be used in comparison. As a third algorithm, we employ Bayesian model merging, which induces a hidden Markov model, from which we extract an alignment. We will evaluate the algorithms in two experiments. In the first experiment, we evaluate against a gold standard of aligned descriptions of rituals. Bayesian model merging achieves the best results, measured using the Blanc metric. Due to difficulties in creating an event alignment gold standard, the second experiment is based on cluster induction. Although this is not a strict evaluation of structural similarities, it gives some insight into the behavior of the algorithms.
We induce a document similarity measure from the generated alignments and use this measure to cluster the documents. The clustering is then compared against a gold standard classification of documents from both scenarios. In this experiment, the lemma alignment baseline achieves the best numerical performance on folktales (but as it aligns lemmas instead of event representations, its expressiveness is limited), followed by predicate alignment, Bayesian model merging and Needleman-Wunsch. On descriptions of rituals, the predicate alignment algorithm outperforms all shallow and more specialized baselines. Shallow measures of semantic similarities of texts outperform the alignment-based algorithms on folktales, but they do not allow the exact localization of similarities.
Finally, we present a graph-based algorithm that ranks events according to their participation in structurally similar regions across documents. This allows us to direct researchers from humanities to interesting cases, which are worth manual inspection. Because in digital humanities scenarios, the accessibility of results to researchers from humanities is of utmost importance, we close the thesis with a showcase scenario in which we analyze descriptions of rituals using the alignment, clustering and event ranking algorithms we have described before. We will show in this showcase how results can be visualized and interpreted by researchers of rituals.},
	institution = {University of Heidelberg},
	type = {Dissertation},
	author = {Reiter, Nils},
	urldate = {2014-09-22},
	date = {2014},
	keywords = {dm\_compling, ob\_narrative, ob\_plot},
}

@thesis{buchler_informationstheoretische_2013,
	location = {Leipzig},
	title = {Informationstheoretische Aspekte des Text Re-Use},
	url = {urn:nbn:de:bsz:15-qucosa-108515},
	pagetotal = {277},
	institution = {Leipzig},
	type = {phdthesis},
	author = {Büchler, Marco},
	date = {2013},
	langid = {german},
	keywords = {object: Texts},
}

@thesis{brunner_automatische_2013,
	location = {Würzburg},
	title = {Automatische Erkennung von Redewiedergabe in literarischen Texten},
	institution = {Universität Würzburg},
	type = {phdthesis},
	author = {Brunner, Annelen},
	date = {2013},
	langid = {german},
	keywords = {activity: Analyze quantitatively, object: Texts},
}

@thesis{wutschel_rechnergestutzte_1988,
	title = {Die rechnergestützte Simulationsmethode in den Sozialwissenschaften: theoret. Hintergrund u. Entwicklung von {PC}-Bausteinen simulierter Elementarprozesse},
	type = {phdthesis},
	author = {Wutschel, Brigitte},
	date = {1988},
	note = {Bochum, Univ., Diss., 1989},
	keywords = {activity: Simulate, field: Social Science, university: Bochum},
}

@book{wanske_musiknotation_1988,
	location = {Mainz [u.a.]},
	title = {Musiknotation : von der Syntax des Notenstichs zum {EDV}-gesteuerten Notensatz},
	isbn = {3-7957-2886-X},
	publisher = {Schott},
	author = {Wanske, Helene},
	date = {1988},
}

@thesis{walkowski_research_nodate,
	title = {Research Objects and the objectification of a research culture: Decolonizing e-Research to reconstruct a Semantic Web based concept for the support of Humanities in the Digital},
	url = {http://nowalkowski.de/blog/index.php},
	abstract = {The goal is to evaluate and adapt different approaches of Compound Objects (in the {OAI}-{ORE} meaning) in scientific context. Because these approaches mainly belong to science and not to humanities a critique revision has to take place. This leads to the {eScience} discourse which has to be mirrored by a humanities perspective as well.
Starting from a culture scientific view on the epistemic of network and the concept of data as "Faitiche" by Latour relevant units of {eScience} will be relocated making possible to give recommendations for Compound Research Objects in the humanities as "Extensive Narrations".},
	institution = {Humboldt-Universität zu Berlin},
	type = {Dissertation},
	author = {Walkowski, Niels-Oliver},
	note = {Advisor: Prof. Dr. Stefan Gradmann},
	keywords = {supervisor: Gradmann, Stefan, university: Berlin ({HU})},
}

@thesis{vasold_computergestutzte_1996,
	location = {Graz},
	title = {Computergestützte Itinerarerstellung und -analyse am Beispiel eines Salzburger Erzbischofs: Konrad {IV}. (1291 - 1312)},
	institution = {Karl-Franzens-Universität Graz},
	type = {Diplomarbeit},
	author = {Vasold, Gunter},
	date = {1996},
	note = {Betreuer: H. Ebner},
	keywords = {F-History, activity: Enrich, field: History, supervisor: Ebner, H., type: Thesis ({MA} level), university: Graz, {AT}},
}

@thesis{thibaut_verwendbarkeit_2010,
	title = {Die Verwendbarkeit des mobilen Web als Medium der Präsentation des kulturellen Erbes},
	pagetotal = {85},
	institution = {Köln},
	type = {Magisterarbeit},
	author = {Thibaut, Jasper},
	date = {2010-08-11},
	keywords = {F-Cultural Heritage, activity: Assess, object: Archives, object: Infrastructure, supervisor: Thaller, Manfred, type: Thesis ({MA} level), university: Cologne},
}

@thesis{strohmaier_methoden_2004,
	title = {Methoden der lexikalischen Nachkorrektur {OCR}-erfasster Dokumente},
	url = {http://deposit.ddb.de/cgi-bin/dokserv?idn=975146777},
	institution = {München},
	type = {phdthesis},
	author = {Strohmaier, Christian M.},
	date = {2004},
	note = {Online-Ressource
München, Univ., Diss, 2005},
	keywords = {Korpus {\textless}Linguistik{\textgreater}, {OCR}, Optische Zeichenerkennung, Uni: München},
}

@book{stenvert_constructing_1991,
	title = {Constructing the past: computer-assisted architectural-historical research ; the application of image-processing using the computer and computer-aided design for the study of the urban environment, illustrated by the use of treatises in seventeenth-century architecture},
	isbn = {90-9004-536-8},
	pagetotal = {492},
	author = {Stenvert, Ronald},
	date = {1991},
	note = {Utrecht, Rijksuniv., Diss.},
	keywords = {activity: Digitize, activity: Enrich, activity: Simulate, field: Art History, object: Architecture, object: Images, special: {CAD}, university: Utrecht, {NL}},
}

@book{steinbeck_struktur_1982,
	location = {Kassel [u.a.]},
	title = {Struktur und Ähnlichkeit: Methoden automatisierter Melodienanalyse},
	isbn = {3-7618-0681-7},
	publisher = {Bärenreiter},
	author = {Steinbeck, Wolfram},
	date = {1982},
	note = {Kiel, Univ., Habil.-Schr., 1979},
	keywords = {university: Kiel},
}

@book{schwanke_name_1992,
	location = {Heidelberg},
	title = {Name und Namengebung bei Goethe: computergestützte Studien zu epischen Werken},
	volume = {Beiheft 38},
	isbn = {3-533-04495-5},
	series = {Beiträge zur Namenforschung : Neue Folge},
	pagetotal = {490},
	publisher = {Winter},
	author = {Schwanke, Martina},
	date = {1992},
	note = {Kiel, Univ., Diss., 1992},
	keywords = {field: Literature, object: Texts, type: Thesis ({PhD} level), university: Kiel},
}

@thesis{rommel_and_1995,
	location = {Tübingen},
	title = {"And trace it in this poem every line" : Methoden und Verfahren computerunterstützter Textanalyse am Beispiel von Lord Byrons "Don Juan"},
	pagetotal = {404},
	institution = {Eberhard Karls Universität Tübingen},
	type = {Dissertation},
	author = {Rommel, Thomas},
	date = {1995},
}

@thesis{perstling_multimediale_nodate,
	location = {Graz},
	title = {Multimediale Dokumentation und Edition mehrschichtiger Texte: Das steirisch-landesfürstliche "Marchfutterurbar" von 1414/1426},
	institution = {Karl-Franzens-Universität Graz},
	type = {Dissertation},
	author = {Perstling, Matthias P.},
	note = {Betreuer: I. H. Kropač},
	keywords = {supervisor: Kropač, I.H., university: Graz, {AT}},
}

@thesis{naser_digitale_2010,
	location = {Würzburg},
	title = {Digitale Karten zur Geschichte der Städte in Franken},
	abstract = {Die Dissertation dokumentiert die Erstellung und Webimplementierung eines Geoinformationssystems ({GIS}) zu den Städten in Franken in Mittelalter und Früher Neuzeit. Der methodische Ansatz ist interdisziplinär und vereint geschichtswissenschaftliche und computerkartographische Herangehensweisen. Nach einer kurzen Einführung (Kapitel 2 und 3) werden die für die {EDV}-gestützte Aufbereitung nötigen Daten mittels klassischer geschichtswissenschaftlicher Methodik aus Quellen und Literatur extrahiert (Kapitel 4). Der Rest der Arbeit befasst sich mit der Aufarbeitung und Implementierung der Daten in zwei Desktop-{GIS}-Anwendungen (Kapitel 5 und 6) und schließlich mit verschiedenen Möglichkeiten der Publikation der Ergebnisse des {GIS} im Internet (Kapitel 7). Das Herzstück des Buches bilden 33 mit Hilfe des {GIS} erstellte Karten zur Geschichte der Städte in Franken (Kapitel 8) und Vergleiche mit bereits erschienenen Karten zur selben Thematik (Kapitel 9). Die Dissertation ist so konzipiert, dass sich die historischen Kapitel unabhängig vom {EDV}-Teil lesen lassen. Dem Leser steht es also frei, sich nur mit dem geschichtswissenschaftlichen oder nur mit dem computerkartographischen Teil zu befassen, falls er an dem jeweils anderen Teil kein Interesse hat.},
	pagetotal = {360},
	institution = {Universität Würzburg},
	type = {Dissertation},
	author = {Naser, Markus},
	date = {2010},
	note = {Betreuer: Prof. Dr. Helmut Flachenecker
Published as {ISBN} 978-3-88778-346-4},
}

@book{mosell_sprache_1974,
	location = {Darmstadt},
	title = {Sprache im Computer - ein Weg zur Gesellschaftsanalyse?: eine Untersuchung der Möglichkeiten automatischer Inhaltsanalyse anhand der Wahlhirtenbriefe des deutschen Episkopates seit 1946},
	volume = {18},
	isbn = {3-534-06435-6},
	series = {Impulse der Forschung},
	pagetotal = {211},
	publisher = {Wiss. Buchges.},
	author = {Mosell, Heinz},
	date = {1974},
	keywords = {F-History, activity: Analyze qualitatively, activity: Analyze quantitatively, field: History, object: Documents, object: Language},
}

@thesis{mertins_elektronische_2008,
	location = {Berlin},
	title = {Elektronische Editionen – Möglichkeiten, Grenzen, Perspektiven},
	rights = {http://www.diss.fu-berlin.de/diss/content/main/leitlinien/nutzungsbedingungen.xml},
	url = {http://www.diss.fu-berlin.de/diss/servlets/MCRFileNodeServlet/FUDISS_derivate_000000004413/DissEEFU.pdf;jsessionid=98EE2746A6C71C8E315C8F336CC876DF?hosts=},
	abstract = {Die Arbeit »Elektronische Editionen – Möglichkeiten, Grenzen, Perspektiven« stellt Konzepte und Technologien für elektronische Editionen auf dem Gebiet der Germanistik vor. Der Schwerpunkt der Arbeit liegt darauf darzustellen, welche Informationen schriftliche Texte enthalten, die über den buchstäblichen Informationsgehalt hinausgehen und welche Möglichkeiten es gibt, diese Informationen in elektronischen Editionen zu kodieren und damit einer elektronischen Auswertung zugänglich zu machen.
Ausgangspunkt ist die Betrachtung bestehender elektronischer Editionen, die sich grob in zwei Kategorien aufteilen lassen: elektronische Editionen, die sich an Büchern orientieren und elektronische Editionen, die die sich stärker an den Möglichkeiten orientieren, die elektronische Medien bieten.
Diese Beobachtung bildet den Anlass, die medialen Bedingungen von Texten in Büchern und von Texten in elektronischen Medien zu untersuchen. Im Mittelpunkt stehen dabei Informationen, die Bücher jenseits der Buchstaben, aus denen schriftliche Texte bestehen, übermitteln – beispielsweise durch Typografie und Layout oder durch Aspekte der Buchproduktion und –distribution. Diesen »Dispositiven der Gutenberg-Galaxis« werden die Möglichkeiten der Informationsübertragung und Informationsmodellierung in elektronischen Medien gegenübergestellt. Im Vordergrund stehen dabei Konzepte auf der Grundlage semantischer Textauszeichnung mit Hilfe von Auszeichnungssprachen wie der Standard Generalized Markup Language ({SGML}) bzw. der Extensible Markup Language ({XML}). Neben Technologien zum Umgang mit {SGML}/{XML}-kodierten Daten wird die Textauszeichnung nach den Richtlinien der Text Encoding Initiative ({TEI}) dargestellt; verschiedene Kodierungsmöglichkeiten und modelle werden diskutiert. Als Textgrundlagen dienen Ausschnitte aus Franz Kafka: »Der Process« (Frankfurter Kafka Ausgabe) und Georg Büchner: »Dantons Tod« (Marburger Büchner Ausgabe).
Abschließend werden Möglichkeiten vorgestellt, ergänzende Informationen zu elektronischen Texten als Metadaten zu erfassen. Auch hier werden geeignete Kodierungsmöglichkeiten aufgezeigt und diskutiert, etwa das Maschinelle Austauschformat für Bibliotheken ({MAB}, {MABxml}), die Dublin Core Metadata Initiative ({DC}, {DCMI}) oder die diesbezüglichen {TEI}-Richtlinien.},
	pagetotal = {158},
	institution = {Freie Universität Berlin},
	type = {Dissertation},
	author = {Mertins, Martin},
	urldate = {2011-05-16},
	date = {2008-11-07},
	note = {Gutachter - Prof. Dr. Ursula Kocher
Weitere Gutachter - Prof. Dr. Wolfgang Neuber},
}

@book{kuckartz_computer_1988,
	location = {Frankfurt am Main u.a.},
	title = {Computer und verbale Daten: Chancen zur Innovation sozialwissenschaftlicher Forschungstechnik},
	volume = {173},
	isbn = {3-631-40769-6},
	series = {Europäische Hochschulschriften},
	pagetotal = {257},
	number = {22, Soziologie},
	publisher = {Lang},
	author = {Kuckartz, Udo},
	date = {1988},
	note = {Berlin, Techn. Univ., Diss., 1988},
	keywords = {O-Methods, activity: Analyze quantitatively, field: Social Science, object: Texts, university: Berlin ({TU})},
}
@book{kortendick_drei_1996,
	location = {Canterbury},
	title = {Drei Schwestern und ihre Kinder: Rekonstruktion von Familiengeschichte und Identitätstransmission bei indischen Nederlanders mit Hilfe computerunterstützter Inhaltsanalyse},
	isbn = {0-904938-98-0},
	author = {Kortendick, Oliver},
	date = {1996},
	note = {Köln, Univ., Diss., 1995},
	keywords = {activity: Analyze qualitatively, field: Social Science, object: People, university: Cologne},
}

@thesis{iordanidis_anwendbarkeit_nodate,
	title = {Anwendbarkeit von {XML} Schema für Daten und Metadaten im Bereich digitaler Bibliotheken},
	url = {http://old.hki.uni-koeln.de/studium/MA/MA_iordanidis.pdf},
	pagetotal = {98},
	institution = {Köln},
	type = {Magisterarbeit},
	author = {Iordanidis, Martin},
	keywords = {activity: Enrich, object: Archives, object: Metadata, special: {XML}, supervisor: Schmitz, Wolfgang, supervisor: Thaller, Manfred, type: Thesis ({MA} level), university: Cologne},
}

@book{herkt_anwendungsmoglichkeiten_1991,
	location = {Bochum},
	title = {Anwendungsmöglichkeiten computergestützter Erfassungs- und Auswertungshilfen am Beispiel der Güter- und Einkünfteverzeichnisse des Kollegiatstiftes St. Mauritz in Münster},
	isbn = {3-88339-902-7},
	series = {Bochumer historische Studien / Mittelalterliche Geschichte},
	number = {9},
	publisher = {Brockmeyer},
	author = {Herkt, Matthias},
	date = {1991},
	note = {Bochum, Univ., Diss., 1991},
	keywords = {F-History, activity: Analyze quantitatively, field: History, object: Documents, type: Thesis ({PhD} level), university: Bochum},
}

@book{heckner_tagging_2009,
	location = {Boizenburg},
	title = {Tagging, rating, posting: studying forms of user contribution for web-based information management and information retrieval},
	isbn = {978-3-940317-39-1},
	series = {Schriften zur Informationswissenschaft},
	pagetotal = {240},
	number = {49},
	publisher = {Hülsbusch},
	author = {Heckner, Markus},
	date = {2009},
	note = {Regensburg, Univ., Diss., 2008},
	keywords = {activity: Tagging, university: Regensburg},
}

@book{hanlein_studies_1999,
	location = {Frankfurt am Main [u.a.]},
	title = {Studies in authorship recognition: a corpus based approach},
	volume = {352},
	isbn = {3-631-34420-1},
	series = {Europäische Hochschulschriften},
	pagetotal = {426},
	number = {14},
	publisher = {Lang},
	author = {Hänlein, Heike},
	date = {1999},
	note = {Augsburg, Univ., Diss., 1998},
	keywords = {activity: Analyze quantitatively, object: Corpora, special: Stylistics, special: Stylometry, university: Augsburg},
}

@book{geldbach_anaphernresolution_2001,
	location = {Frankfurt am Main [u.a.]},
	title = {Anaphernresolution und -übersetzung in der Sprachrichtung Russisch-Deutsch},
	isbn = {3-631-37738-X},
	series = {Berliner slawistische Arbeiten , {ISSN} 1430-192X},
	publisher = {Lang},
	author = {Geldbach, Stefanie},
	date = {2001},
	note = {Dissertation, Humboldt-Universität, Berlin, 2000},
}

@thesis{evert_statistics_2004,
	title = {The statistics of word cooccurrences: word pairs and collocations},
	url = {http://deposit.d-nb.de/cgi-bin/dokserv?idn=976240033},
	institution = {Stuttgart},
	type = {phdthesis},
	author = {Evert, Stefan},
	date = {2004},
	note = {Online-Ressource
Stuttgart, Univ., Diss, 2004},
	keywords = {Computerlinguistik, Kollokation, Korpus {\textless}Linguistik{\textgreater}, Polynomialverteilung, Uni: Stuttgart, Visualisierung},
}

@thesis{czmiel_adaquate_2003,
	title = {Adäquate Markupsysteme für die digitale Behandlung altägyptischer Texte},
	url = {http://old.hki.uni-koeln.de/studium/MA/MA_czmiel.pdf},
	pagetotal = {94},
	institution = {Köln},
	type = {Magisterarbeit},
	author = {Czmiel, Alexander},
	date = {2003-10-06},
	keywords = {activity: Enrich, field: Egyptology, object: Texts, special: {XML}, supervisor: Thaller, Manfred, type: Thesis ({MA} level), university: Cologne},
}

@thesis{bresinsky_computersimulation_2003,
	location = {Münster},
	title = {Die Computersimulation von sicherheitspolitischen Entscheidungsprozessen in Krisen- und Konfliktsituationen: ein interdisziplinärer Modellansatz},
	url = {http://miami.uni-muenster.de/servlets/DerivateServlet/Derivate-1747/Diss_Elo_Ver.pdf},
	abstract = {Gegenstand der Arbeit ist die Computersimulation von sicherheitspolitischen Entscheidungsprozessen. Methodisch werden die Grundlagen der Computersimulation in der Politikwissenschaft im interdisziplinären Kontext von Psychologie und kognitiver Modellierung erörtert. Erkenntnistheoretisch wird anhand der methodischen Grundlagen ein kognitiver Agent modelliert und die Entscheidungsprozesse am Beispiel eines sicherheitspolitischen Szenarios untersucht. Grundlage des Szenarios sind die Ereignisse um den gescheiterten {UNO} Einsatz zum Schutz der Stadt Srebrenica im Jahre 1995. Die Ergebnisse der Arbeit zeigen zum einen, dass die interdisziplinäre Zusammenarbeit zur Untersuchung sicherheitspolitischer Entscheidungsprozesse ohne Alternative bleibt. Zum anderen wir mit Hilfe des kognitiven Agentenmodells deutlich, dass die formale Modellierung und Computersimulation komplexer Szenarien in der Politikwissenschaft ein nützliches Instrument für die Theorienbildung und -überprüfung darstellt.},
	pagetotal = {255},
	institution = {Westfälische Wilhelms-Universität zu Münster (Westf.)},
	type = {Dissertation},
	author = {Bresinsky, Markus Andreas Stefanus},
	date = {2003-10-16},
	note = {Dekan: Prof. Dr. Tomas Tomasek
Referent: Prof. Dr. Dr. h.c. Reinhard Meyers
Korreferent: Prof. Dr. Dr. h.c. Wichard Woyke},
}

@thesis{zolper_digitale_2009,
	title = {Digitale Visualisierungen historischer Architektur},
	institution = {Köln},
	type = {Magisterarbeit},
	author = {Zolper, Susanne},
	date = {2009},
	keywords = {activity: Visualize, object: Architecture, supervisor: Nußbaum, Norbert, type: Thesis ({MA} level), university: Cologne},
}

@thesis{zifko_retrospektive_2009,
	location = {Graz},
	title = {Retrospektive Digitalisierung und Verlinkung digitaler Kulturdokumente gezeigt am ersten Band des Regensburger Urkundenbuchs},
	institution = {Karl-Franzens-Universität Graz},
	type = {Diplomarbeit},
	author = {Zifko, Theresa Elisabeth},
	date = {2009},
	note = {Betreuer: I.H. Kropač},
	keywords = {supervisor: Kropač, I.H., university: Graz, {AT}},
}

@thesis{wolfrum_beschreibung_2003,
	title = {Beschreibung der Reiß - Entwicklung eines Modells zur elektronischen Edition der Festschrift zur Brautfahrt Friedrichs V. von der Pfalz nach London (1613)},
	abstract = {Als am Valentinstag des Jahres 1613 die Ehe zwischen dem pfälzischen Kurfürsten Friedrich V. und
Elisabeth Stuart, einziger Tochter des regierenden englischen Königs James I., in der Kapelle von
Whitehall geschlossen wurde, bildeten die begleitenden, mehrtägigen Feierlichkeiten den fulminanten
Höhepunkt festlichen Glanzes seit der Ankunft des deutschen Fürsten in England im
Oktober zuvor.
Die Beschreibung der Reiß stellt einen der kulturhistorisch bedeutendsten Texte zur Pfälzischen
Hochzeit dar. Als ausführlichste Überlieferung schildert die deutsche Festbeschreibung die Ereignisse
von Friedrichs Aufbruch am 17. September des Jahres 1612 bis zur gemeinsamen Rückkehr
des neuvermählten Paares nach Heidelberg im darauffolgenden Juni. Die im Verlag Gotthard
Vögelins, des Pfälzischen Hofbuchdruckers, anonym erschienene Festbeschreibung umfaßt knapp
dreihundert Quarto-Druckseiten, die in einem Hauptteil zunächst die chronikalische Beschreibung
der Ereignisse behandelt und in einem Anhang anthologisch einige der Heidelberger Aufzüge sowie
die Aufstellung der Gefolge, die Furier- und Futterzettel, und schließlich die nach der Rückkehr
gehaltene Predigt des Heidelberger Hofkaplans, Abraham Scultetus, beifügt. Darüber hinaus zieren
zahlreiche Kupferstiche die Darstellung.
Kulturgeschichtlich ein Grenzgänger zwischen englischer und deutscher Hofkultur der Frühen
Neuzeit, fällt die Heidelberger Festbeschreibung literatur- und sprachhistorisch ganz in die Epoche
der mittleren deutschen Literatur. Das editorische Mo dell hat sich daher in einem Spannungsfeld
zwischen frühneuzeitlicher Edition und kulturellem Text versucht. Die Schwerpunkte der editorischen
Modellbildung liegen sowohl - durch die Einbindung der außergewöhnlich breit gestreuten
Festüberlieferung - in der archivarischen Anlage der Edition als insbesondere auch in der Kommentierung
eines solchen Gegenstands. Die als „kritisches Archiv“ gelöste Edition hat neben dem konventionellen
Realienkommentar daher die Kommentarform des {HyperIndex} entwickelt, welche die
motivischen Strukturen der gattungstypischen Choreographie zu erschließen sucht.
Die im Juli 2003 abgeschlossene Promotion legt in einem diskursiven Teil das Modell für die
kritische Edition der frühneuzeitlichen Festbeschreibung vor und in einer Internet-Edition dessen
Ausführung für das zehnte und siebzehnte Kapitel der Beschreibung der Reiß. Die Publikation sowohl
des elektronischen als auch des diskursiven Teils ist für den Sommer 2004 geplant.},
	institution = {München},
	type = {Dissertation},
	author = {Wolfrum, Ulrike},
	date = {2003-07},
	keywords = {F-History, activity: Create, activity: Enrich, field: History, object: Archives, type: Thesis ({PhD} level), university: München},
}

@thesis{wieners_zur_2008,
	title = {Zur Erweiterungsfähigkeit bestehender {OCR} Verfahren auf den Bereich extrem früher Drucke},
	pagetotal = {81},
	institution = {Köln},
	type = {Magisterarbeit},
	author = {Wieners, Jan Gerrit},
	date = {2008-08-29},
	keywords = {activity: Digitize, object: Texts, special: {OCR}, supervisor: Thaller, Manfred, type: Thesis ({MA} level), university: Cologne},
}

@thesis{vasold_netzbasierte_nodate,
	location = {Graz},
	title = {Netzbasierte Zugriffsformen auf komplexe geschichtswissenschaftliche Informationssysteme},
	institution = {Karl-Franzens-Universität Graz},
	type = {Dissertation},
	author = {Vasold, Gunter},
	note = {Betreuung: I.H. Kropač},
	keywords = {supervisor: Kropač, I.H., university: Graz, {AT}},
}

@thesis{varela_unscharfte_2004,
	title = {Unscharfte Wissensrepräsentationen bei der Implementation des Semantic Web},
	url = {http://old.hki.uni-koeln.de/studium/MA/MA_botana.pdf},
	pagetotal = {87},
	institution = {Köln},
	type = {Magisterarbeit},
	author = {Varela, Javier Botana},
	date = {2004-07-07},
	keywords = {activity: Conceptualize, object: Knowledge, special: Semantics, supervisor: Förtsch, Reinhard, supervisor: Thaller, Manfred, type: Thesis ({MA} level), university: Cologne},
}

@thesis{stuckardt_qualitative_2000,
	location = {Frankfurt am Main},
	title = {Qualitative Inhaltsanalyse durch Computer - ein uneinlösbarer Anspruch? : Untersuchungen zur algorithmischen Textinhaltserschließung am Beispiel der referentiellen Interpretation},
	url = {http://publikationen.ub.uni-frankfurt.de/volltexte/2005/1274/index.html},
	shorttitle = {Qualitative Inhaltsanalyse durch Computer - ein uneinlösbarer Anspruch?},
	abstract = {Als Methode zur inhaltlichen Erschließung von Texten, dem überwiegenden Ausgangsmaterial empirischer Untersuchungen, kommt der Inhaltsanalyse in den Sozialwissenschaften eine Schlüsselstellung zu. Allgemein wird unterschieden zwischen quantitativen, wörterbuchbasierten und qualitativen, »hermeneutischen« Verfahren; gemäß der weithin vertretenen Lehrmeinung ist nur die quantitative, einzelwortorientierte Inhaltsanalyse von Computern durchführbar. Der Autor zeigt auf, daß sich auf der Grundlage der Dichotomisierung »quantitativ-qualitativ « kein geeignetes Kriterium ergibt, um die Frage nach Reichweite und Grenzen der algorithmischen Inhaltsanalyse abschließend zu beantworten. Unter interdisziplinärem Rekurs auf aktuelle Entwicklungen in Computerlinguistik, Künstlicher Intelligenz und Kognitionswissenschaften wird der Nachweis erbracht, daß die computergestützte Textinhaltserschließung nicht notwendig auf die Einzelwortanalyse beschränkt ist. Für ein zentrales qualitatives Problem der klassischen wörterbuchbasierten Inhaltsanalyse, die referentielle Interpretation von Pronomen, wird eine algorithmische Lösung erarbeitet, softwaretechnisch umgesetzt und unter Anwendungsbedingungen empirisch evaluiert. Mit der vorliegenden Arbeit gelingt der Nachweis, daß die im Kontext der »Qualitativ- Quantitativ « - Kontroverse postulierten »prinzipiellen Grenzen« der computergestützten Inhaltsanalyse nichtzutreffend, da auf algorithmischem Wege transzendierbar sind. Somit ergeben sich völlig neue Perspektiven für den Einsatz von Computern in der Inhaltsanalyse.},
	pagetotal = {326},
	institution = {Goethe Universität, Frankfurt am Main},
	type = {Dissertation},
	author = {Stuckardt, Roland},
	urldate = {2011-05-26},
	date = {2000-01-07},
	note = {1. Gutachter: Mans, Dieter (Prof. Dr. Dr.)},
}

@thesis{steding_computer-based_2002,
	location = {Regensburg},
	title = {Computer-based scholarly editions: context, concept, creation, clientele},
	pagetotal = {350},
	institution = {Universität Regensburg},
	type = {Dissertation},
	author = {Steding, Sören A.},
	date = {2002},
}

@book{sitter_computergestutzte_2000,
	location = {Osnabrück},
	title = {Computergestützte Arbeitsmethoden in der Musikwissenschaft: ein Beitrag zu ihrer Entwicklung},
	isbn = {3-932147-59-6},
	series = {Musik und neue Technologie},
	pagetotal = {187},
	number = {2},
	publisher = {Univ.-Verl. Rasch},
	author = {Sitter, Peer},
	date = {2000},
	note = {Osnabrück, Univ., Diss.},
	keywords = {O-Methods, field: Musicology, object: Music, type: Thesis ({PhD} level), university: Osnabrück},
}

@thesis{simonis_framework_2004,
	title = {A framework for processing and presenting parallel text corpora},
	url = {http://deposit.ddb.de/cgi-bin/dokserv?idn=971862257},
	institution = {Tübingen},
	type = {phdthesis},
	author = {Simonis, Volker},
	date = {2004},
	note = {Online-Ressource
Tübingen, Univ., Diss, 2004},
	keywords = {Korpus {\textless}Linguistik{\textgreater}, Uni: Tübingen (Eberhard Karls Universität), {XML}},
}

@thesis{schrader_xml-datenformat_2007,
	location = {Tübingen},
	title = {Ein {XML}-Datenformat zur Repräsentation kritischer Musikedition unter besonderer Berücksichtigung von Neumennotation},
	institution = {Tübingen},
	type = {phdthesis},
	author = {Schräder, Gregor},
	date = {2007},
	keywords = {activity: Encode, field: Musicology, special: {XML}, supervisor: Crestani, Marcus, supervisor: Klaeren, Herbert, supervisor: Morent, Stefan, type: Thesis ({MA} level), university: Tübingen},
}

@book{schnell_eingabe_1985,
	location = {Bern},
	title = {Die Eingabe Musikalischer Information Als Teil Eines Arbeitsinstrumentes: Ein Beitrag Zur Computeranwendung in Der Musikwissenschaft},
	isbn = {3261040475},
	shorttitle = {Die Eingabe Musikalischer Information Als Teil Eines Arbeitsinstrumentes},
	pagetotal = {490},
	publisher = {P. Lang},
	author = {Schnell, Christoph},
	date = {1985},
	keywords = {Data processing, Musicology},
}

@thesis{sasaki_sekundare_2004,
	title = {Sekundäre Informationsstrukturierung: eine Methodologie zur Verbindung {XML}- und {RDF}-basierter Informationsmodellierung sowie ihre Anwendung auf linguistische Korpora},
	url = {http://deposit.ddb.de/cgi-bin/dokserv?idn=972873007},
	institution = {Bielefeld},
	type = {phdthesis},
	author = {Sasaki, Felix},
	date = {2004},
	note = {Online-Ressource
Bielefeld, Univ., Diss, 2004},
	keywords = {Korpus {\textless}Linguistik{\textgreater}, {RDF}, Uni: Bielefeld, {XML}},
}

@thesis{saller_neues_2004,
	title = {Ein neues Editionskonzept für die Schriften Notkers des Deutschen anhand von De interpretatione},
	abstract = {Die ersten nachweisbaren Aristoteles-Kommentare sind nicht etwa in lateinischer, sondern in althochdeutscher Sprache verfasst: Notkers des Deutschen Bearbeitungen von De interpretatione und der Kategorien (um das Jahr 1000). Obwohl diese von interdisziplinärem Interesse sind, wurden sie bisher kaum außerhalb der Sprachgeschichtsforschung wahrgenommen. Die wissenschaftliche Editorik beschränkte sich darauf, möglichst handschriftengetreu Sprachmaterial bereitzustellen; dabei wurden Aspekte der Textualität ebenso wie inhaltlich-sachliche Fragen vernachlässigt.

Unter Einbezug der Entstehungs- und Überlieferungsgeschichte von Peri hermeneias / De interpretatione wurde ein Editionsmodell entwickelt, das einige Desiderate erfüllt. Zur Erprobung und Demonstration des Modells wurden die ersten 15 Kapitel (nach Notkers Einteilung in 93 Kapitel; entspricht den Kapiteln 1-4 in der heute gebräuchlichen Einteilung) von De interpretatione ediert. Die Textstruktur wird mit typographischen Mitteln visualisiert. Eine synoptisch angelegte Übersetzung beider Textsprachen (Althochdeutsch und Latein) dient der Vermittlung über die Disziplinengrenzen hinaus. Dasselbe gilt für den Kommentar, der sich auf sachliche (philosophische), philologisch-textkritische und sprachliche Fragen erstreckt.

Das neue Editionskonzept ist dazu geeignet, sowohl den Wissensstand Notkers und seiner Klosterschule als auch die Methodik der Wissensvermittlung zu erhellen, da die Charakteristik der Notkerschen Textgestaltung stark mit dem - mündlichen - Unterricht korreliert.},
	institution = {München},
	type = {Dissertation},
	author = {Saller, Harald},
	date = {2004-10-27},
	keywords = {F-History, activity: Conceptualize, activity: Enrich, field: History, object: Archives, type: Thesis ({PhD} level), university: München},
}
@book{riepl_sind_1993,
	location = {St. Ottilien},
	title = {Sind David und Saul berechenbar?: von der sprachlichen Analyse zur literarischen Struktur von 1 Sam 21 und 22},
	volume = {39},
	isbn = {3-88096-539-0},
	series = {Münchener Universitätsschriften / Philosophische Fakultät Altertumskunde und Kulturwissenschaften / Arbeiten zu Text und Sprache im Alten Testament},
	pagetotal = {393},
	publisher = {{EOS}-Verl.},
	author = {Riepl, Christian},
	date = {1993},
	note = {München, Univ., Diss., 1992},
	keywords = {activity: Analyze quantitatively, field: Literature, field: Theology, object: Texts, type: Thesis ({PhD} level), university: München},
}

@thesis{rehbein_gottinger_2008,
	location = {Göttingen},
	title = {Göttinger Statuten im 15. Jahrhundert: Enstehung – Entwicklung – Edition},
	url = {http://webdoc.sub.gwdg.de/diss/2010/rehbein/rehbein.pdf},
	abstract = {Abstract ({ENG})

This dissertation forms the methodological basis for the digital edition of one of Göttingen's late medieval town records, "kundige bok 2" (second book of announcements). "Kundige bok 2" contains mainly a special kind of municipal statutes, the so called burspraken, which the city council regarded as so important for everyday life and the social, economical and political order of the town that they annually announced (read aloud) the statutes to the public. The dissertation shows that this late medieval law is subject to frequent change and develops over the course of time. In accordance with this evolution of the town law, the written evidence of the burspraken in "kundige bok 2" is modified by the scribes on a regular basis. Hence, the manuscript appears to us containing a multilayered and complex text, and the multidimensionality and dynamics of this cannot be adequately represented by traditional forms of output such as print-based editions. It is therefore the main purpose of this work to develop a scholarly edition of "kundige bok 2" which is both, digital and dynamic, and which exceeds the possibilities of its print-based model. It is not a mere clone of a traditional edition in a new medium, but it opens the field for augmentation of research by allowing random access to any text layer, diachronic as well as synchronic, as well as automatic comparison of texts and user- and usage-driven visualisation. This text-based theoretical part of the dissertation serves as an introductory study to the digital edition which is published on the internet. The study includes the critical introduction to "kundige bok 2", a discussion on methodology, and a description of the technical realisation of the digital edition.

Abstract ({GER})

Die Arbeit bildet die theoretische Grundlage der digitalen Edition des "kundige bok 2", eines Göttinger Amtsbuches des Spätmittelalters. Die in ihm enthaltenen Bursprakentexte bilden im 15. Jahrhundert einen wichtigen Teil des Göttinger Stadtrechts, bezeichnen sie doch jenen Teil der städtischen Statuten, der wegen seiner Bedeutung für das gesellschaftliche Miteinander, vor allem des Alltagslebens, alljährlich öffentlich den Bewohnern der Stadt verkündet wird. Die Arbeit zeigt auf, daß das spätmittelalterliche Recht alles andere als unveränderlich ist, sondern sich im Laufe der Jahrzehnte fortwährend weiterentwickelt. Mit der Entwicklung dieses Rechts einher gehen die regelmäßigen Überarbeitungen der Texte im "kundige bok 2", die es somit als eine vielschichtige und komplexe Quelle erscheinen lassen. Die Untersuchung zeigt, daß eine herkömmliche Print-Ausgabe durch die Restriktionen des Mediums nicht geeignet ist, diese Vielschichtigkeit des Textes und die Dynamik des zugrundeliegenden Stadtrechts zu repräsentieren. Hauptanliegen der Arbeit ist daher die Herleitung einer sowohl digitalen als auch dynamischen Edition des "kundige bok 2", die über das Potential einer reinen Print-Edition hinausgeht. Sie ist nicht eine bloße Kopie ihres gedruckten Vorbildes in einem neuen Medium, sondern eröffnet darüber hinaus der Forschung neue Möglichkeiten in Form von gezielten, diachronen wie synchronen, Zugriffen auf sämtliche Textschichten, automatischem Textvergleich und sowohl nutzer- als auch nutzungsorientierte Visualisierungen. Der hier in Textform vorliegende theoretische Teil präsentiert als begleitende Studie die Einleitung zur im Internet veröffentlichten Edition. Neben der kritischen Einführung in die Quelle umfaßt dies vor allem eine Methodendiskussion sowie die Beschreibung der technischen Umsetzung.},
	pagetotal = {187},
	institution = {Universität Göttingen},
	type = {Dissertation},
	author = {Rehbein, Malte},
	urldate = {2011-05-16},
	date = {2008},
	note = {Betreuer:	Petke, Wolfgang; Prof. Dr.
Gutachter: Röckelein, Hedwig; Prof. Dr.
Gutachter: Aufgebauer, Peter; {PD} Dr.},
}

@book{rechenmacher_jungfrau_1995,
	location = {St. Ottilien},
	title = {Jungfrau, Tochter Babel: eine Studie zur sprachwissenschaftlichen Beschreibung althebräischer Texte am Beispiel von Jes 47},
	volume = {44},
	isbn = {3-88096-544-7},
	series = {Münchener Universitätsschriften / Philosophische Fakultät Altertumskunde und Kulturwissenschaften / Arbeiten zu Text und Sprache im Alten Testament},
	pagetotal = {421},
	publisher = {{EOS}-Verl.},
	author = {Rechenmacher, Hans},
	date = {1995},
	note = {München, Univ., Diss., 1994},
	keywords = {X-{CHECK}, activity: Analyze qualitatively, activity: Interpret, field: Study of Hebrew, field: Theology, object: Language, university: München},
}

@thesis{puhl_empirische_2008,
	title = {Empirische Evaluation Digitaler Wasserzeichen-Techniken für die Authentizität von Fotografien},
	pagetotal = {73},
	institution = {Köln},
	type = {Magisterarbeit},
	author = {Puhl, Johanna},
	date = {2008-01-14},
	keywords = {activity: Authenticate, object: Images, special: Watermarks, supervisor: Thaller, Manfred, university: Cologne},
}

@book{muller-hagedorn_wissenschaftliche_2002,
	location = {Tübingen},
	title = {Wissenschaftliche Kommunikation im multimedialen Hypertext},
	volume = {3},
	isbn = {{ISBN} 3-86057-872-3},
	series = {Stauffenburg Medien},
	publisher = {Stauffenburg},
	author = {Müller-Hagedorn, Silke},
	date = {2002},
	note = {Karlsruhe, Univ., Diss., 2000},
	keywords = {Digitale Medien, Hyperdokument, Hypertext, Hypertextdokument, Höfische Dichtung, Höfische Literatur, Informationsprozess, Kommunikation, Kommunikationsprozess, Literaturwissenschaft, Literaturwissenschaften, Mediävistik, Mittelalterforschung, Mittelhochdeutsch, Mittelhochdeutsche Sprache, Neue Medien, Ritterdichtung, Scientific Community, Wissenschaftliche Fachgemeinschaft, Wissenschaftliche Gemeinschaft, Wissenschaftsgemeinschaft},
}

@thesis{malm_editing_2004,
	title = {Editing Economic History: A Study and Electronic-Genetic Edition of Ezra Pound`s The Fifth Decad of Cantos},
	url = {http://web.archive.org/web/20070702015817/http://www.textkritik.uni-muenchen.de/dissertation.php?mmalm#en},
	abstract = {Editing Economic History is the first comprehensive study and scholarly edition of Ezra Pound's cycle of poems The Fifth Decad of Cantos (published 1937). My thesis comprises a monograph that analyzes the historical background, genetic development, and editorial dimension of Pound's work, plus a {CD}-{ROM} containing the electronic genetic edition of Pound's manuscript and typescript materials, as related to The Fifth Decad of Cantos.

My study seeks to encompass the whole context of textual genesis, biography, and mental history by which The Fifth Decad of Cantos is defined. This includes the analysis of Pound's working methods, his mental background, and his theories concerning some subjects that play a crucial role in The Fifth Decad of Cantos: economics, politics, religion, and predjudice. At the same time, editing the Pound papers requires the development of a critical, methodological framework and editorial principles that allow us to analyze the autographs, to edit and finally interpret them.

The basic contents of my thesis result from those requirements, being divided into work-centered and editorial chapters. After a methodological introduction and terminological definitions of central textual/editorial terms (part I), my thesis deals with Pound's working methods and the genesis of the work as a foundation for the interpretation of the poems (part {II}). Third, The Fifth Decad of Cantos is placed in its historical-intellectual context (part {III}). Finally, the editorial assumptions on which my edition is based are explained (part {IV}).},
	institution = {München},
	type = {Dissertation},
	author = {Malm, Mike W.},
	date = {2004-06-02},
	keywords = {F-History, activity: Create, field: History, field: Literature, object: Archives, type: Thesis ({PhD} level), university: München},
}

@book{macqueen_integration_2010,
	location = {Frankfurt am Main [u.a.]},
	title = {The integration of {MILLION} into the English system of number words: a diachronic study},
	isbn = {978-3-631-60156-3},
	series = {English corpus linguistics},
	pagetotal = {297},
	number = {11},
	publisher = {Lang},
	author = {{MacQueen}, Donald Sims},
	date = {2010},
	note = {Uppsala, Univ., Diss., 2009},
	keywords = {object: Corpora, university: Uppsala, S},
}

@book{lonneker_konzeptframes_2003,
	location = {Berlin},
	title = {Konzeptframes und Relationen: Extraktion, Annotation und Analyse französischer Corpora aus dem World Wide Web},
	volume = {275},
	isbn = {3-89838-275-3},
	series = {Dissertationen zur künstlichen Intelligenz},
	pagetotal = {341},
	publisher = {Aka},
	author = {Lönneker, Birte},
	date = {2003},
	note = {Hamburg, Univ., Diss., 2003},
	keywords = {activity: Enrich, object: Corpora, special: Semantics, university: Hamburg},
}

@thesis{kummer_towards_2007,
	title = {Towards semantic interoperability of cultural information systems: making ontologies work},
	institution = {Köln},
	type = {Magisterarbeit},
	author = {Kummer, Robert},
	date = {2007},
	keywords = {special: Interoperability, special: Ontologies, special: Semantics, university: Cologne},
}

@book{kehren_moglichkeiten_1994,
	location = {Erkelenz},
	title = {Möglichkeiten und Grenzen der computativen Auswertung von Daten des Atlas der deutschen Volkskunde ({ADV})},
	isbn = {3-924928-16-X},
	series = {Bonner kleine Reihe zur Alltagskultur},
	pagetotal = {277},
	number = {2},
	publisher = {Kehren},
	author = {Kehren, Georg},
	date = {1994},
	note = {Bonn, Univ., Diss.},
	keywords = {activity: Analyze quantitatively, object: Maps, type: Thesis ({PhD} level), university: Bonn},
}

@book{jesser_interaktive_1991,
	location = {Bern ; Frankfurt am Main[u.a.]},
	title = {Interaktive Melodieanalyse: Methodik und Anwendung computergestützter Analyseverfahren in Musikethnologie und Volksliedforschung: typologiche Untersuchung der Balladenforschung des {DVA}},
	isbn = {3-261-04360-1},
	series = {Studien zur Volksliedforschung},
	pagetotal = {308},
	number = {12},
	publisher = {Lang},
	author = {Jesser, Barbara},
	date = {1991},
	note = {Essen, Univ., Diss., 1990},
	keywords = {activity: Analyze qualitatively, activity: Analyze quantitatively, field: Musicology, object: Music, type: Thesis ({PhD} level), university: Essen},
}

@book{illgner_verkundigende_1990,
	title = {Verkündigende Sprache und automatische Sprachverarbeitung: Überlegungen zur Semantik},
	pagetotal = {182},
	author = {Illgner, Susanne},
	date = {1990},
	note = {Heidelberg, Univ., Diss.},
	keywords = {activity: Analyze qualitatively, field: Theology, object: Language, object: Texts, special: Semantics, type: Thesis ({PhD} level), university: Heidelberg},
}

@thesis{gruber_pischelsdorf_1994,
	location = {Graz},
	title = {Pischelsdorf im Spiegel des  Franziszeischen Katasters. Quellenkunde und Sozialtopographie},
	url = {http://hfi.uni-graz.at/hfi/students/gruber/diplomar.html},
	abstract = {Diese Diplomarbeit betritt in gewisser Weise Neuland. Die Verwendung von quantifizierenden Methoden in der Geschichtswissenschaft ist zwar keine neue Erfindung, aber sie steckt in weiten Bereichen noch in den Kinderschuhen. Dies trifft auch auf die Nutzbarmachung von historischen Karten und Plänen mit Hilfe von fachspezifischen datentechnischen Lösungen zu. Karten und Pläne werden bisher fast ausschließlich zur Illustration von Ergebnissen verwendet. Diese Diplomarbeit soll zum Einsatz von Karten zur Gewinnung neuer Erkenntnisse in der historischen Forschung beitragen.

Die Erstellung von digitalisierten Stadtplänen ist kein grundsätzliches Problem mehr. Als Beispiel dafür sei der Stadtplan von Göttingen genannt, der von N. Winnige digitalisiert wurde.gif Leistungsfähige Digitalisiergeräte sind auf den Universitäten vorhanden (wenn auch nicht in den Geschichte-Instituten) und durch das Datenbanksystem ist es möglich, diese digitalisierten Daten zu verwalten und mit anderen Daten über diese topographischen Objekte zu verbinden.

In dieser Diplomarbeit soll der Versuch gemacht werden, anhand der Schriftoperate des Franziszeischen Katasters und der dazugehörigen Riedkarte eine sozialtopographische Untersuchung einer Gemeinde vorzunehmen. Dazu ist es notwendig, sich zuerst mit dem Informationsgehalt und der Geschichte dieser Quellen auseinander zu setzen.},
	institution = {Karl-Franzens-Universität Graz},
	type = {Diplomarbeit},
	author = {Gruber, Siegfried},
	date = {1994-11},
	note = {Betreuung: I.H. Kropač},
}

@book{gotz_berichterstattung_2000,
	location = {Berlin},
	title = {Die Berichterstattung über Bibliotheken in der Presse: eine computerunterstützte Inhaltsanalyse},
	isbn = {3-89722-549-2},
	series = {Berliner Arbeiten zur Bibliothekswissenschaft},
	pagetotal = {156},
	number = {3},
	publisher = {Logos},
	author = {Götz, Martin},
	date = {2000},
	note = {Berlin, Humboldt-Univ., Diss., 2000},
	keywords = {activity: Analyze qualitatively, object: Documents, object: Texts, type: Thesis ({PhD} level), university: Berlin ({HU})},
}

@thesis{gontek_java_2006,
	title = {Ein Java Tool zur 3D Navigation in Objektbeschreibungen},
	url = {http://old.hki.uni-koeln.de/studium/MA/MA_gontek.pdf},
	institution = {Köln},
	type = {Magisterarbeit},
	author = {Gontek, Mirko},
	date = {2006-08-05},
	keywords = {activity: Visualize, object: Metadata, special: 3D, supervisor: Thaller, Manfred, type: Thesis ({MA} level), university: Cologne},
}

@book{eberhardt_sprachliche_2007,
	location = {Münster ; München [u.a.]},
	title = {Die sprachliche Umsetzung neuer Technologien im Französischen: am Beispiel des Internet- und Computerwortschatzes},
	isbn = {978-3-8309-1854-7},
	url = {http://www.waxmann.com/?id=20&cHash=1&buchnr=1854},
	abstract = {How does a language keep up with rapid technical innovations? What are new developments called and are there certain regularities in finding suitable designations?

This book examines naming processes in the French language for Internet and computer terminology in the period 1996 to 2001. Within this special time period the official acceptance of the new medium Internet arose to the detriment of the old Minitel system in France.

This corpus-based linguistic analysis focuses on the use of Internet and computer terminology within French general language. It points out pragmatic aspects of the different uses of terms in computer contexts and other contexts of use.

The first chapter treats the history of the French language with its episodes of normative interventions; it points out the conflicts as well as the measures taken by the state. Against the background of this history the contact between language communities is discussed, especially concerning Anglicisms and traditional loan classifications. Language change and new tendencies in French vocabulary are examined.

The topics of language contact and borrowing processes are often discussed as a threat to the French language under the aspect of language maintenance. Previous studies in this field have described the Anglo-American influence on the French language and the characteristics of terms created by the French terminology commissions (Beinke 1990). The integration of Anglicisms in French (Klein / Lienart / Ostyn 1997) has been the subject of studies as well as the possible acceptance or rejection of neologisms among speakers of French (Helfrich 1993). Other studies focus on language for special purposes (Jansen 2005, Ahlers und Holtus 1999, Walter 1997), or on the effect of French terminology decrees on the language of computer scientists (Le Guilly-Wallis 1991).

The present study follows an empirical corpus-based approach and analyzes the actual occurrences and forms of Internet and computer terms. Furthermore, the period of six years allows a diachronic view of the development of naming processes.

The data sources for the present analysis were a self-compiled corpus of a French daily newspaper (Le Monde) and a weekly journal (Le Nouvel Observateur) as well as two annual business reports, one from a traditional business sector and the second from the telecommunications industry. A total of 254 terms were searched for in the corpus. A database system was developed in order to examine productive naming processes; the approach adopted was a componential analysis.},
	publisher = {Waxmann},
	author = {Eberhardt, Alexandra},
	date = {2007},
	note = {Münster (Westfalen), Univ., Diss., 2006},
	keywords = {activity: Analyze qualitatively, field: Linguistics, field: Romance Philology, object: Corpora, object: Language, special: Lexicon, type: Thesis ({PhD} level), university: Münster ({WWU})},
}

@book{dotzler_computerprogramme_1999,
	title = {Computerprogramme zur Unterstützung qualitativer Textanalyse: ein Beitrag vor dem Hintergrund der aktuellen Praxis und Technik},
	author = {Dotzler, Hans},
	date = {1999},
	note = {München, Univ., Philos. Fak., Diss., 1994},
	keywords = {field: Literature, university: München},
}

@thesis{dimpel_computergestutzte_2002,
	location = {Erlangen-Nürnberg},
	title = {Computergestützte textstatistische Untersuchungen an mittelhochdeutschen Texten},
	institution = {Freidrich-Alexander-Universität Erlangen-Nürnberg},
	type = {Dissertation},
	author = {Dimpel, Friedrich Michael},
	date = {2002},
	note = {Hauptberichter: Kugler, Hartmut (Prof. Dr.)},
}

@thesis{dickmann_einsatzmoglichkeiten_2003,
	location = {Göttingen},
	title = {Einsatzmöglichkeiten neuer Informationstechnologien für die Aufbereitung und Vermittlung geographischer Informationen - das Beispiel kartengestützte Online-Systeme},
	rights = {http://webdoc.sub.gwdg.de/diss/copyr\_diss.html},
	url = {http://webdoc.sub.gwdg.de/diss/habil/2004/dickmann/index.html},
	abstract = {Abstract ({ENG})

The internet is a prime means for distributing and visualising spatial data from cartographic and geographic databases. Reaching a broad audience, the number of maps designed and distributed on the World Wide Web has increased dramatically since the mid-1990s. From an original scientific niche, internet-based technology has grown to almost universal application in many practical contexts.

In line with demands for fast access to information and rapid transfer of spatial data, internet-based maps are now delivered to the user in a fraction of the time required to distribute print maps. A wide range of distribution instruments exists, including raw data servers (e.g. {DEM}), maps on demand (Internet Map Servers), interactive image maps, Web-{GIS}, animated maps and 3D-maps (virtual reality). A growing industry is involved in developing software specifically for distribution and retrieval of spatial (geographic) data. The process of data distribution via the internet is characterized by increasing commercialization and professionalism, leading to the development of very user-friendly systems that are used by a growing range of enterprises and organisations. Multimedia components now widely accompany standard map generation, meeting the user’s growing multimedia orientation and opening new ways of sharing spatial information. In contrast to non-digital maps, ordinary {HTML}-page Internet Map Servers are characterized by a wide range of techniques and interactive features which make information more individual and specific. Therefore, such servers offer interesting perspectives for digital cartography in the context of presentation and exploration of spatial data.

In practice, both print and internet-based maps are now equally well used. But how good are internet-based maps? Are they more effective than conventional print maps or just a pretty technological toy? Adequate transmission of information requires two features. First of all, information must be transmitted completely and correctly (effectiveness). But secondly, the result must be seen in relation to the amount of time required to achieve full and correct information (efficiency). To estimate the potential added value of the new technology, the effectiveness of web maps (interactive and animated maps) needs to be compared to that of print maps. This study describes a first attempt at experimentally assessing the effectiveness of information transmission for both map types.

The experiment analysed patterns of map use in 84 first-term geography students of Göttingen University. The students were split into two equally sized groups, dealing with one type of map respectively. Their task was to respond to a list of questions, which aimed to establish how well topographical and land use information could be extracted from each type of map. In order to be comparable, questions were identical for the conventional map and the online version. This also enabled an estimate of the time required for assimilating and processing information. Results indicate a general difference in perceptive behaviour of both sample groups, although the differences between the two forms of visualisation are not always readily apparent. Of particular interest for evaluating the success of map-based online information transfer was the proportion of correct answers. In six out of nine questions, the interactive web map yielded more correct answers than the conventional map. The average proportion of correct answers was about 76\% for web maps compared to 61\% for print maps, indicating a higher effectiveness of map-based online systems. Although the study was unable to include all forms of cartographic presentations available on the World Wide Web, results indicate that spatial information is more effectively transmitted by internet-based maps as part of an integrated visualisation system than classic printed maps.

Abstract ({GER})

Für die Bearbeitung sowohl praktischer als auch wissenschaftlicher Fragestellungen gewinnt der Einsatz kartengestützter Online-Systeme heute zunehmend an Bedeutung und führt zu einer außerordentlich großen Aufwertung der klassischen Funktion der Kartendarstellung in raumbezogenen Arbeits- und Forschungsprozessen. Dies bezieht sich nicht nur auf das Fach Geographie, sondern auch auf andere geowissenschaftliche Disziplinen. Alle Raumwissenschaften greifen auf räumliche Modelle in Form von Karten oder kartenverwandten Darstellungen zurück, um Zugang zur räumlichen Realität zu erhalten und theoriegeleitet arbeiten zu können.

Wie bei Fernerkundungstechniken oder statistischen Analyseverfahren, die in zahlreichen geographischen Forschungsprozessen oder praktischen Auswertungstätigkeiten angewandt werden, ist es auch im Bereich der Online-Informationsvermittlung heute erforderlich, über hinreichende Kenntnisse der Wirkungsweise der eingesetzten Techniken (Verfahren) zu verfügen. Gerade im Hinblick auf die optische Perfektion, die mittlerweile kennzeichnend für die digitalen Produkte raumbezogener Visualisierungstechniken ist, erscheint es notwendig, technische Zusammenhänge und Grundlagen aufzuzeigen.

Die Analyse der Konstruktions- und Distributionsweisen und die Herausstellung der informationstheoretischen Vorzüge kartengestützter Online-Systeme wurde dabei um eine empirische Überprüfung ergänzt, um eine umfassendere Bewertung des tatsächlichen Vermittlungserfolges von geographischen Informationen erreichen zu können. Mit diesem Ansatz wurde der Versuch unternommen, die These von der größeren Effizienz von Webkarten zu überprüfen und aufzuzeigen, ob und ggfs. in welchem Umfang sich die mit einer Web-Karte verfolgte Intention, einen Raumausschnitt thematisch oder topographisch wiederzugeben, im Vergleich zu konventionellen Printkarten unterschiedlich umsetzen lässt. Die Datenerhebung erfolgte über eine Befragung von zwei unterschiedlichen Versuchsgruppen, deren Wahrnehmungsleistungen als Indikator für die Wirkung der unterschiedlich codierten Rauminformationen gewertet wurden.

Insgesamt belegen die Ergebnisse des kartographischen Experiments die Annahme, dass der Vermittlungserfolg von kartengestützten Rauminformationen, die über das Internet übertragen werden, größer ist als bei konventionellen Printkarten. Der Anteil an korrekten Antworten, der bei der vergleichenden Befragung erzielt wurde, ist bei den mit der Web-Technologie visualisierten Karten durchschnittlich um rund 15\% größer als bei den normalen Printkarten. Insbesondere bei komplexeren Rauminformationen, bei denen es um vergleichende Betrachtungen oder das Erfassen von kausalen Zusammenhängen geht, besitzen interaktive und multimedial unterstützte Raumdarstellungen, wie sie das World Wide Web ermöglicht, eindeutig Vorteile. Die Möglichkeiten zum räumlich, zeitlich und inhaltlich nahezu unbegrenzten Informationsabruf und zur Variation der Datenwiedergabe von Webkarten kommen dem jeweils unterschiedlich ausgeprägten Wahrnehmungsvermögen der Kartennutzer zugute und verbessern offensichtlich die individuelle Informationsaufnahme.},
	pagetotal = {222},
	institution = {Georg-August-Universität Göttingen},
	type = {Habilitation},
	author = {Dickmann, Frank},
	urldate = {2011-05-26},
	date = {2003},
	keywords = {550 Geowissenschaften, 74.40; 74.48;, {QBD} 300, Web-Mapping, Internet, Kartenserver, web mapping, Internet, map-based online system, mapserver},
}

@thesis{choo_study_2004,
	location = {München},
	title = {Study on Computer-Aided Design Support of Traditional Architectural Theories},
	url = {http://d-nb.info/971275300/34},
	abstract = {Abstract in English

The research presented in this thesis describes a computer-aided design support of traditional architectural theories. Traditional architectural theories in western architecture have been considered as a basis for answering the fundamental questions of architecture: proportion, symmetry, colour, harmony and so on. In particular, the aesthetic aspect of these theories has been one of many important architectural aspects, and which is concerned with the field of architecture in determining the beauty of architectural form. The most significant role of the traditional theories in architecture is to maintain unity, to avoid chaos and then to achieve harmony in a design, using some specific design principles. However, current technology-guided constructions tend to neglect often the importance of these theories due to the standardization of building elements, due to mechanically-prepared construction and the reducing completion costs, etc. Thus, this research proposes a design support system as a design assistant that gives an intelligent advice on architectural design, using analytical design- and ordering- principles of traditional theories for the optimization of the architectural design from the aesthetic perspective. To evaluate the aesthetic quality of an architectural design, this system is implemented in the {AutoCAD} environment, using the {AutoLISP}. It is applied so as to explain and develop aesthetic qualities of a design. Designs proposed by this system include optimum designs, which are based on the traditional architectural theories, and new ones which can be in future connected to information models. To do this, the definition of information about building elements is accomplished by using the neutral format {EXPRESS} and {EXPRESS}-G for such application systems. The results of the application system are presented, such as the easily generating and quickly conceptualising of an object model, the checking of the aesthetic value of the design during the various design phases, the helping to find direction during rational searching for a solution. The user can easily appreciate the usefulness of the proposed system as a set of tools for searching for rational architectural aesthetics and formal solutions at different design-stages. It is to be hoped that a new "traditional" fundamental of architecture, such as the proposed system, incorporating {CAAD} systems, will find its place among new technological methods in the {AEC} industry and so help to bridge the gap between the value of traditional architecture and {CAAD} systems.

Abstract in Deutsch

Diese Dissertation befasst sich mit der Untersuchung von Anwendungsmöglichkeiten der traditionellen Gestaltungs- Theorien, wie z.B. Harmonie, Proportion, Takt und Rhythmus, Farbenlehre, und deren Implementierung in {CAAD}-Systeme. Allgemeines Ziel der Arbeit ist es, an einigen Beispielen Möglichkeiten zu zeigen, wie die Quantifizierung von Qualität mit Hilfe von {CAAD}-Unterstützung erfolgen könnte. Traditionelle Architekturtheorien zielen oft darauf ab, eine gestalterische Einheit zu erzeugen, das Chaos zu vermeiden und "Harmonie" zu erzeugen. Im Besonderen benutzt die sogenannte "Ästhetik" oder Bestimmung der Schönheit von Architekturformen immer wieder bestimmte Ordnungsprinzipien, Ähnlichkeiten und oder Kontraste, um ein spannungsvolles Verhältnis von Ordnung und Chaos, von Gleichartigkeiten und Verschiedenartigkeiten zu erzeugen. Seit Jahrhunderten, scheiterten, vielen Gestaltungstheorien wie Farb- und Harmonie-Lehre, Proportions- und andere Ordnungs-Systeme am Problem der großen Zahl - d.h. der nicht mehr praktikablen viel zu komplizierten Zahlreichen oder Verhältnisgleichungen. Seit kurzen erst gibt es Computer bei der Planung und gestalterischen Kontrolle für problemlose Bewältigung komplizierter Zahlenwerke hinter den Gestaltungstheorien. Gegenwärtig rücken durch Computerbasierte Methoden und durch die Standardisierung von Bauten und Bauteilen, sowie durch die Maschinengesteuerten Fertigungstechniken zur Reduzierung von Baukosten, etc., die ästhetischen Aspekte der Baugestaltung in den Hintergrund. Daher wird ein Computerunterstütztes Entwurfssystem in dieser Dissertation vorgeschlagen, der die Qualität der architektonischen Gestaltung überprüft und, unter Verwendung analytischer Ordnungsprinzipien der traditionellen Architekturtheorien, optimiert. Um die Qualität der Gestaltung im Entwurfsprozess zu evaluieren, wird jedes Gestaltungsprinzip mit Hilfe der Programmiersprache {AutoLISP} in die {AutoCAD}-Umgebung implementiert. Dieses System schlägt dann dem Benutzer einige dieser auf traditionellen Architekturtheorien basierende Alternativen vor. Es sieht darüber hinaus die Implementierung in Bauteilbasierte Gebäudemodelle vor. Dafür werden die Informationen der Bauteile mit dem neutralen Format {EXPRESS} und {EXPRESS}-G für solche Systeme definiert. Der hier entwickelte Entwurfsassistent ist in der Lage, einfach und schnell abstrakte Objektmodelle zu generieren, und deren gestalterische Qualität zu prüfen, zu variieren und zu optimieren. Er soll dem Benutzer bei der komplexen Suche nach ästhetisch ansprechenden Lösungen in den verschiedenen Entwurfsphasen helfen. Es ist zu hoffen, dass neue "traditionelle" Gestaltungs-Methoden, wie das vorgeschlagene System, in den Computerbasierten Planungsprozess der Bauindustrie Eingang finden wird und helfen kann, eine Brücke zwischen traditioneller Architektur und {CAAD} Systemen zu schlagen.},
	pagetotal = {173},
	institution = {Technische Universität München},
	type = {Dissertation},
	author = {Choo, Seung Yeon},
	urldate = {2011-05-26},
	date = {2004-02-12},
	note = {Betreuer: Wienands. R.; Univ.-Prof. Dr.-Ing., Junge, R.; Univ.-Prof.
Gutachter: Junge, R.; Univ.-Prof.
Gutachter: Wienands. R.; Univ.-Prof. Dr.-Ing.},
	keywords = {45 Architektur, Architektur,, {AutoCAD}/{AutoLISP}, {CAAD}, Design Theorie, {IFC}, Ordnungsprinzip, Produkt Model,, Ästhetik},
}

@thesis{burkard_design_2005,
	title = {Design und Implementation eines editorischen Redaktionsmoduls für ein digitales Bibliotheks- / Archivsystem},
	url = {http://old.hki.uni-koeln.de/studium/MA/MA_burkard.pdf},
	pagetotal = {74},
	institution = {Köln},
	type = {Magisterarbeit},
	author = {Burkard, Benjamin},
	date = {2005-02-22},
	keywords = {F-Cultural Heritage, activity: Collaborate, object: Archives, object: Infrastructure, special: Crowdsourcing, supervisor: Thaller, Manfred, type: Thesis ({MA} level), university: Cologne},
}

@thesis{bernedo_schneider_wenn_2009,
	location = {Kassel},
	title = {Wenn Agenten sich streiten. Ein Agentenmodell zur Erforschung sozialer Konflikte},
	url = {http://www.uni-kassel.de/upress/online/frei/978-3-89958-852-1.volltext.frei.pdf},
	abstract = {Pages 4-7 of the pdf in the {URL} listed below.
My abstract: Uses a computer simulation model to examine social conflict.},
	pagetotal = {281},
	institution = {Universität Kassel},
	type = {Dissertation},
	author = {Bernedo Schneider, Gordon},
	date = {2009},
	note = {Erster Gutachter: Prof. Dr. Andreas Ernst
Zweiter Gutacther: Prof. Dr. Frank Beckenbach
Tag der Disputation: 20. November 2009},
}

@thesis{bathke_design_2006,
	title = {Design und Implementation einer paläographischen Arbeitsumgebung},
	pagetotal = {97},
	institution = {Köln},
	type = {Magisterarbeit},
	author = {Bathke, Ivo},
	date = {2006-03-20},
	keywords = {activity: Create, field: Palaeography, object: Infrastructure, supervisor: Thaller, Manfred, type: Thesis ({MA} level), university: Cologne},
}

@book{bader_simson_1991,
	location = {Tübingen},
	title = {Simson bei Delila: computerlinguistische Interpretation des Textes Ri 13 - 16},
	volume = {3},
	isbn = {3-7720-1952-8},
	series = {Textwissenschaft, Theologie, Hermeneutik, Linguistik, Literaturanalyse, Informatik},
	pagetotal = {468},
	publisher = {Francke},
	author = {Bader, Winfried},
	date = {1991},
	note = {Tübingen, Univ., Diss., 1989},
	keywords = {activity: Analyze qualitatively, activity: Analyze quantitatively, activity: Interpret, field: Theology, object: Texts, university: Tübingen},
}